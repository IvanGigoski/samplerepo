{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 1A20-2135\n",
      "\n",
      " Directory of C:\\Users\\ksi\\IT\\Toptal dijabetes Ivan\n",
      "\n",
      "23.06.2022.  18:40    <DIR>          .\n",
      "23.06.2022.  18:40    <DIR>          ..\n",
      "23.06.2022.  18:39    <DIR>          .ipynb_checkpoints\n",
      "20.06.2022.  18:24           309.025 diabetes_test_analysis.csv\n",
      "20.06.2022.  18:24           241.604 diabetes_test_info.csv\n",
      "20.06.2022.  18:24         1.846.315 diabetes_train_analysis.csv\n",
      "20.06.2022.  18:25         1.439.845 diabetes_train_info.csv\n",
      "23.06.2022.  18:40         1.686.433 Toptal dijabetes Ivan petljao.ipynb\n",
      "18.06.2022.  11:39         1.631.397 Toptal dijabetes Ivan.ipynb\n",
      "               6 File(s)      7.154.619 bytes\n",
      "               3 Dir(s)  284.423.544.832 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 1A20-2135\n",
      "\n",
      " Directory of C:\\Users\\ksi\\IT\\Toptal dijabetes Ivan\n",
      "\n",
      "20.06.2022.  18:24           309.025 diabetes_test_analysis.csv\n",
      "20.06.2022.  18:24           241.604 diabetes_test_info.csv\n",
      "20.06.2022.  18:24         1.846.315 diabetes_train_analysis.csv\n",
      "20.06.2022.  18:25         1.439.845 diabetes_train_info.csv\n",
      "               4 File(s)      3.836.789 bytes\n",
      "               0 Dir(s)  284.423.528.448 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir diabetes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('diabetes_test_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95466</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94851</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95740</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91845</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88603</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id cholesterol    gluc  smoke  alco  active pressure  diabetes\n",
       "0  95306         low  medium      0     0       0   120/80         1\n",
       "1  86688         low     low      0     0       1   100\\70         0\n",
       "2  98038         low     low      0     0       0  140/100         1\n",
       "3  88694         low     low      0     0       1   120\\90         0\n",
       "4  92856         low     low      0     0       0   130\\80         0\n",
       "5  95466         low     low      0     0       1   120/80         0\n",
       "6  94851         low     low      1     0       1   120/80         0\n",
       "7  95740         low     low      0     0       1   120/80         0\n",
       "8  91845         low  medium      0     0       1   120\\80         0\n",
       "9  88603         low     low      0     0       1   120/80         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           10000 non-null  int64 \n",
      " 1   cholesterol  10000 non-null  object\n",
      " 2   gluc         10000 non-null  object\n",
      " 3   smoke        10000 non-null  int64 \n",
      " 4   alco         10000 non-null  int64 \n",
      " 5   active       10000 non-null  int64 \n",
      " 6   pressure     10000 non-null  object\n",
      " 7   diabetes     10000 non-null  int64 \n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('diabetes_test_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85656</td>\n",
       "      <td>19149</td>\n",
       "      <td>165</td>\n",
       "      <td>62.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85658</td>\n",
       "      <td>14453</td>\n",
       "      <td>159</td>\n",
       "      <td>67.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85659</td>\n",
       "      <td>15877</td>\n",
       "      <td>168</td>\n",
       "      <td>59.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85660</td>\n",
       "      <td>21228</td>\n",
       "      <td>167</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85661</td>\n",
       "      <td>54</td>\n",
       "      <td>163</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85662</td>\n",
       "      <td>15485</td>\n",
       "      <td>169</td>\n",
       "      <td>65.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85665</td>\n",
       "      <td>17304</td>\n",
       "      <td>153</td>\n",
       "      <td>70.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85666</td>\n",
       "      <td>20421</td>\n",
       "      <td>160</td>\n",
       "      <td>76.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85668</td>\n",
       "      <td>15509</td>\n",
       "      <td>169</td>\n",
       "      <td>67.8</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85669</td>\n",
       "      <td>21817</td>\n",
       "      <td>153</td>\n",
       "      <td>81.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    age  height  weight  gender\n",
       "0  85656  19149     165    62.0       m\n",
       "1  85658  14453     159    67.0       m\n",
       "2  85659  15877     168    59.0       m\n",
       "3  85660  21228     167    70.0  female\n",
       "4  85661     54     163    82.0       f\n",
       "5  85662  15485     169    65.0       m\n",
       "6  85665  17304     153    70.0    male\n",
       "7  85666  20421     160    76.0    male\n",
       "8  85668  15509     169    67.8       f\n",
       "9  85669  21817     153    81.0       m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F4BE641C48>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.hist('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F4BE85B708>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.hist('age', bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92808.565800</td>\n",
       "      <td>13736.01190</td>\n",
       "      <td>164.423900</td>\n",
       "      <td>74.239897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4134.279941</td>\n",
       "      <td>9105.18069</td>\n",
       "      <td>8.281755</td>\n",
       "      <td>14.815373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>85656.000000</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>89241.750000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>92771.500000</td>\n",
       "      <td>18196.00000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>96379.250000</td>\n",
       "      <td>20552.25000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>23701.00000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id          age        height       weight\n",
       "count  10000.000000  10000.00000  10000.000000  9680.000000\n",
       "mean   92808.565800  13736.01190    164.423900    74.239897\n",
       "std     4134.279941   9105.18069      8.281755    14.815373\n",
       "min    85656.000000     39.00000     57.000000    21.000000\n",
       "25%    89241.750000     60.00000    159.000000    64.000000\n",
       "50%    92771.500000  18196.00000    165.000000    72.000000\n",
       "75%    96379.250000  20552.25000    170.000000    83.000000\n",
       "max    99999.000000  23701.00000    198.000000   183.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85656</td>\n",
       "      <td>52.427105</td>\n",
       "      <td>165</td>\n",
       "      <td>62.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85658</td>\n",
       "      <td>39.570157</td>\n",
       "      <td>159</td>\n",
       "      <td>67.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85659</td>\n",
       "      <td>43.468857</td>\n",
       "      <td>168</td>\n",
       "      <td>59.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85660</td>\n",
       "      <td>58.119097</td>\n",
       "      <td>167</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85661</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>99993</td>\n",
       "      <td>52.676249</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>99995</td>\n",
       "      <td>61.878166</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>99996</td>\n",
       "      <td>52.199863</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>99998</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>99999</td>\n",
       "      <td>56.235455</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        age  height  weight  gender\n",
       "0     85656  52.427105     165    62.0       m\n",
       "1     85658  39.570157     159    67.0       m\n",
       "2     85659  43.468857     168    59.0       m\n",
       "3     85660  58.119097     167    70.0  female\n",
       "4     85661  54.000000     163    82.0       f\n",
       "...     ...        ...     ...     ...     ...\n",
       "9995  99993  52.676249     168    76.0       f\n",
       "9996  99995  61.878166     158   126.0       m\n",
       "9997  99996  52.199863     183   105.0  female\n",
       "9998  99998  61.000000     163    72.0       m\n",
       "9999  99999  56.235455     170    72.0       m\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[df2['age'] > 130, 'age'] /= 365.25\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F4BE987B48>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.hist('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            85656\n",
       "age       52.427105\n",
       "height          165\n",
       "weight         62.0\n",
       "gender            m\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85656</td>\n",
       "      <td>52.427105</td>\n",
       "      <td>165</td>\n",
       "      <td>62.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85658</td>\n",
       "      <td>39.570157</td>\n",
       "      <td>159</td>\n",
       "      <td>67.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85659</td>\n",
       "      <td>43.468857</td>\n",
       "      <td>168</td>\n",
       "      <td>59.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85660</td>\n",
       "      <td>58.119097</td>\n",
       "      <td>167</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85661</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        age  height  weight  gender\n",
       "0  85656  52.427105     165    62.0       m\n",
       "1  85658  39.570157     159    67.0       m\n",
       "2  85659  43.468857     168    59.0       m\n",
       "3  85660  58.119097     167    70.0  female\n",
       "4  85661  54.000000     163    82.0       f"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    52.427105\n",
       "1    39.570157\n",
       "2    43.468857\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[[0,1,2], 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function hist_frame in module pandas.plotting._core:\n",
      "\n",
      "hist_frame(data: 'DataFrame', column: 'IndexLabel' = None, by=None, grid: 'bool' = True, xlabelsize: 'int | None' = None, xrot: 'float | None' = None, ylabelsize: 'int | None' = None, yrot: 'float | None' = None, ax=None, sharex: 'bool' = False, sharey: 'bool' = False, figsize: 'tuple[int, int] | None' = None, layout: 'tuple[int, int] | None' = None, bins: 'int | Sequence[int]' = 10, backend: 'str | None' = None, legend: 'bool' = False, **kwargs)\n",
      "    Make a histogram of the DataFrame's columns.\n",
      "    \n",
      "    A `histogram`_ is a representation of the distribution of data.\n",
      "    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n",
      "    the DataFrame, resulting in one histogram per column.\n",
      "    \n",
      "    .. _histogram: https://en.wikipedia.org/wiki/Histogram\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : DataFrame\n",
      "        The pandas object holding the data.\n",
      "    column : str or sequence, optional\n",
      "        If passed, will be used to limit data to a subset of columns.\n",
      "    by : object, optional\n",
      "        If passed, then used to form histograms for separate groups.\n",
      "    grid : bool, default True\n",
      "        Whether to show axis grid lines.\n",
      "    xlabelsize : int, default None\n",
      "        If specified changes the x-axis label size.\n",
      "    xrot : float, default None\n",
      "        Rotation of x axis labels. For example, a value of 90 displays the\n",
      "        x labels rotated 90 degrees clockwise.\n",
      "    ylabelsize : int, default None\n",
      "        If specified changes the y-axis label size.\n",
      "    yrot : float, default None\n",
      "        Rotation of y axis labels. For example, a value of 90 displays the\n",
      "        y labels rotated 90 degrees clockwise.\n",
      "    ax : Matplotlib axes object, default None\n",
      "        The axes to plot the histogram on.\n",
      "    sharex : bool, default True if ax is None else False\n",
      "        In case subplots=True, share x axis and set some x axis labels to\n",
      "        invisible; defaults to True if ax is None otherwise False if an ax\n",
      "        is passed in.\n",
      "        Note that passing in both an ax and sharex=True will alter all x axis\n",
      "        labels for all subplots in a figure.\n",
      "    sharey : bool, default False\n",
      "        In case subplots=True, share y axis and set some y axis labels to\n",
      "        invisible.\n",
      "    figsize : tuple, optional\n",
      "        The size in inches of the figure to create. Uses the value in\n",
      "        `matplotlib.rcParams` by default.\n",
      "    layout : tuple, optional\n",
      "        Tuple of (rows, columns) for the layout of the histograms.\n",
      "    bins : int or sequence, default 10\n",
      "        Number of histogram bins to be used. If an integer is given, bins + 1\n",
      "        bin edges are calculated and returned. If bins is a sequence, gives\n",
      "        bin edges, including left edge of first bin and right edge of last\n",
      "        bin. In this case, bins is returned unmodified.\n",
      "    \n",
      "    backend : str, default None\n",
      "        Backend to use instead of the backend specified in the option\n",
      "        ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to\n",
      "        specify the ``plotting.backend`` for the whole session, set\n",
      "        ``pd.options.plotting.backend``.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    legend : bool, default False\n",
      "        Whether to show the legend.\n",
      "    \n",
      "        .. versionadded:: 1.1.0\n",
      "    \n",
      "    **kwargs\n",
      "        All other plotting keyword arguments to be passed to\n",
      "        :meth:`matplotlib.pyplot.hist`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    matplotlib.AxesSubplot or numpy.ndarray of them\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    This example draws a histogram based on the length and width of\n",
      "    some animals, displayed in three bins\n",
      "    \n",
      "    .. plot::\n",
      "        :context: close-figs\n",
      "    \n",
      "        >>> df = pd.DataFrame({\n",
      "        ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n",
      "        ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n",
      "        ...     }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n",
      "        >>> hist = df.hist(bins=3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (pd.DataFrame.hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      10000 non-null  int64  \n",
      " 1   age     10000 non-null  float64\n",
      " 2   height  10000 non-null  int64  \n",
      " 3   weight  9680 non-null   float64\n",
      " 4   gender  10000 non-null  object \n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('diabetes_train_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62538</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49159</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60683</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42924</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52888</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54781</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160/100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10661</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12127</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20298</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78105</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123/83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id cholesterol gluc  smoke  alco  active pressure  diabetes\n",
       "0  62538         low  low      0     0       1   100/80         0\n",
       "1  49159         low  low      0     0       1   120/82         0\n",
       "2  60683         low  low      0     0       1   120/80         0\n",
       "3  42924         low  low      0     0       0   120\\80         0\n",
       "4  52888         low  low      0     0       0   120/80         0\n",
       "5  54781         low  low      0     0       1  160/100         0\n",
       "6  10661         low  low      0     0       1   120\\80         0\n",
       "7  12127         low  low      0     0       1   120\\80         0\n",
       "8  20298         low  low      0     0       1   120/80         0\n",
       "9  78105         low  low      0     0       1   123/83         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           60000 non-null  int64 \n",
      " 1   cholesterol  60000 non-null  object\n",
      " 2   gluc         60000 non-null  object\n",
      " 3   smoke        60000 non-null  int64 \n",
      " 4   alco         60000 non-null  int64 \n",
      " 5   active       60000 non-null  int64 \n",
      " 6   pressure     60000 non-null  object\n",
      " 7   diabetes     60000 non-null  int64 \n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('diabetes_train_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>21914</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>157</td>\n",
       "      <td>93.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>178</td>\n",
       "      <td>95.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>158</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>19834</td>\n",
       "      <td>164</td>\n",
       "      <td>68.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  height  weight gender\n",
       "0   0     50     168    62.0      f\n",
       "1   1     55     156    85.0      m\n",
       "2   2  18857     165    64.0   male\n",
       "3   3  17623     169    82.0      f\n",
       "4   4     47     156    56.0      m\n",
       "5   8  21914     151    67.0      m\n",
       "6   9     60     157    93.0   male\n",
       "7  12     61     178    95.0      f\n",
       "8  13     48     158    71.0      m\n",
       "9  14  19834     164    68.0      m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      60000 non-null  int64  \n",
      " 1   age     60000 non-null  int64  \n",
      " 2   height  60000 non-null  int64  \n",
      " 3   weight  58002 non-null  float64\n",
      " 4   gender  60000 non-null  object \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>51.627652</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48.249144</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>85650</td>\n",
       "      <td>56.098563</td>\n",
       "      <td>178</td>\n",
       "      <td>77.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>85651</td>\n",
       "      <td>61.949350</td>\n",
       "      <td>160</td>\n",
       "      <td>79.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>85652</td>\n",
       "      <td>41.998631</td>\n",
       "      <td>156</td>\n",
       "      <td>52.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>85653</td>\n",
       "      <td>57.349760</td>\n",
       "      <td>167</td>\n",
       "      <td>75.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>85655</td>\n",
       "      <td>61.831622</td>\n",
       "      <td>169</td>\n",
       "      <td>86.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        age  height  weight  gender\n",
       "0          0  50.000000     168    62.0       f\n",
       "1          1  55.000000     156    85.0       m\n",
       "2          2  51.627652     165    64.0    male\n",
       "3          3  48.249144     169    82.0       f\n",
       "4          4  47.000000     156    56.0       m\n",
       "...      ...        ...     ...     ...     ...\n",
       "59995  85650  56.098563     178    77.0  female\n",
       "59996  85651  61.949350     160    79.0       m\n",
       "59997  85652  41.998631     156    52.0    male\n",
       "59998  85653  57.349760     167    75.0  female\n",
       "59999  85655  61.831622     169    86.0       f\n",
       "\n",
       "[60000 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.loc[df4['age'] > 130, 'age'] /= 365.25\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F4BEA3C448>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4.hist('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df24 = pd.merge(df2, df4, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, age, height, weight, gender]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df24[df24.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13 = pd.merge(df1, df3, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, cholesterol, gluc, smoke, alco, active, pressure, diabetes]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13[df13.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df13, df24, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306         low  medium      0     0       0   120/80         1   \n",
       "1      86688         low     low      0     0       1   100\\70         0   \n",
       "2      98038         low     low      0     0       0  140/100         1   \n",
       "3      88694         low     low      0     0       1   120\\90         0   \n",
       "4      92856         low     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997         low     low      0     0       1   130\\80         0   \n",
       "69996   8296         low     low      0     0       1   130\\90         0   \n",
       "69997  17896         low     low      1     0       1   110/70         0   \n",
       "69998  40803      medium     low      1     0       1   152/90         1   \n",
       "69999  11945         low     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender  \n",
       "0      61.000000     165    90.0       f  \n",
       "1      39.923340     162    50.0       m  \n",
       "2      64.035592     156    74.0       m  \n",
       "3      47.000000     162    89.0       m  \n",
       "4      50.343600     162    72.0       f  \n",
       "...          ...     ...     ...     ...  \n",
       "69995  53.377139     178   103.0  female  \n",
       "69996  60.032854     160    82.0       m  \n",
       "69997  55.477070     158    60.0    male  \n",
       "69998  55.969884     175    85.0       f  \n",
       "69999  48.000000     169    71.0       m  \n",
       "\n",
       "[70000 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       52385\n",
       "medium     9549\n",
       "high       8066\n",
       "Name: cholesterol, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cholesterol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       59479\n",
       "high       5331\n",
       "medium     5190\n",
       "Name: gluc, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gluc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m         31775\n",
       "f         17247\n",
       "male      13755\n",
       "female     7223\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>67682.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49972.419900</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.053771</td>\n",
       "      <td>0.803729</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>53.164078</td>\n",
       "      <td>164.359229</td>\n",
       "      <td>74.193823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28851.302323</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.225568</td>\n",
       "      <td>0.397179</td>\n",
       "      <td>0.433016</td>\n",
       "      <td>6.762960</td>\n",
       "      <td>8.210126</td>\n",
       "      <td>14.638862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.563313</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25006.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.271047</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50001.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.935661</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74889.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>58.299795</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.922656</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         smoke          alco        active      diabetes  \\\n",
       "count  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean   49972.419900      0.088129      0.053771      0.803729      0.250000   \n",
       "std    28851.302323      0.283484      0.225568      0.397179      0.433016   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    25006.750000      0.000000      0.000000      1.000000      0.000000   \n",
       "50%    50001.500000      0.000000      0.000000      1.000000      0.000000   \n",
       "75%    74889.250000      0.000000      0.000000      1.000000      0.250000   \n",
       "max    99999.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                age        height        weight  \n",
       "count  70000.000000  70000.000000  67682.000000  \n",
       "mean      53.164078    164.359229     74.193823  \n",
       "std        6.762960      8.210126     14.638862  \n",
       "min       29.563313     55.000000     10.000000  \n",
       "25%       48.271047    159.000000     64.000000  \n",
       "50%       53.935661    165.000000     71.000000  \n",
       "75%       58.299795    170.000000     82.000000  \n",
       "max       64.922656    250.000000    200.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>98630</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.854894</td>\n",
       "      <td>120</td>\n",
       "      <td>80.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>95141</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/90</td>\n",
       "      <td>0</td>\n",
       "      <td>51.553730</td>\n",
       "      <td>57</td>\n",
       "      <td>61.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>86203</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/60</td>\n",
       "      <td>0</td>\n",
       "      <td>56.717317</td>\n",
       "      <td>120</td>\n",
       "      <td>70.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>93223</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90/60</td>\n",
       "      <td>0</td>\n",
       "      <td>50.559890</td>\n",
       "      <td>99</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>86640</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>47.575633</td>\n",
       "      <td>110</td>\n",
       "      <td>70.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66645</th>\n",
       "      <td>34317</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>60.268309</td>\n",
       "      <td>120</td>\n",
       "      <td>90.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66851</th>\n",
       "      <td>65775</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>51.304586</td>\n",
       "      <td>111</td>\n",
       "      <td>69.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67907</th>\n",
       "      <td>20459</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>67</td>\n",
       "      <td>57.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68920</th>\n",
       "      <td>32895</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>56.369610</td>\n",
       "      <td>120</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69875</th>\n",
       "      <td>67631</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>63.783710</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "608    98630         low  medium      0     0       1   140/90         1   \n",
       "1450   95141         low     low      0     0       1   130/90         0   \n",
       "1904   86203      medium     low      0     0       0   120/60         0   \n",
       "2407   93223         low     low      0     0       1    90/60         0   \n",
       "3513   86640         low     low      0     0       1   110/70         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "66645  34317         low     low      0     0       0   120/80         1   \n",
       "66851  65775         low     low      0     0       1   130\\90         0   \n",
       "67907  20459         low     low      0     0       1   120\\90         0   \n",
       "68920  32895         low     low      0     0       0   120/80         0   \n",
       "69875  67631         low     low      0     0       1   120/80         0   \n",
       "\n",
       "             age  height  weight gender  \n",
       "608    55.854894     120    80.0      f  \n",
       "1450   51.553730      57    61.0      m  \n",
       "1904   56.717317     120    70.0      m  \n",
       "2407   50.559890      99    60.0   male  \n",
       "3513   47.575633     110    70.0      m  \n",
       "...          ...     ...     ...    ...  \n",
       "66645  60.268309     120    90.0   male  \n",
       "66851  51.304586     111    69.0   male  \n",
       "67907  60.000000      67    57.0   male  \n",
       "68920  56.369610     120    80.0   male  \n",
       "69875  63.783710      75     NaN      m  \n",
       "\n",
       "[92 rows x 12 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df['height'].between(130, 320)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F4BEC5DF08>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist('height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>86303</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/90</td>\n",
       "      <td>0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>149</td>\n",
       "      <td>85.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>97687</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.522245</td>\n",
       "      <td>144</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>92595</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130/100</td>\n",
       "      <td>1</td>\n",
       "      <td>56.306639</td>\n",
       "      <td>147</td>\n",
       "      <td>73.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>99535</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>49.325120</td>\n",
       "      <td>150</td>\n",
       "      <td>75.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>97468</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>43.668720</td>\n",
       "      <td>144</td>\n",
       "      <td>91.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69814</th>\n",
       "      <td>22138</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "      <td>49.856263</td>\n",
       "      <td>148</td>\n",
       "      <td>66.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69824</th>\n",
       "      <td>68452</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.511294</td>\n",
       "      <td>149</td>\n",
       "      <td>51.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69857</th>\n",
       "      <td>39333</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/90</td>\n",
       "      <td>0</td>\n",
       "      <td>62.310746</td>\n",
       "      <td>150</td>\n",
       "      <td>72.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69952</th>\n",
       "      <td>46550</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>150</td>\n",
       "      <td>65.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69960</th>\n",
       "      <td>50132</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "      <td>46.149213</td>\n",
       "      <td>145</td>\n",
       "      <td>83.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2496 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "62     86303         low     low      0     0       1   130/90         0   \n",
       "84     97687         low  medium      0     0       1   120/80         1   \n",
       "88     92595        high     low      0     0       0  130/100         1   \n",
       "113    99535         low     low      0     0       1   120/80         0   \n",
       "116    97468         low     low      0     0       0   120/80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69814  22138      medium     low      0     0       1   130/80         0   \n",
       "69824  68452         low    high      0     0       1   140\\80         0   \n",
       "69857  39333         low     low      0     0       1   130/90         0   \n",
       "69952  46550         low     low      0     0       1   140\\80         0   \n",
       "69960  50132      medium     low      0     0       1   130/80         0   \n",
       "\n",
       "             age  height  weight gender  \n",
       "62     57.000000     149    85.0      m  \n",
       "84     61.522245     144    80.0   male  \n",
       "88     56.306639     147    73.0      m  \n",
       "113    49.325120     150    75.0      m  \n",
       "116    43.668720     144    91.0   male  \n",
       "...          ...     ...     ...    ...  \n",
       "69814  49.856263     148    66.0      m  \n",
       "69824  53.511294     149    51.0      m  \n",
       "69857  62.310746     150    72.0      m  \n",
       "69952  63.000000     150    65.0      m  \n",
       "69960  46.149213     145    83.0      m  \n",
       "\n",
       "[2496 rows x 12 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['height'].between(130, 150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306         low  medium      0     0       0   120/80         1   \n",
       "1      86688         low     low      0     0       1   100\\70         0   \n",
       "2      98038         low     low      0     0       0  140/100         1   \n",
       "3      88694         low     low      0     0       1   120\\90         0   \n",
       "4      92856         low     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997         low     low      0     0       1   130\\80         0   \n",
       "69996   8296         low     low      0     0       1   130\\90         0   \n",
       "69997  17896         low     low      1     0       1   110/70         0   \n",
       "69998  40803      medium     low      1     0       1   152/90         1   \n",
       "69999  11945         low     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[70000 rows x 13 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BMI'] = df.weight*1e4/df.height**2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>86303</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/90</td>\n",
       "      <td>0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>149</td>\n",
       "      <td>85.0</td>\n",
       "      <td>m</td>\n",
       "      <td>38.286564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>97687</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.522245</td>\n",
       "      <td>144</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>38.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>92595</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130/100</td>\n",
       "      <td>1</td>\n",
       "      <td>56.306639</td>\n",
       "      <td>147</td>\n",
       "      <td>73.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.782220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>97468</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>43.668720</td>\n",
       "      <td>144</td>\n",
       "      <td>91.0</td>\n",
       "      <td>male</td>\n",
       "      <td>43.885031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>94810</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>49.253936</td>\n",
       "      <td>145</td>\n",
       "      <td>69.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.818074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69644</th>\n",
       "      <td>48991</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140/90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.150582</td>\n",
       "      <td>140</td>\n",
       "      <td>80.0</td>\n",
       "      <td>m</td>\n",
       "      <td>40.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69676</th>\n",
       "      <td>41019</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/90</td>\n",
       "      <td>0</td>\n",
       "      <td>55.479808</td>\n",
       "      <td>149</td>\n",
       "      <td>68.0</td>\n",
       "      <td>male</td>\n",
       "      <td>30.629251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69681</th>\n",
       "      <td>71696</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>47.904175</td>\n",
       "      <td>144</td>\n",
       "      <td>83.0</td>\n",
       "      <td>m</td>\n",
       "      <td>40.027006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69814</th>\n",
       "      <td>22138</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "      <td>49.856263</td>\n",
       "      <td>148</td>\n",
       "      <td>66.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.131483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69960</th>\n",
       "      <td>50132</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "      <td>46.149213</td>\n",
       "      <td>145</td>\n",
       "      <td>83.0</td>\n",
       "      <td>m</td>\n",
       "      <td>39.476813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "62     86303         low     low      0     0       1   130/90         0   \n",
       "84     97687         low  medium      0     0       1   120/80         1   \n",
       "88     92595        high     low      0     0       0  130/100         1   \n",
       "116    97468         low     low      0     0       0   120/80         0   \n",
       "152    94810      medium     low      0     0       0   100\\90         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69644  48991         low     low      0     0       1   140/90         0   \n",
       "69676  41019         low     low      0     0       1   120/90         0   \n",
       "69681  71696         low     low      0     0       1   120/80         0   \n",
       "69814  22138      medium     low      0     0       1   130/80         0   \n",
       "69960  50132      medium     low      0     0       1   130/80         0   \n",
       "\n",
       "             age  height  weight gender        BMI  \n",
       "62     57.000000     149    85.0      m  38.286564  \n",
       "84     61.522245     144    80.0   male  38.580247  \n",
       "88     56.306639     147    73.0      m  33.782220  \n",
       "116    43.668720     144    91.0   male  43.885031  \n",
       "152    49.253936     145    69.0      m  32.818074  \n",
       "...          ...     ...     ...    ...        ...  \n",
       "69644  60.150582     140    80.0      m  40.816327  \n",
       "69676  55.479808     149    68.0   male  30.629251  \n",
       "69681  47.904175     144    83.0      m  40.027006  \n",
       "69814  49.856263     148    66.0      m  30.131483  \n",
       "69960  46.149213     145    83.0      m  39.476813  \n",
       "\n",
       "[747 rows x 13 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('height < 150 and BMI > 30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69848 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306         low  medium      0     0       0   120/80         1   \n",
       "1      86688         low     low      0     0       1   100\\70         0   \n",
       "2      98038         low     low      0     0       0  140/100         1   \n",
       "3      88694         low     low      0     0       1   120\\90         0   \n",
       "4      92856         low     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997         low     low      0     0       1   130\\80         0   \n",
       "69996   8296         low     low      0     0       1   130\\90         0   \n",
       "69997  17896         low     low      1     0       1   110/70         0   \n",
       "69998  40803      medium     low      1     0       1   152/90         1   \n",
       "69999  11945         low     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[69848 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df.height < 140].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31718</th>\n",
       "      <td>9223</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>58.097194</td>\n",
       "      <td>250</td>\n",
       "      <td>86.0</td>\n",
       "      <td>m</td>\n",
       "      <td>13.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id cholesterol gluc  smoke  alco  active pressure  diabetes  \\\n",
       "31718  9223        high  low      0     0       1  140/100         1   \n",
       "\n",
       "             age  height  weight gender    BMI  \n",
       "31718  58.097194     250    86.0      m  13.76  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['height'] > 220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.height > 220].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69847 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306         low  medium      0     0       0   120/80         1   \n",
       "1      86688         low     low      0     0       1   100\\70         0   \n",
       "2      98038         low     low      0     0       0  140/100         1   \n",
       "3      88694         low     low      0     0       1   120\\90         0   \n",
       "4      92856         low     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997         low     low      0     0       1   130\\80         0   \n",
       "69996   8296         low     low      0     0       1   130\\90         0   \n",
       "69997  17896         low     low      1     0       1   110/70         0   \n",
       "69998  40803      medium     low      1     0       1   152/90         1   \n",
       "69999  11945         low     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[69847 rows x 13 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>85931</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>59.835729</td>\n",
       "      <td>162</td>\n",
       "      <td>21.0</td>\n",
       "      <td>male</td>\n",
       "      <td>8.001829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8725</th>\n",
       "      <td>86650</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>171</td>\n",
       "      <td>29.0</td>\n",
       "      <td>m</td>\n",
       "      <td>9.917581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>92896</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/1000</td>\n",
       "      <td>1</td>\n",
       "      <td>62.116359</td>\n",
       "      <td>145</td>\n",
       "      <td>33.0</td>\n",
       "      <td>m</td>\n",
       "      <td>15.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10652</th>\n",
       "      <td>50443</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>146</td>\n",
       "      <td>32.0</td>\n",
       "      <td>male</td>\n",
       "      <td>15.012197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25370</th>\n",
       "      <td>21040</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100/70</td>\n",
       "      <td>0</td>\n",
       "      <td>62.047912</td>\n",
       "      <td>143</td>\n",
       "      <td>34.0</td>\n",
       "      <td>male</td>\n",
       "      <td>16.626730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28197</th>\n",
       "      <td>59853</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103/61</td>\n",
       "      <td>0</td>\n",
       "      <td>58.409309</td>\n",
       "      <td>143</td>\n",
       "      <td>30.0</td>\n",
       "      <td>male</td>\n",
       "      <td>14.670644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28391</th>\n",
       "      <td>24167</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>150/90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.288159</td>\n",
       "      <td>170</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>10.726644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34422</th>\n",
       "      <td>42156</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>55.874059</td>\n",
       "      <td>177</td>\n",
       "      <td>22.0</td>\n",
       "      <td>f</td>\n",
       "      <td>7.022248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39029</th>\n",
       "      <td>31439</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100/70</td>\n",
       "      <td>0</td>\n",
       "      <td>42.050650</td>\n",
       "      <td>146</td>\n",
       "      <td>32.0</td>\n",
       "      <td>m</td>\n",
       "      <td>15.012197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41534</th>\n",
       "      <td>38312</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>63.748118</td>\n",
       "      <td>157</td>\n",
       "      <td>23.0</td>\n",
       "      <td>m</td>\n",
       "      <td>9.331007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46782</th>\n",
       "      <td>47872</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>1</td>\n",
       "      <td>57.716632</td>\n",
       "      <td>153</td>\n",
       "      <td>34.0</td>\n",
       "      <td>m</td>\n",
       "      <td>14.524328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47342</th>\n",
       "      <td>26503</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "      <td>49.664613</td>\n",
       "      <td>160</td>\n",
       "      <td>30.0</td>\n",
       "      <td>m</td>\n",
       "      <td>11.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47768</th>\n",
       "      <td>82567</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180/1100</td>\n",
       "      <td>1</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>10.0</td>\n",
       "      <td>female</td>\n",
       "      <td>3.673095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49480</th>\n",
       "      <td>79686</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140/90</td>\n",
       "      <td>0</td>\n",
       "      <td>63.983573</td>\n",
       "      <td>152</td>\n",
       "      <td>34.0</td>\n",
       "      <td>male</td>\n",
       "      <td>14.716066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58343</th>\n",
       "      <td>48318</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130/90</td>\n",
       "      <td>0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>178</td>\n",
       "      <td>11.0</td>\n",
       "      <td>f</td>\n",
       "      <td>3.471784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58538</th>\n",
       "      <td>68667</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100/60</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>143</td>\n",
       "      <td>33.0</td>\n",
       "      <td>m</td>\n",
       "      <td>16.137708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67269</th>\n",
       "      <td>54851</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>154</td>\n",
       "      <td>32.0</td>\n",
       "      <td>m</td>\n",
       "      <td>13.493001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active  pressure  diabetes  \\\n",
       "784    85931      medium     low      0     0       1    120/80         0   \n",
       "8725   86650      medium     low      0     0       1    110/70         0   \n",
       "9709   92896      medium     low      0     0       1  130/1000         1   \n",
       "10652  50443         low  medium      0     0       0    130/80         0   \n",
       "25370  21040         low     low      0     0       1    100/70         0   \n",
       "28197  59853      medium     low      0     0       1    103/61         0   \n",
       "28391  24167      medium  medium      0     0       1    150/90         0   \n",
       "34422  42156         low     low      1     1       1    120/80         0   \n",
       "39029  31439         low     low      0     0       0    100/70         0   \n",
       "41534  38312         low     low      0     0       1    110\\80         0   \n",
       "46782  47872        high    high      0     0       1    110/70         1   \n",
       "47342  26503         low     low      0     0       1    120/80         0   \n",
       "47768  82567      medium  medium      0     0       1  180/1100         1   \n",
       "49480  79686         low     low      0     0       1    140/90         0   \n",
       "58343  48318         low     low      0     0       1    130/90         0   \n",
       "58538  68667         low     low      0     0       1    100/60         0   \n",
       "67269  54851         low     low      0     0       1    110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "784    59.835729     162    21.0    male   8.001829  \n",
       "8725   51.000000     171    29.0       m   9.917581  \n",
       "9709   62.116359     145    33.0       m  15.695600  \n",
       "10652  54.000000     146    32.0    male  15.012197  \n",
       "25370  62.047912     143    34.0    male  16.626730  \n",
       "28197  58.409309     143    30.0    male  14.670644  \n",
       "28391  47.288159     170    31.0       f  10.726644  \n",
       "34422  55.874059     177    22.0       f   7.022248  \n",
       "39029  42.050650     146    32.0       m  15.012197  \n",
       "41534  63.748118     157    23.0       m   9.331007  \n",
       "46782  57.716632     153    34.0       m  14.524328  \n",
       "47342  49.664613     160    30.0       m  11.718750  \n",
       "47768  51.000000     165    10.0  female   3.673095  \n",
       "49480  63.983573     152    34.0    male  14.716066  \n",
       "58343  59.000000     178    11.0       f   3.471784  \n",
       "58538  52.000000     143    33.0       m  16.137708  \n",
       "67269  59.000000     154    32.0       m  13.493001  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['weight'] < 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306         low  medium      0     0       0   120/80         1   \n",
       "1      86688         low     low      0     0       1   100\\70         0   \n",
       "2      98038         low     low      0     0       0  140/100         1   \n",
       "3      88694         low     low      0     0       1   120\\90         0   \n",
       "4      92856         low     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997         low     low      0     0       1   130\\80         0   \n",
       "69996   8296         low     low      0     0       1   130\\90         0   \n",
       "69997  17896         low     low      1     0       1   110/70         0   \n",
       "69998  40803      medium     low      1     0       1   152/90         1   \n",
       "69999  11945         low     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[69830 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df.weight < 35].index, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>69830.000000</td>\n",
       "      <td>67516.000000</td>\n",
       "      <td>67516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49972.836875</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.053816</td>\n",
       "      <td>0.803838</td>\n",
       "      <td>0.250050</td>\n",
       "      <td>53.164161</td>\n",
       "      <td>164.466333</td>\n",
       "      <td>74.203177</td>\n",
       "      <td>27.472465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28853.192320</td>\n",
       "      <td>0.283587</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>0.397095</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>6.761498</td>\n",
       "      <td>7.811688</td>\n",
       "      <td>14.576476</td>\n",
       "      <td>5.279729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.563313</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.254473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25012.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.273785</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>23.833005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50006.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.935661</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>26.298488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74888.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58.299110</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>30.346074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.922656</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>69.827553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         smoke          alco        active      diabetes  \\\n",
       "count  69830.000000  69830.000000  69830.000000  69830.000000  69830.000000   \n",
       "mean   49972.836875      0.088200      0.053816      0.803838      0.250050   \n",
       "std    28853.192320      0.283587      0.225657      0.397095      0.433045   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    25012.250000      0.000000      0.000000      1.000000      0.000000   \n",
       "50%    50006.500000      0.000000      0.000000      1.000000      0.000000   \n",
       "75%    74888.250000      0.000000      0.000000      1.000000      1.000000   \n",
       "max    99999.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                age        height        weight           BMI  \n",
       "count  69830.000000  69830.000000  67516.000000  67516.000000  \n",
       "mean      53.164161    164.466333     74.203177     27.472465  \n",
       "std        6.761498      7.811688     14.576476      5.279729  \n",
       "min       29.563313    140.000000     35.000000     12.254473  \n",
       "25%       48.273785    159.000000     65.000000     23.833005  \n",
       "50%       53.935661    165.000000     71.000000     26.298488  \n",
       "75%       58.299110    170.000000     82.000000     30.346074  \n",
       "max       64.922656    207.000000    200.000000     69.827553  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol    gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306           0  medium      0     0       0   120/80         1   \n",
       "1      86688           0     low      0     0       1   100\\70         0   \n",
       "2      98038           0     low      0     0       0  140/100         1   \n",
       "3      88694           0     low      0     0       1   120\\90         0   \n",
       "4      92856           0     low      0     0       0   130\\80         0   \n",
       "...      ...         ...     ...    ...   ...     ...      ...       ...   \n",
       "69995  55997           0     low      0     0       1   130\\80         0   \n",
       "69996   8296           0     low      0     0       1   130\\90         0   \n",
       "69997  17896           0     low      1     0       1   110/70         0   \n",
       "69998  40803         0.5     low      1     0       1   152/90         1   \n",
       "69999  11945           0     low      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[69830 rows x 13 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'low':0, 'medium':0.5, 'high':1}\n",
    "for k in dic.keys():\n",
    "    df.loc[df['cholesterol']==k,'cholesterol'] = dic[k]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>f</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>m</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>m</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>m</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>female</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>m</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>f</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>m</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306           0  0.5      0     0       0   120/80         1   \n",
       "1      86688           0    0      0     0       1   100\\70         0   \n",
       "2      98038           0    0      0     0       0  140/100         1   \n",
       "3      88694           0    0      0     0       1   120\\90         0   \n",
       "4      92856           0    0      0     0       0   130\\80         0   \n",
       "...      ...         ...  ...    ...   ...     ...      ...       ...   \n",
       "69995  55997           0    0      0     0       1   130\\80         0   \n",
       "69996   8296           0    0      0     0       1   130\\90         0   \n",
       "69997  17896           0    0      1     0       1   110/70         0   \n",
       "69998  40803         0.5    0      1     0       1   152/90         1   \n",
       "69999  11945           0    0      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       f  33.057851  \n",
       "1      39.923340     162    50.0       m  19.051974  \n",
       "2      64.035592     156    74.0       m  30.407627  \n",
       "3      47.000000     162    89.0       m  33.912513  \n",
       "4      50.343600     162    72.0       f  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0  female  32.508522  \n",
       "69996  60.032854     160    82.0       m  32.031250  \n",
       "69997  55.477070     158    60.0    male  24.034610  \n",
       "69998  55.969884     175    85.0       f  27.755102  \n",
       "69999  48.000000     169    71.0       m  24.859074  \n",
       "\n",
       "[69830 rows x 13 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in dic.keys():\n",
    "    df.loc[df['gluc']==k,'gluc'] = dic[k]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152/90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110/60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306           0  0.5      0     0       0   120/80         1   \n",
       "1      86688           0    0      0     0       1   100\\70         0   \n",
       "2      98038           0    0      0     0       0  140/100         1   \n",
       "3      88694           0    0      0     0       1   120\\90         0   \n",
       "4      92856           0    0      0     0       0   130\\80         0   \n",
       "...      ...         ...  ...    ...   ...     ...      ...       ...   \n",
       "69995  55997           0    0      0     0       1   130\\80         0   \n",
       "69996   8296           0    0      0     0       1   130\\90         0   \n",
       "69997  17896           0    0      1     0       1   110/70         0   \n",
       "69998  40803         0.5    0      1     0       1   152/90         1   \n",
       "69999  11945           0    0      0     0       1   110/60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       0  33.057851  \n",
       "1      39.923340     162    50.0       1  19.051974  \n",
       "2      64.035592     156    74.0       1  30.407627  \n",
       "3      47.000000     162    89.0       1  33.912513  \n",
       "4      50.343600     162    72.0       0  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0       0  32.508522  \n",
       "69996  60.032854     160    82.0       1  32.031250  \n",
       "69997  55.477070     158    60.0       1  24.034610  \n",
       "69998  55.969884     175    85.0       0  27.755102  \n",
       "69999  48.000000     169    71.0       1  24.859074  \n",
       "\n",
       "[69830 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.gender = (df.gender.str.startswith('m'))*1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['gender'].astype(str).str.startswith('f'), 'gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['gender'].astype(str).str.startswith('m'), 'gender'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "cholesterol     object\n",
       "gluc            object\n",
       "smoke            int64\n",
       "alco             int64\n",
       "active           int64\n",
       "pressure        object\n",
       "diabetes         int64\n",
       "age            float64\n",
       "height           int64\n",
       "weight         float64\n",
       "gender           int32\n",
       "BMI            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.gluc = df.gluc.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cholesterol = df.cholesterol.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.gender = df.gender.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.051974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140\\100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.407627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.508522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.034610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152\\90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.755102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110\\60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.859074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306           0  0.5      0     0       0   120\\80         1   \n",
       "1      86688           0    0      0     0       1   100\\70         0   \n",
       "2      98038           0    0      0     0       0  140\\100         1   \n",
       "3      88694           0    0      0     0       1   120\\90         0   \n",
       "4      92856           0    0      0     0       0   130\\80         0   \n",
       "...      ...         ...  ...    ...   ...     ...      ...       ...   \n",
       "69995  55997           0    0      0     0       1   130\\80         0   \n",
       "69996   8296           0    0      0     0       1   130\\90         0   \n",
       "69997  17896           0    0      1     0       1   110\\70         0   \n",
       "69998  40803         0.5    0      1     0       1   152\\90         1   \n",
       "69999  11945           0    0      0     0       1   110\\60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI  \n",
       "0      61.000000     165    90.0       0  33.057851  \n",
       "1      39.923340     162    50.0       1  19.051974  \n",
       "2      64.035592     156    74.0       1  30.407627  \n",
       "3      47.000000     162    89.0       1  33.912513  \n",
       "4      50.343600     162    72.0       0  27.434842  \n",
       "...          ...     ...     ...     ...        ...  \n",
       "69995  53.377139     178   103.0       0  32.508522  \n",
       "69996  60.032854     160    82.0       1  32.031250  \n",
       "69997  55.477070     158    60.0       1  24.034610  \n",
       "69998  55.969884     175    85.0       0  27.755102  \n",
       "69999  48.000000     169    71.0       1  24.859074  \n",
       "\n",
       "[69830 rows x 13 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pressure = df.pressure.str.replace('/', '\\\\')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140\\100</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130\\90</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110\\70</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152\\90</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110\\60</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol gluc  smoke  alco  active pressure  diabetes  \\\n",
       "0      95306           0  0.5      0     0       0   120\\80         1   \n",
       "1      86688           0    0      0     0       1   100\\70         0   \n",
       "2      98038           0    0      0     0       0  140\\100         1   \n",
       "3      88694           0    0      0     0       1   120\\90         0   \n",
       "4      92856           0    0      0     0       0   130\\80         0   \n",
       "...      ...         ...  ...    ...   ...     ...      ...       ...   \n",
       "69995  55997           0    0      0     0       1   130\\80         0   \n",
       "69996   8296           0    0      0     0       1   130\\90         0   \n",
       "69997  17896           0    0      1     0       1   110\\70         0   \n",
       "69998  40803         0.5    0      1     0       1   152\\90         1   \n",
       "69999  11945           0    0      0     0       1   110\\60         0   \n",
       "\n",
       "             age  height  weight  gender        BMI diastolic systolic  \n",
       "0      61.000000     165    90.0       0  33.057851       120       80  \n",
       "1      39.923340     162    50.0       1  19.051974       100       70  \n",
       "2      64.035592     156    74.0       1  30.407627       140      100  \n",
       "3      47.000000     162    89.0       1  33.912513       120       90  \n",
       "4      50.343600     162    72.0       0  27.434842       130       80  \n",
       "...          ...     ...     ...     ...        ...       ...      ...  \n",
       "69995  53.377139     178   103.0       0  32.508522       130       80  \n",
       "69996  60.032854     160    82.0       1  32.031250       130       90  \n",
       "69997  55.477070     158    60.0       1  24.034610       110       70  \n",
       "69998  55.969884     175    85.0       0  27.755102       152       90  \n",
       "69999  48.000000     169    71.0       1  24.859074       110       60  \n",
       "\n",
       "[69830 rows x 15 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['diastolic','systolic']] = df.pressure.str.split('\\\\', expand = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69830 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id cholesterol gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306           0  0.5      0     0       0         1  61.000000   \n",
       "1      86688           0    0      0     0       1         0  39.923340   \n",
       "2      98038           0    0      0     0       0         1  64.035592   \n",
       "3      88694           0    0      0     0       1         0  47.000000   \n",
       "4      92856           0    0      0     0       0         0  50.343600   \n",
       "...      ...         ...  ...    ...   ...     ...       ...        ...   \n",
       "69995  55997           0    0      0     0       1         0  53.377139   \n",
       "69996   8296           0    0      0     0       1         0  60.032854   \n",
       "69997  17896           0    0      1     0       1         0  55.477070   \n",
       "69998  40803         0.5    0      1     0       1         1  55.969884   \n",
       "69999  11945           0    0      0     0       1         0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI diastolic systolic  \n",
       "0         165    90.0       0  33.057851       120       80  \n",
       "1         162    50.0       1  19.051974       100       70  \n",
       "2         156    74.0       1  30.407627       140      100  \n",
       "3         162    89.0       1  33.912513       120       90  \n",
       "4         162    72.0       0  27.434842       130       80  \n",
       "...       ...     ...     ...        ...       ...      ...  \n",
       "69995     178   103.0       0  32.508522       130       80  \n",
       "69996     160    82.0       1  32.031250       130       90  \n",
       "69997     158    60.0       1  24.034610       110       70  \n",
       "69998     175    85.0       0  27.755102       152       90  \n",
       "69999     169    71.0       1  24.859074       110       60  \n",
       "\n",
       "[69830 rows x 14 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['pressure']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             float64\n",
       "cholesterol    float64\n",
       "gluc           float64\n",
       "smoke          float64\n",
       "alco           float64\n",
       "active         float64\n",
       "diabetes       float64\n",
       "age            float64\n",
       "height         float64\n",
       "weight         float64\n",
       "gender         float64\n",
       "BMI            float64\n",
       "diastolic      float64\n",
       "systolic       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>95849.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.737166</td>\n",
       "      <td>158.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.233456</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>91725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>174.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.753864</td>\n",
       "      <td>90.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>91584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.403833</td>\n",
       "      <td>160.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.656250</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>86208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.987680</td>\n",
       "      <td>161.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.918830</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>91198.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.779603</td>\n",
       "      <td>186.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.459822</td>\n",
       "      <td>100.0</td>\n",
       "      <td>901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69802</th>\n",
       "      <td>66548.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.196441</td>\n",
       "      <td>165.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.344353</td>\n",
       "      <td>12.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69876</th>\n",
       "      <td>40901.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.564682</td>\n",
       "      <td>167.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.497867</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69965</th>\n",
       "      <td>84457.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.180250</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69986</th>\n",
       "      <td>40383.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.403149</td>\n",
       "      <td>153.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.631167</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69993</th>\n",
       "      <td>76440.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.969884</td>\n",
       "      <td>160.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "103    95849.0          0.0   0.0    0.0   0.0     1.0       0.0  43.737166   \n",
       "178    91725.0          0.0   0.0    1.0   1.0     1.0       1.0  51.000000   \n",
       "194    91584.0          0.0   0.0    0.0   0.0     0.0       1.0  46.403833   \n",
       "201    86208.0          0.0   0.0    0.0   0.0     1.0       1.0  53.987680   \n",
       "224    91198.0          0.5   0.5    0.0   0.0     1.0       1.0  49.779603   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69802  66548.0          0.0   0.0    0.0   0.0     1.0       0.0  45.196441   \n",
       "69876  40901.0          0.5   0.0    0.0   0.0     1.0       1.0  43.564682   \n",
       "69965  84457.0          0.5   0.0    0.0   0.0     1.0       1.0  59.000000   \n",
       "69986  40383.0          1.0   0.0    0.0   0.0     1.0       1.0  55.403149   \n",
       "69993  76440.0          0.0   0.0    0.0   0.0     1.0       1.0  47.969884   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "103     158.0    58.0     1.0  23.233456       11.0      70.0  \n",
       "178     174.0    81.0     0.0  26.753864       90.0     130.0  \n",
       "194     160.0    58.0     1.0  22.656250      120.0    1000.0  \n",
       "201     161.0    62.0     1.0  23.918830      150.0    1000.0  \n",
       "224     186.0    95.0     0.0  27.459822      100.0     901.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69802   165.0    69.0     1.0  25.344353       12.0      80.0  \n",
       "69876   167.0    99.0     1.0  35.497867      180.0    1100.0  \n",
       "69965   164.0   100.0     1.0  37.180250      140.0    1000.0  \n",
       "69986   153.0    60.0     1.0  25.631167      136.0    1000.0  \n",
       "69993   160.0   108.0     1.0  42.187500      160.0    1000.0  \n",
       "\n",
       "[1229 rows x 14 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.systolic > df.diastolic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>91584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.403833</td>\n",
       "      <td>160.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.656250</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>86208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.987680</td>\n",
       "      <td>161.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.918830</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>91198.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.779603</td>\n",
       "      <td>186.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.459822</td>\n",
       "      <td>100.0</td>\n",
       "      <td>901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>90106.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.863107</td>\n",
       "      <td>178.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.714556</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>94089.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.062971</td>\n",
       "      <td>155.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.212279</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69782</th>\n",
       "      <td>20113.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.600274</td>\n",
       "      <td>177.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.088991</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69876</th>\n",
       "      <td>40901.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.564682</td>\n",
       "      <td>167.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.497867</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69965</th>\n",
       "      <td>84457.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.180250</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69986</th>\n",
       "      <td>40383.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.403149</td>\n",
       "      <td>153.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.631167</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69993</th>\n",
       "      <td>76440.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.969884</td>\n",
       "      <td>160.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "194    91584.0          0.0   0.0    0.0   0.0     0.0       1.0  46.403833   \n",
       "201    86208.0          0.0   0.0    0.0   0.0     1.0       1.0  53.987680   \n",
       "224    91198.0          0.5   0.5    0.0   0.0     1.0       1.0  49.779603   \n",
       "308    90106.0          1.0   0.0    0.0   0.0     1.0       1.0  59.863107   \n",
       "355    94089.0          0.5   0.5    0.0   0.0     1.0       1.0  64.062971   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69782  20113.0          0.0   0.0    0.0   0.0     1.0       1.0  59.600274   \n",
       "69876  40901.0          0.5   0.0    0.0   0.0     1.0       1.0  43.564682   \n",
       "69965  84457.0          0.5   0.0    0.0   0.0     1.0       1.0  59.000000   \n",
       "69986  40383.0          1.0   0.0    0.0   0.0     1.0       1.0  55.403149   \n",
       "69993  76440.0          0.0   0.0    0.0   0.0     1.0       1.0  47.969884   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "194     160.0    58.0     1.0  22.656250      120.0    1000.0  \n",
       "201     161.0    62.0     1.0  23.918830      150.0    1000.0  \n",
       "224     186.0    95.0     0.0  27.459822      100.0     901.0  \n",
       "308     178.0   129.0     0.0  40.714556      140.0    1110.0  \n",
       "355     155.0    87.0     1.0  36.212279      160.0    1200.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69782   177.0    88.0     0.0  28.088991      181.0    1177.0  \n",
       "69876   167.0    99.0     1.0  35.497867      180.0    1100.0  \n",
       "69965   164.0   100.0     1.0  37.180250      140.0    1000.0  \n",
       "69986   153.0    60.0     1.0  25.631167      136.0    1000.0  \n",
       "69993   160.0   108.0     1.0  42.187500      160.0    1000.0  \n",
       "\n",
       "[991 rows x 14 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('systolic > diastolic and systolic > 130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>49972.836875</td>\n",
       "      <td>28853.192320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25012.250000</td>\n",
       "      <td>50006.500000</td>\n",
       "      <td>74888.250000</td>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.183539</td>\n",
       "      <td>0.340225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluc</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.113261</td>\n",
       "      <td>0.286175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoke</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.283587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alco</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.053816</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.803838</td>\n",
       "      <td>0.397095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.250050</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>53.164161</td>\n",
       "      <td>6.761498</td>\n",
       "      <td>29.563313</td>\n",
       "      <td>48.273785</td>\n",
       "      <td>53.935661</td>\n",
       "      <td>58.299110</td>\n",
       "      <td>64.922656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>164.466333</td>\n",
       "      <td>7.811688</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>67516.0</td>\n",
       "      <td>74.203177</td>\n",
       "      <td>14.576476</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>0.650308</td>\n",
       "      <td>0.476876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>67516.0</td>\n",
       "      <td>27.472465</td>\n",
       "      <td>5.279729</td>\n",
       "      <td>12.254473</td>\n",
       "      <td>23.833005</td>\n",
       "      <td>26.298488</td>\n",
       "      <td>30.346074</td>\n",
       "      <td>69.827553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>128.829873</td>\n",
       "      <td>154.194769</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>16020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic</th>\n",
       "      <td>69830.0</td>\n",
       "      <td>96.632493</td>\n",
       "      <td>188.598765</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>11000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count          mean           std         min           25%  \\\n",
       "id           69830.0  49972.836875  28853.192320    0.000000  25012.250000   \n",
       "cholesterol  69830.0      0.183539      0.340225    0.000000      0.000000   \n",
       "gluc         69830.0      0.113261      0.286175    0.000000      0.000000   \n",
       "smoke        69830.0      0.088200      0.283587    0.000000      0.000000   \n",
       "alco         69830.0      0.053816      0.225657    0.000000      0.000000   \n",
       "active       69830.0      0.803838      0.397095    0.000000      1.000000   \n",
       "diabetes     69830.0      0.250050      0.433045    0.000000      0.000000   \n",
       "age          69830.0     53.164161      6.761498   29.563313     48.273785   \n",
       "height       69830.0    164.466333      7.811688  140.000000    159.000000   \n",
       "weight       67516.0     74.203177     14.576476   35.000000     65.000000   \n",
       "gender       69830.0      0.650308      0.476876    0.000000      0.000000   \n",
       "BMI          67516.0     27.472465      5.279729   12.254473     23.833005   \n",
       "diastolic    69830.0    128.829873    154.194769 -150.000000    120.000000   \n",
       "systolic     69830.0     96.632493    188.598765  -70.000000     80.000000   \n",
       "\n",
       "                      50%           75%           max  \n",
       "id           50006.500000  74888.250000  99999.000000  \n",
       "cholesterol      0.000000      0.500000      1.000000  \n",
       "gluc             0.000000      0.000000      1.000000  \n",
       "smoke            0.000000      0.000000      1.000000  \n",
       "alco             0.000000      0.000000      1.000000  \n",
       "active           1.000000      1.000000      1.000000  \n",
       "diabetes         0.000000      1.000000      1.000000  \n",
       "age             53.935661     58.299110     64.922656  \n",
       "height         165.000000    170.000000    207.000000  \n",
       "weight          71.000000     82.000000    200.000000  \n",
       "gender           1.000000      1.000000      1.000000  \n",
       "BMI             26.298488     30.346074     69.827553  \n",
       "diastolic      120.000000    140.000000  16020.000000  \n",
       "systolic        80.000000     90.000000  11000.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>87036.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.538672</td>\n",
       "      <td>176.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>95562.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.357974</td>\n",
       "      <td>160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>87253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.861739</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>88146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.097194</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>86292.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.843943</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69808</th>\n",
       "      <td>25383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.637919</td>\n",
       "      <td>164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69827</th>\n",
       "      <td>22870.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.642710</td>\n",
       "      <td>165.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69886</th>\n",
       "      <td>67393.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.635866</td>\n",
       "      <td>174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69925</th>\n",
       "      <td>48346.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.916496</td>\n",
       "      <td>169.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69954</th>\n",
       "      <td>16707.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.376454</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2314 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "37     87036.0          0.0   0.0    0.0   0.0     1.0       0.0  57.538672   \n",
       "71     95562.0          0.0   0.0    0.0   0.0     1.0       0.0  41.357974   \n",
       "73     87253.0          0.0   0.0    0.0   0.0     0.0       0.0  61.861739   \n",
       "148    88146.0          1.0   0.0    0.0   0.0     1.0       0.0  54.097194   \n",
       "197    86292.0          0.0   0.0    0.0   0.0     0.0       0.0  59.843943   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69808  25383.0          0.0   0.0    0.0   0.0     1.0       0.0  64.637919   \n",
       "69827  22870.0          0.5   0.5    0.0   0.0     0.0       1.0  49.642710   \n",
       "69886  67393.0          0.0   0.0    0.0   0.0     1.0       0.0  51.635866   \n",
       "69925  48346.0          0.0   0.0    0.0   0.0     0.0       1.0  61.916496   \n",
       "69954  16707.0          0.0   0.0    0.0   0.0     1.0       0.0  54.376454   \n",
       "\n",
       "       height  weight  gender  BMI  diastolic  systolic  \n",
       "37      176.0     NaN     0.0  NaN      120.0      80.0  \n",
       "71      160.0     NaN     1.0  NaN      110.0      70.0  \n",
       "73      168.0     NaN     1.0  NaN      115.0      70.0  \n",
       "148     162.0     NaN     1.0  NaN      140.0      80.0  \n",
       "197     150.0     NaN     1.0  NaN      120.0      80.0  \n",
       "...       ...     ...     ...  ...        ...       ...  \n",
       "69808   164.0     NaN     1.0  NaN      120.0      80.0  \n",
       "69827   165.0     NaN     1.0  NaN      110.0      80.0  \n",
       "69886   174.0     NaN     1.0  NaN      120.0      80.0  \n",
       "69925   169.0     NaN     0.0  NaN      160.0     100.0  \n",
       "69954   162.0     NaN     1.0  NaN      110.0      70.0  \n",
       "\n",
       "[2314 rows x 14 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.weight != df.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>87036.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.538672</td>\n",
       "      <td>176.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>95562.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.357974</td>\n",
       "      <td>160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>87253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.861739</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>88146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.097194</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>86292.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.843943</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69808</th>\n",
       "      <td>25383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.637919</td>\n",
       "      <td>164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69827</th>\n",
       "      <td>22870.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.642710</td>\n",
       "      <td>165.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69886</th>\n",
       "      <td>67393.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.635866</td>\n",
       "      <td>174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69925</th>\n",
       "      <td>48346.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.916496</td>\n",
       "      <td>169.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69954</th>\n",
       "      <td>16707.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.376454</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2314 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "37     87036.0          0.0   0.0    0.0   0.0     1.0       0.0  57.538672   \n",
       "71     95562.0          0.0   0.0    0.0   0.0     1.0       0.0  41.357974   \n",
       "73     87253.0          0.0   0.0    0.0   0.0     0.0       0.0  61.861739   \n",
       "148    88146.0          1.0   0.0    0.0   0.0     1.0       0.0  54.097194   \n",
       "197    86292.0          0.0   0.0    0.0   0.0     0.0       0.0  59.843943   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69808  25383.0          0.0   0.0    0.0   0.0     1.0       0.0  64.637919   \n",
       "69827  22870.0          0.5   0.5    0.0   0.0     0.0       1.0  49.642710   \n",
       "69886  67393.0          0.0   0.0    0.0   0.0     1.0       0.0  51.635866   \n",
       "69925  48346.0          0.0   0.0    0.0   0.0     0.0       1.0  61.916496   \n",
       "69954  16707.0          0.0   0.0    0.0   0.0     1.0       0.0  54.376454   \n",
       "\n",
       "       height  weight  gender  BMI  diastolic  systolic  \n",
       "37      176.0     NaN     0.0  NaN      120.0      80.0  \n",
       "71      160.0     NaN     1.0  NaN      110.0      70.0  \n",
       "73      168.0     NaN     1.0  NaN      115.0      70.0  \n",
       "148     162.0     NaN     1.0  NaN      140.0      80.0  \n",
       "197     150.0     NaN     1.0  NaN      120.0      80.0  \n",
       "...       ...     ...     ...  ...        ...       ...  \n",
       "69808   164.0     NaN     1.0  NaN      120.0      80.0  \n",
       "69827   165.0     NaN     1.0  NaN      110.0      80.0  \n",
       "69886   174.0     NaN     1.0  NaN      120.0      80.0  \n",
       "69925   169.0     NaN     0.0  NaN      160.0     100.0  \n",
       "69954   162.0     NaN     1.0  NaN      110.0      70.0  \n",
       "\n",
       "[2314 rows x 14 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.BMI != df.BMI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67516 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67516 rows x 14 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 67516 entries, 0 to 69999\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           67516 non-null  float64\n",
      " 1   cholesterol  67516 non-null  float64\n",
      " 2   gluc         67516 non-null  float64\n",
      " 3   smoke        67516 non-null  float64\n",
      " 4   alco         67516 non-null  float64\n",
      " 5   active       67516 non-null  float64\n",
      " 6   diabetes     67516 non-null  float64\n",
      " 7   age          67516 non-null  float64\n",
      " 8   height       67516 non-null  float64\n",
      " 9   weight       67516 non-null  float64\n",
      " 10  gender       67516 non-null  float64\n",
      " 11  BMI          67516 non-null  float64\n",
      " 12  diastolic    67516 non-null  float64\n",
      " 13  systolic     67516 non-null  float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             False\n",
       "cholesterol    False\n",
       "gluc           False\n",
       "smoke          False\n",
       "alco           False\n",
       "active         False\n",
       "diabetes       False\n",
       "age            False\n",
       "height         False\n",
       "weight         False\n",
       "gender         False\n",
       "BMI            False\n",
       "diastolic      False\n",
       "systolic       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f4c2e7b808>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['height'],df['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 8), (70000, 5))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.shape, df24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>pressure</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100\\70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140/100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>95012</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>87194</td>\n",
       "      <td>medium</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>92108</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130/80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>89873</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120\\80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>94772</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140\\100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id cholesterol    gluc  smoke  alco  active pressure  diabetes\n",
       "0     95306         low  medium      0     0       0   120/80         1\n",
       "1     86688         low     low      0     0       1   100\\70         0\n",
       "2     98038         low     low      0     0       0  140/100         1\n",
       "3     88694         low     low      0     0       1   120\\90         0\n",
       "4     92856         low     low      0     0       0   130\\80         0\n",
       "...     ...         ...     ...    ...   ...     ...      ...       ...\n",
       "9995  95012         low     low      0     0       0   120/80         0\n",
       "9996  87194      medium     low      0     0       1   120/80         0\n",
       "9997  92108         low     low      1     1       1   130/80         0\n",
       "9998  89873         low     low      0     0       1   120\\80         0\n",
       "9999  94772        high     low      0     0       1  140\\100         0\n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85656</td>\n",
       "      <td>52.427105</td>\n",
       "      <td>165</td>\n",
       "      <td>62.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85658</td>\n",
       "      <td>39.570157</td>\n",
       "      <td>159</td>\n",
       "      <td>67.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85659</td>\n",
       "      <td>43.468857</td>\n",
       "      <td>168</td>\n",
       "      <td>59.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85660</td>\n",
       "      <td>58.119097</td>\n",
       "      <td>167</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85661</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>82.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>99993</td>\n",
       "      <td>52.676249</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>99995</td>\n",
       "      <td>61.878166</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>99996</td>\n",
       "      <td>52.199863</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>99998</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>99999</td>\n",
       "      <td>56.235455</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        age  height  weight  gender\n",
       "0     85656  52.427105     165    62.0       m\n",
       "1     85658  39.570157     159    67.0       m\n",
       "2     85659  43.468857     168    59.0       m\n",
       "3     85660  58.119097     167    70.0  female\n",
       "4     85661  54.000000     163    82.0       f\n",
       "...     ...        ...     ...     ...     ...\n",
       "9995  99993  52.676249     168    76.0       f\n",
       "9996  99995  61.878166     158   126.0       m\n",
       "9997  99996  52.199863     183   105.0  female\n",
       "9998  99998  61.000000     163    72.0       m\n",
       "9999  99999  56.235455     170    72.0       m\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20297</th>\n",
       "      <td>50055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.860370</td>\n",
       "      <td>168.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.904195</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25878</th>\n",
       "      <td>66571.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.046875</td>\n",
       "      <td>-120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28085</th>\n",
       "      <td>34295.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.196921</td>\n",
       "      <td>-140.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41373</th>\n",
       "      <td>6525.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.650138</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53163</th>\n",
       "      <td>22881.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.528405</td>\n",
       "      <td>161.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.720883</td>\n",
       "      <td>-115.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61249</th>\n",
       "      <td>29313.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.658453</td>\n",
       "      <td>153.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.068051</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63894</th>\n",
       "      <td>36025.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.276523</td>\n",
       "      <td>168.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.715420</td>\n",
       "      <td>-120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "20297  50055.0          0.0   0.0    0.0   0.0     1.0       0.0  63.860370   \n",
       "25878  66571.0          0.0   0.0    0.0   0.0     0.0       1.0  64.000000   \n",
       "28085  34295.0          0.0   0.0    0.0   0.0     1.0       0.0  50.000000   \n",
       "41373   6525.0          0.5   0.0    0.0   0.0     1.0       0.0  41.000000   \n",
       "53163  22881.0          0.0   0.0    0.0   0.0     1.0       0.0  60.528405   \n",
       "61249  29313.0          0.0   0.0    0.0   0.0     1.0       0.0  42.658453   \n",
       "63894  36025.0          0.5   0.0    0.0   0.0     0.0       0.0  40.276523   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "20297   168.0    59.0     0.0  20.904195     -150.0      80.0  \n",
       "25878   160.0    59.0     0.0  23.046875     -120.0      80.0  \n",
       "28085   162.0    74.0     1.0  28.196921     -140.0      90.0  \n",
       "41373   165.0    78.0     1.0  28.650138     -100.0      80.0  \n",
       "53163   161.0    90.0     0.0  34.720883     -115.0      70.0  \n",
       "61249   153.0    54.0     1.0  23.068051     -100.0      70.0  \n",
       "63894   168.0    50.0     0.0  17.715420     -120.0      80.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('systolic < 0 or diastolic < 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67516 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67516 rows x 14 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diastolic = abs(df.diastolic)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67516 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67516 rows x 14 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in ['diastolic', 'systolic']:\n",
    "    df.loc[df[col] > 300, col] /= 10\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16756</th>\n",
       "      <td>36414.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.483231</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31265</th>\n",
       "      <td>66998.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.297057</td>\n",
       "      <td>180.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.074074</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>58374.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.508946</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38830</th>\n",
       "      <td>67502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.020534</td>\n",
       "      <td>160.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.390625</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49664</th>\n",
       "      <td>11089.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.582478</td>\n",
       "      <td>175.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.122449</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56241</th>\n",
       "      <td>73356.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.427789</td>\n",
       "      <td>168.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.030045</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60646</th>\n",
       "      <td>79679.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>161.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.507696</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "16756  36414.0          1.0   1.0    0.0   0.0     1.0       1.0  58.483231   \n",
       "31265  66998.0          0.0   0.0    0.0   0.0     1.0       0.0  46.297057   \n",
       "35018  58374.0          0.0   0.0    0.0   0.0     0.0       0.0  47.000000   \n",
       "38830  67502.0          0.0   0.0    0.0   0.0     1.0       0.0  54.020534   \n",
       "49664  11089.0          0.0   0.0    0.0   0.0     1.0       0.0  57.582478   \n",
       "56241  73356.0          0.0   0.0    0.0   0.0     1.0       0.0  51.427789   \n",
       "60646  79679.0          0.0   0.0    0.0   0.0     0.0       1.0  63.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "16756   169.0    71.0     1.0  24.859074     1402.0      80.0  \n",
       "31265   180.0    78.0     0.0  24.074074     1402.0      90.0  \n",
       "35018   169.0    70.0     1.0  24.508946     1602.0      80.0  \n",
       "38830   160.0    65.0     1.0  25.390625     1402.0      90.0  \n",
       "49664   175.0    80.0     1.0  26.122449     1150.0      90.0  \n",
       "56241   168.0    65.0     0.0  23.030045     1102.0      80.0  \n",
       "60646   161.0   105.0     1.0  40.507696     1301.0      80.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.diastolic > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>95886.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.565366</td>\n",
       "      <td>165.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.977043</td>\n",
       "      <td>113.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>97907.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.188912</td>\n",
       "      <td>166.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.402816</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9840</th>\n",
       "      <td>90139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>159.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.128792</td>\n",
       "      <td>110.0</td>\n",
       "      <td>807.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>34098.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.560575</td>\n",
       "      <td>169.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.110991</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>3352.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.478439</td>\n",
       "      <td>186.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.350330</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12382</th>\n",
       "      <td>22832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>179.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.847009</td>\n",
       "      <td>120.0</td>\n",
       "      <td>850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12958</th>\n",
       "      <td>25348.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.587953</td>\n",
       "      <td>151.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.998904</td>\n",
       "      <td>140.0</td>\n",
       "      <td>809.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13678</th>\n",
       "      <td>47030.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.198494</td>\n",
       "      <td>156.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.709402</td>\n",
       "      <td>150.0</td>\n",
       "      <td>901.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14318</th>\n",
       "      <td>13066.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.232717</td>\n",
       "      <td>165.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.078972</td>\n",
       "      <td>120.0</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15497</th>\n",
       "      <td>62921.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.097194</td>\n",
       "      <td>165.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.180900</td>\n",
       "      <td>120.0</td>\n",
       "      <td>820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>17260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.309968</td>\n",
       "      <td>130.0</td>\n",
       "      <td>901.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18217</th>\n",
       "      <td>20438.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.324435</td>\n",
       "      <td>160.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.343750</td>\n",
       "      <td>160.0</td>\n",
       "      <td>710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24064</th>\n",
       "      <td>29821.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.350445</td>\n",
       "      <td>155.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.714880</td>\n",
       "      <td>160.0</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28265</th>\n",
       "      <td>33191.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.570842</td>\n",
       "      <td>170.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.221453</td>\n",
       "      <td>112.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32202</th>\n",
       "      <td>61901.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.015625</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43673</th>\n",
       "      <td>53083.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.470910</td>\n",
       "      <td>176.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.826446</td>\n",
       "      <td>140.0</td>\n",
       "      <td>809.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47668</th>\n",
       "      <td>62058.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.975359</td>\n",
       "      <td>179.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.350208</td>\n",
       "      <td>130.0</td>\n",
       "      <td>980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49565</th>\n",
       "      <td>54286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.802875</td>\n",
       "      <td>174.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.120624</td>\n",
       "      <td>130.0</td>\n",
       "      <td>809.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54676</th>\n",
       "      <td>53614.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.812457</td>\n",
       "      <td>182.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.019442</td>\n",
       "      <td>120.0</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56493</th>\n",
       "      <td>75482.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.854894</td>\n",
       "      <td>164.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.026175</td>\n",
       "      <td>125.0</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57894</th>\n",
       "      <td>9482.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.464750</td>\n",
       "      <td>162.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.291724</td>\n",
       "      <td>130.0</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60396</th>\n",
       "      <td>59157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>161.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.147255</td>\n",
       "      <td>150.0</td>\n",
       "      <td>709.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60472</th>\n",
       "      <td>4208.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.846680</td>\n",
       "      <td>168.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.636054</td>\n",
       "      <td>140.0</td>\n",
       "      <td>804.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64753</th>\n",
       "      <td>33580.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.061602</td>\n",
       "      <td>169.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.408179</td>\n",
       "      <td>120.0</td>\n",
       "      <td>807.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "1106   95886.0          0.0   0.0    0.0   0.0     1.0       1.0  50.565366   \n",
       "2619   97907.0          0.5   0.0    0.0   0.0     1.0       1.0  52.188912   \n",
       "9840   90139.0          0.0   0.0    0.0   0.0     1.0       1.0  53.000000   \n",
       "10018  34098.0          0.5   0.5    0.0   1.0     1.0       1.0  49.560575   \n",
       "12143   3352.0          0.0   0.0    0.0   0.0     1.0       1.0  57.478439   \n",
       "12382  22832.0          0.0   0.0    0.0   0.0     1.0       1.0  39.000000   \n",
       "12958  25348.0          0.0   0.0    0.0   0.0     0.0       1.0  53.587953   \n",
       "13678  47030.0          0.5   0.5    0.0   0.0     1.0       1.0  50.198494   \n",
       "14318  13066.0          0.0   0.0    0.0   0.0     1.0       1.0  52.232717   \n",
       "15497  62921.0          0.5   1.0    0.0   0.0     1.0       1.0  58.097194   \n",
       "17504  17260.0          0.0   0.0    1.0   1.0     1.0       1.0  58.000000   \n",
       "18217  20438.0          0.0   0.0    0.0   1.0     1.0       1.0  50.324435   \n",
       "24064  29821.0          0.0   0.0    0.0   0.0     1.0       1.0  52.350445   \n",
       "28265  33191.0          0.0   0.5    0.0   0.0     1.0       1.0  54.570842   \n",
       "32202  61901.0          0.0   0.0    0.0   0.0     1.0       1.0  63.000000   \n",
       "43673  53083.0          0.0   0.0    0.0   0.0     1.0       1.0  56.470910   \n",
       "47668  62058.0          0.0   0.0    0.0   0.0     1.0       1.0  59.975359   \n",
       "49565  54286.0          0.0   0.0    0.0   0.0     0.0       1.0  43.802875   \n",
       "54676  53614.0          0.0   0.0    0.0   0.0     0.0       1.0  41.812457   \n",
       "56493  75482.0          0.0   0.0    0.0   0.0     1.0       1.0  55.854894   \n",
       "57894   9482.0          0.0   0.0    0.0   0.0     1.0       1.0  53.464750   \n",
       "60396  59157.0          0.0   0.0    0.0   0.0     1.0       1.0  49.000000   \n",
       "60472   4208.0          1.0   1.0    0.0   0.0     0.0       1.0  55.846680   \n",
       "64753  33580.0          0.0   0.0    0.0   0.0     0.0       1.0  54.061602   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "1106    165.0    68.0     0.0  24.977043      113.0     570.0  \n",
       "2619    166.0    70.0     1.0  25.402816      160.0    1000.0  \n",
       "9840    159.0    61.0     1.0  24.128792      110.0     807.7  \n",
       "10018   169.0    86.0     0.0  30.110991      150.0    1000.0  \n",
       "12143   186.0   105.0     0.0  30.350330      140.0    1000.0  \n",
       "12382   179.0    70.0     0.0  21.847009      120.0     850.0  \n",
       "12958   151.0    57.0     1.0  24.998904      140.0     809.9  \n",
       "13678   156.0    65.0     1.0  26.709402      150.0     901.1  \n",
       "14318   165.0    71.0     1.0  26.078972      120.0     800.0  \n",
       "15497   165.0    74.0     1.0  27.180900      120.0     820.0  \n",
       "17504   169.0    78.0     0.0  27.309968      130.0     901.1  \n",
       "18217   160.0    70.0     1.0  27.343750      160.0     710.0  \n",
       "24064   155.0    81.0     1.0  33.714880      160.0     810.0  \n",
       "28265   170.0    70.0     0.0  24.221453      112.0     570.0  \n",
       "32202   160.0   105.0     1.0  41.015625      200.0    1100.0  \n",
       "43673   176.0    80.0     0.0  25.826446      140.0     809.9  \n",
       "47668   179.0    62.0     0.0  19.350208      130.0     980.0  \n",
       "49565   174.0    70.0     0.0  23.120624      130.0     809.9  \n",
       "54676   182.0    63.0     0.0  19.019442      120.0     800.0  \n",
       "56493   164.0    70.0     1.0  26.026175      125.0     680.0  \n",
       "57894   162.0    69.0     1.0  26.291724      130.0     910.0  \n",
       "60396   161.0    60.0     1.0  23.147255      150.0     709.9  \n",
       "60472   168.0    78.0     1.0  27.636054      140.0     804.4  \n",
       "64753   169.0    64.0     0.0  22.408179      120.0     807.9  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.systolic > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>93224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.646133</td>\n",
       "      <td>176.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.960227</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>98095.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.801506</td>\n",
       "      <td>156.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.847469</td>\n",
       "      <td>90.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>97439.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.496235</td>\n",
       "      <td>173.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.412409</td>\n",
       "      <td>130.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>97950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.124572</td>\n",
       "      <td>163.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.722873</td>\n",
       "      <td>110.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>91129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>178.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.777553</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>91073.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.306639</td>\n",
       "      <td>161.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.533043</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10518</th>\n",
       "      <td>19258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.971937</td>\n",
       "      <td>150.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>33295.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.560575</td>\n",
       "      <td>172.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.309356</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>13943.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.690623</td>\n",
       "      <td>166.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.362172</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12691</th>\n",
       "      <td>45400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.145329</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15351</th>\n",
       "      <td>32749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.993155</td>\n",
       "      <td>166.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.677021</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16582</th>\n",
       "      <td>60565.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>176.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.275310</td>\n",
       "      <td>90.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17109</th>\n",
       "      <td>39577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.530864</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17498</th>\n",
       "      <td>75007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.010951</td>\n",
       "      <td>155.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.471384</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20524</th>\n",
       "      <td>53102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.572895</td>\n",
       "      <td>165.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.038567</td>\n",
       "      <td>170.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>28742.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.688569</td>\n",
       "      <td>160.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22166</th>\n",
       "      <td>34120.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.164271</td>\n",
       "      <td>161.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.492458</td>\n",
       "      <td>90.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22296</th>\n",
       "      <td>31965.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.134155</td>\n",
       "      <td>170.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.837370</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24596</th>\n",
       "      <td>12550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.919233</td>\n",
       "      <td>162.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.910684</td>\n",
       "      <td>110.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25377</th>\n",
       "      <td>81298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.801506</td>\n",
       "      <td>164.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.026175</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27377</th>\n",
       "      <td>35356.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.939767</td>\n",
       "      <td>168.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.092971</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30339</th>\n",
       "      <td>9610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.735797</td>\n",
       "      <td>156.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.065746</td>\n",
       "      <td>150.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30572</th>\n",
       "      <td>27069.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.780972</td>\n",
       "      <td>170.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.453287</td>\n",
       "      <td>110.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30649</th>\n",
       "      <td>27242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.092971</td>\n",
       "      <td>110.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31189</th>\n",
       "      <td>58537.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.495551</td>\n",
       "      <td>170.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31292</th>\n",
       "      <td>37874.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>157.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.993022</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32338</th>\n",
       "      <td>58088.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.167009</td>\n",
       "      <td>162.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.100290</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37757</th>\n",
       "      <td>61874.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.198494</td>\n",
       "      <td>151.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.437481</td>\n",
       "      <td>170.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39187</th>\n",
       "      <td>14410.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>152.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.641274</td>\n",
       "      <td>150.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41089</th>\n",
       "      <td>55119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.530459</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.507805</td>\n",
       "      <td>120.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41300</th>\n",
       "      <td>63710.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.418207</td>\n",
       "      <td>165.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.875115</td>\n",
       "      <td>130.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44048</th>\n",
       "      <td>50210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>146.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.802214</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44981</th>\n",
       "      <td>54780.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.540041</td>\n",
       "      <td>170.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.027682</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46069</th>\n",
       "      <td>75399.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.651608</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.795360</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48235</th>\n",
       "      <td>57646.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.638604</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49736</th>\n",
       "      <td>23512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.008214</td>\n",
       "      <td>156.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.545694</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52608</th>\n",
       "      <td>68612.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.052019</td>\n",
       "      <td>160.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.812500</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52763</th>\n",
       "      <td>24837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.900068</td>\n",
       "      <td>164.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000595</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52964</th>\n",
       "      <td>2845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>167.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.155294</td>\n",
       "      <td>90.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54678</th>\n",
       "      <td>65470.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.711662</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55448</th>\n",
       "      <td>16884.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.158818</td>\n",
       "      <td>128.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57364</th>\n",
       "      <td>59301.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.412731</td>\n",
       "      <td>154.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.287907</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60491</th>\n",
       "      <td>68121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.526352</td>\n",
       "      <td>156.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.091387</td>\n",
       "      <td>140.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60898</th>\n",
       "      <td>81260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>155.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.633715</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62336</th>\n",
       "      <td>62754.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000595</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63023</th>\n",
       "      <td>57023.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>177.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.832392</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64687</th>\n",
       "      <td>26983.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.997262</td>\n",
       "      <td>171.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.255019</td>\n",
       "      <td>110.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66584</th>\n",
       "      <td>40122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.786448</td>\n",
       "      <td>161.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.060954</td>\n",
       "      <td>90.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "664    93224.0          0.0   1.0    0.0   0.0     1.0       1.0  64.646133   \n",
       "1439   98095.0          0.0   0.0    0.0   0.0     1.0       0.0  49.801506   \n",
       "1532   97439.0          0.0   0.0    0.0   0.0     1.0       0.0  51.496235   \n",
       "2444   97950.0          0.0   0.0    0.0   0.0     1.0       0.0  42.124572   \n",
       "4795   91129.0          0.0   0.0    0.0   0.0     1.0       0.0  46.000000   \n",
       "9544   91073.0          0.0   0.0    0.0   0.0     1.0       0.0  40.306639   \n",
       "10518  19258.0          0.0   0.0    0.0   0.0     0.0       0.0  40.971937   \n",
       "11498  33295.0          0.0   0.0    0.0   0.0     0.0       0.0  45.560575   \n",
       "11683  13943.0          1.0   0.0    0.0   0.0     1.0       0.0  59.690623   \n",
       "12691  45400.0          0.0   0.0    0.0   0.0     1.0       0.0  44.000000   \n",
       "15351  32749.0          0.0   0.0    0.0   0.0     1.0       0.0  57.993155   \n",
       "16582  60565.0          0.0   0.0    0.0   0.0     1.0       0.0  49.000000   \n",
       "17109  39577.0          0.0   0.0    1.0   1.0     1.0       0.0  64.000000   \n",
       "17498  75007.0          0.0   0.0    0.0   0.0     1.0       0.0  60.010951   \n",
       "20524  53102.0          0.0   0.0    0.0   0.0     1.0       0.0  39.572895   \n",
       "21585  28742.0          0.5   0.5    0.0   0.0     0.0       0.0  58.688569   \n",
       "22166  34120.0          0.5   0.0    0.0   0.0     1.0       0.0  44.164271   \n",
       "22296  31965.0          0.0   0.0    0.0   0.0     1.0       0.0  44.134155   \n",
       "24596  12550.0          0.0   0.0    0.0   0.0     1.0       0.0  57.919233   \n",
       "25377  81298.0          0.0   0.0    0.0   0.0     1.0       0.0  41.801506   \n",
       "27377  35356.0          0.0   0.0    0.0   0.0     1.0       0.0  55.939767   \n",
       "30339   9610.0          0.0   0.0    0.0   0.0     0.0       0.0  45.735797   \n",
       "30572  27069.0          0.0   0.0    0.0   0.0     1.0       0.0  47.780972   \n",
       "30649  27242.0          0.0   0.0    0.0   0.0     1.0       0.0  54.000000   \n",
       "31189  58537.0          0.0   0.0    0.0   0.0     1.0       0.0  52.495551   \n",
       "31292  37874.0          1.0   0.0    0.0   0.0     1.0       0.0  52.000000   \n",
       "32338  58088.0          0.0   0.0    0.0   0.0     1.0       0.0  40.167009   \n",
       "37757  61874.0          1.0   0.0    0.0   0.0     1.0       0.0  62.198494   \n",
       "39187  14410.0          1.0   0.0    0.0   0.0     1.0       0.0  50.000000   \n",
       "41089  55119.0          0.0   0.0    0.0   0.0     1.0       0.0  49.530459   \n",
       "41300  63710.0          0.0   0.0    0.0   0.0     1.0       0.0  61.418207   \n",
       "44048  50210.0          0.0   0.0    0.0   0.0     0.0       0.0  57.000000   \n",
       "44981  54780.0          0.0   0.0    0.0   0.0     1.0       0.0  63.540041   \n",
       "46069  75399.0          1.0   0.0    0.0   0.0     1.0       0.0  48.651608   \n",
       "48235  57646.0          0.0   0.0    0.0   0.0     1.0       0.0  55.638604   \n",
       "49736  23512.0          1.0   0.0    0.0   0.0     1.0       0.0  56.008214   \n",
       "52608  68612.0          0.0   0.0    0.0   0.0     0.0       0.0  52.052019   \n",
       "52763  24837.0          0.0   0.0    0.0   0.0     1.0       0.0  49.900068   \n",
       "52964   2845.0          0.0   0.0    0.0   0.0     1.0       0.0  62.000000   \n",
       "54678  65470.0          1.0   0.0    1.0   0.0     1.0       0.0  53.000000   \n",
       "55448  16884.0          0.0   0.0    0.0   0.0     1.0       0.0  49.000000   \n",
       "57364  59301.0          0.0   0.0    0.0   0.0     1.0       0.0  57.412731   \n",
       "60491  68121.0          0.0   0.0    0.0   0.0     1.0       0.0  59.526352   \n",
       "60898  81260.0          0.0   0.0    0.0   1.0     1.0       0.0  50.000000   \n",
       "62336  62754.0          0.5   0.0    0.0   0.0     1.0       0.0  51.000000   \n",
       "63023  57023.0          0.5   0.5    0.0   0.0     1.0       0.0  49.000000   \n",
       "64687  26983.0          0.0   0.0    0.0   0.0     1.0       0.0  47.997262   \n",
       "66584  40122.0          0.0   0.0    0.0   0.0     1.0       0.0  55.786448   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "664     176.0    99.0     0.0  31.960227      120.0       0.0  \n",
       "1439    156.0    41.0     1.0  16.847469       90.6       0.0  \n",
       "1532    173.0   100.0     0.0  33.412409      130.0      20.0  \n",
       "2444    163.0    71.0     1.0  26.722873      110.0       6.0  \n",
       "4795    178.0    69.0     1.0  21.777553      110.0      10.0  \n",
       "9544    161.0    61.0     0.0  23.533043      117.0       0.0  \n",
       "10518   150.0    60.0     0.0  26.666667      130.0       0.0  \n",
       "11498   172.0    66.0     0.0  22.309356      120.0      20.0  \n",
       "11683   166.0   125.0     1.0  45.362172      120.0       8.0  \n",
       "12691   170.0    64.0     1.0  22.145329       90.7       0.0  \n",
       "15351   166.0    68.0     0.0  24.677021      149.0       0.0  \n",
       "16582   176.0    69.0     0.0  22.275310       90.6       0.0  \n",
       "17109   180.0    73.0     0.0  22.530864      140.0       0.0  \n",
       "17498   155.0    66.0     1.0  27.471384       13.0       0.0  \n",
       "20524   165.0    60.0     1.0  22.038567      170.0      10.0  \n",
       "21585   160.0    72.0     1.0  28.125000      110.0      20.0  \n",
       "22166   161.0    92.0     1.0  35.492458       90.6       0.0  \n",
       "22296   170.0    66.0     0.0  22.837370      120.0      20.0  \n",
       "24596   162.0    68.0     1.0  25.910684      110.0      20.0  \n",
       "25377   164.0    70.0     1.0  26.026175      120.0       0.0  \n",
       "27377   168.0    68.0     1.0  24.092971      120.0      20.0  \n",
       "30339   156.0    61.0     1.0  25.065746      150.0      10.0  \n",
       "30572   170.0    62.0     0.0  21.453287      110.0      20.0  \n",
       "30649   168.0    68.0     0.0  24.092971      110.0       7.0  \n",
       "31189   170.0    68.0     1.0  23.529412      100.0      20.0  \n",
       "31292   157.0    69.0     1.0  27.993022       16.0      10.0  \n",
       "32338   162.0    58.0     1.0  22.100290      120.0      20.0  \n",
       "37757   151.0    58.0     1.0  25.437481      170.0      10.0  \n",
       "39187   152.0    50.0     1.0  21.641274      150.0      10.0  \n",
       "41089   165.0    64.0     0.0  23.507805      120.0       9.0  \n",
       "41300   165.0    65.0     0.0  23.875115      130.0      20.0  \n",
       "44048   146.0    55.0     1.0  25.802214      130.0       1.0  \n",
       "44981   170.0    81.0     0.0  28.027682      148.0       0.0  \n",
       "46069   164.0    64.0     0.0  23.795360       24.0      20.0  \n",
       "48235   162.0    50.0     1.0  19.051974       30.9       0.0  \n",
       "49736   156.0    50.0     1.0  20.545694      138.0       0.0  \n",
       "52608   160.0    84.0     1.0  32.812500      121.0       0.0  \n",
       "52763   164.0    78.0     1.0  29.000595      138.0       0.0  \n",
       "52964   167.0    59.0     0.0  21.155294       90.6       0.0  \n",
       "54678   165.0    70.0     0.0  25.711662      140.0       0.0  \n",
       "55448   169.0    69.0     1.0  24.158818      128.0      20.0  \n",
       "57364   154.0    41.0     1.0  17.287907       80.6       0.0  \n",
       "60491   156.0   100.0     1.0  41.091387      140.0      10.0  \n",
       "60898   155.0    76.0     1.0  31.633715       70.0      15.0  \n",
       "62336   164.0    78.0     1.0  29.000595      108.0       0.0  \n",
       "63023   177.0    59.0     1.0  18.832392      120.0      20.0  \n",
       "64687   171.0    68.0     1.0  23.255019      110.0       7.0  \n",
       "66584   161.0    52.0     1.0  20.060954       90.0       6.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.systolic < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29758</th>\n",
       "      <td>12494.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.283368</td>\n",
       "      <td>163.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.711845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>208.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54515</th>\n",
       "      <td>7657.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.151951</td>\n",
       "      <td>162.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.291724</td>\n",
       "      <td>7.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64794</th>\n",
       "      <td>60477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>171.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.358845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "29758  12494.0          0.0   0.0    1.0   0.0     1.0       1.0  46.283368   \n",
       "54515   7657.0          0.0   0.0    0.0   0.0     1.0       0.0  58.151951   \n",
       "64794  60477.0          0.0   0.0    0.0   0.0     1.0       1.0  51.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "29758   163.0    63.0     0.0  23.711845        1.0     208.8  \n",
       "54515   162.0    69.0     1.0  26.291724        7.0      80.0  \n",
       "64794   171.0    80.0     1.0  27.358845        1.0     108.8  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.diastolic < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67516 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67516 rows x 14 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df['diastolic'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67496 rows x 14 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[df['systolic'] == 0].index, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67496 rows x 14 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.systolic < 30, 'systolic'] *= 10\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67496 rows x 14 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.diastolic < 30, 'diastolic'] *= 10\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>49973.089102</td>\n",
       "      <td>28847.572488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25033.500000</td>\n",
       "      <td>50007.500000</td>\n",
       "      <td>74874.500000</td>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.184144</td>\n",
       "      <td>0.340693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluc</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.113703</td>\n",
       "      <td>0.286707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoke</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.283283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alco</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.225909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>0.397445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.250948</td>\n",
       "      <td>0.433562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>53.157658</td>\n",
       "      <td>6.763708</td>\n",
       "      <td>29.563313</td>\n",
       "      <td>48.254620</td>\n",
       "      <td>53.927447</td>\n",
       "      <td>58.294319</td>\n",
       "      <td>64.889802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>164.430618</td>\n",
       "      <td>7.817413</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>74.205104</td>\n",
       "      <td>14.575911</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>0.476360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>27.473165</td>\n",
       "      <td>5.279666</td>\n",
       "      <td>12.254473</td>\n",
       "      <td>23.833005</td>\n",
       "      <td>26.298488</td>\n",
       "      <td>30.346074</td>\n",
       "      <td>69.827553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>127.125748</td>\n",
       "      <td>21.176482</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>81.950450</td>\n",
       "      <td>17.556571</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count          mean           std         min           25%  \\\n",
       "id           67496.0  49973.089102  28847.572488    0.000000  25033.500000   \n",
       "cholesterol  67496.0      0.184144      0.340693    0.000000      0.000000   \n",
       "gluc         67496.0      0.113703      0.286707    0.000000      0.000000   \n",
       "smoke        67496.0      0.087990      0.283283    0.000000      0.000000   \n",
       "alco         67496.0      0.053944      0.225909    0.000000      0.000000   \n",
       "active       67496.0      0.803381      0.397445    0.000000      1.000000   \n",
       "diabetes     67496.0      0.250948      0.433562    0.000000      0.000000   \n",
       "age          67496.0     53.157658      6.763708   29.563313     48.254620   \n",
       "height       67496.0    164.430618      7.817413  140.000000    159.000000   \n",
       "weight       67496.0     74.205104     14.575911   35.000000     65.000000   \n",
       "gender       67496.0      0.651935      0.476360    0.000000      0.000000   \n",
       "BMI          67496.0     27.473165      5.279666   12.254473     23.833005   \n",
       "diastolic    67496.0    127.125748     21.176482   10.000000    120.000000   \n",
       "systolic     67496.0     81.950450     17.556571   10.000000     80.000000   \n",
       "\n",
       "                      50%           75%           max  \n",
       "id           50007.500000  74874.500000  99999.000000  \n",
       "cholesterol      0.000000      0.500000      1.000000  \n",
       "gluc             0.000000      0.000000      1.000000  \n",
       "smoke            0.000000      0.000000      1.000000  \n",
       "alco             0.000000      0.000000      1.000000  \n",
       "active           1.000000      1.000000      1.000000  \n",
       "diabetes         0.000000      1.000000      1.000000  \n",
       "age             53.927447     58.294319     64.889802  \n",
       "height         165.000000    170.000000    207.000000  \n",
       "weight          71.000000     82.000000    200.000000  \n",
       "gender           1.000000      1.000000      1.000000  \n",
       "BMI             26.298488     30.346074     69.827553  \n",
       "diastolic      120.000000    140.000000   1602.000000  \n",
       "systolic        80.000000     90.000000   1100.000000  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>91725.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>174.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.753864</td>\n",
       "      <td>90.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>95886.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.565366</td>\n",
       "      <td>165.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.977043</td>\n",
       "      <td>113.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>96339.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>172.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.267171</td>\n",
       "      <td>80.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>97439.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.496235</td>\n",
       "      <td>173.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.412409</td>\n",
       "      <td>130.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>86690.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.480493</td>\n",
       "      <td>159.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.733238</td>\n",
       "      <td>80.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65353</th>\n",
       "      <td>36338.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.007316</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65817</th>\n",
       "      <td>28682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.989049</td>\n",
       "      <td>169.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.758307</td>\n",
       "      <td>90.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65915</th>\n",
       "      <td>35864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.681725</td>\n",
       "      <td>160.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.171875</td>\n",
       "      <td>80.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67973</th>\n",
       "      <td>19858.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.373702</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68016</th>\n",
       "      <td>78468.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.010951</td>\n",
       "      <td>156.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.065746</td>\n",
       "      <td>80.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "178    91725.0          0.0   0.0    1.0   1.0     1.0       1.0  51.000000   \n",
       "1106   95886.0          0.0   0.0    0.0   0.0     1.0       1.0  50.565366   \n",
       "1351   96339.0          0.0   0.0    1.0   0.0     1.0       1.0  58.000000   \n",
       "1532   97439.0          0.0   0.0    0.0   0.0     1.0       0.0  51.496235   \n",
       "1747   86690.0          0.0   0.0    1.0   0.0     1.0       0.0  50.480493   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "65353  36338.0          1.0   1.0    0.0   1.0     1.0       1.0  52.000000   \n",
       "65817  28682.0          0.0   0.0    0.0   0.0     1.0       1.0  51.989049   \n",
       "65915  35864.0          0.0   0.0    0.0   0.0     0.0       0.0  44.681725   \n",
       "67973  19858.0          0.5   0.0    0.0   1.0     1.0       1.0  55.000000   \n",
       "68016  78468.0          0.5   0.5    0.0   0.0     1.0       1.0  60.010951   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "178     174.0    81.0     0.0  26.753864       90.0     130.0  \n",
       "1106    165.0    68.0     0.0  24.977043      113.0     570.0  \n",
       "1351    172.0    57.0     0.0  19.267171       80.0     120.0  \n",
       "1532    173.0   100.0     0.0  33.412409      130.0     200.0  \n",
       "1747    159.0    60.0     1.0  23.733238       80.0     110.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "65353   162.0    84.0     1.0  32.007316       90.0     140.0  \n",
       "65817   169.0    65.0     0.0  22.758307       90.0     130.0  \n",
       "65915   160.0    67.0     1.0  26.171875       80.0     120.0  \n",
       "67973   170.0    82.0     0.0  28.373702       90.0     140.0  \n",
       "68016   156.0    61.0     1.0  25.065746       80.0     130.0  \n",
       "\n",
       "[147 rows x 14 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('diastolic < systolic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  178,  1106,  1351,  1532,  1747,  1958,  2088,  2619,  3211,\n",
       "             4116,\n",
       "            ...\n",
       "            63023, 63449, 64753, 64794, 65117, 65353, 65817, 65915, 67973,\n",
       "            68016],\n",
       "           dtype='int64', length=147)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('diastolic < systolic').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  \n",
       "...       ...     ...     ...        ...        ...       ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  \n",
       "\n",
       "[67496 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.query('diastolic < systolic').index, ['diastolic', 'systolic']] = df.loc[df.query('diastolic < systolic').index, ['systolic', 'diastolic']].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, cholesterol, gluc, smoke, alco, active, diabetes, age, height, weight, gender, BMI, diastolic, systolic]\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('diastolic < systolic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic    pp  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  40.0  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  30.0  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  40.0  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  30.0  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  50.0  \n",
       "...       ...     ...     ...        ...        ...       ...   ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  50.0  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  40.0  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  40.0  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  62.0  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  50.0  \n",
       "\n",
       "[67496 rows x 15 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pp'] = df['diastolic'] - df['systolic']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic    pp        pp%  \n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  40.0  33.333333  \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  30.0  30.000000  \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  40.0  28.571429  \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  30.0  25.000000  \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  50.0  38.461538  \n",
       "...       ...     ...     ...        ...        ...       ...   ...        ...  \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  50.0  38.461538  \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  40.0  30.769231  \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  40.0  36.363636  \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  62.0  40.789474  \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  50.0  45.454545  \n",
       "\n",
       "[67496 rows x 16 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pp%'] = df['pp'] *100/ df['diastolic']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.057851</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.923340</td>\n",
       "      <td>162.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.051974</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.035592</td>\n",
       "      <td>156.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.407627</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.912513</td>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.343600</td>\n",
       "      <td>162.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.434842</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>55997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.377139</td>\n",
       "      <td>178.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.508522</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>8296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.032854</td>\n",
       "      <td>160.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.031250</td>\n",
       "      <td>130.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>17896.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.477070</td>\n",
       "      <td>158.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>40803.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.969884</td>\n",
       "      <td>175.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.755102</td>\n",
       "      <td>152.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.789474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.859074</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "0      95306.0          0.0   0.5    0.0   0.0     0.0       1.0  61.000000   \n",
       "1      86688.0          0.0   0.0    0.0   0.0     1.0       0.0  39.923340   \n",
       "2      98038.0          0.0   0.0    0.0   0.0     0.0       1.0  64.035592   \n",
       "3      88694.0          0.0   0.0    0.0   0.0     1.0       0.0  47.000000   \n",
       "4      92856.0          0.0   0.0    0.0   0.0     0.0       0.0  50.343600   \n",
       "...        ...          ...   ...    ...   ...     ...       ...        ...   \n",
       "69995  55997.0          0.0   0.0    0.0   0.0     1.0       0.0  53.377139   \n",
       "69996   8296.0          0.0   0.0    0.0   0.0     1.0       0.0  60.032854   \n",
       "69997  17896.0          0.0   0.0    1.0   0.0     1.0       0.0  55.477070   \n",
       "69998  40803.0          0.5   0.0    1.0   0.0     1.0       1.0  55.969884   \n",
       "69999  11945.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic    pp  \\\n",
       "0       165.0    90.0     0.0  33.057851      120.0      80.0  40.0   \n",
       "1       162.0    50.0     1.0  19.051974      100.0      70.0  30.0   \n",
       "2       156.0    74.0     1.0  30.407627      140.0     100.0  40.0   \n",
       "3       162.0    89.0     1.0  33.912513      120.0      90.0  30.0   \n",
       "4       162.0    72.0     0.0  27.434842      130.0      80.0  50.0   \n",
       "...       ...     ...     ...        ...        ...       ...   ...   \n",
       "69995   178.0   103.0     0.0  32.508522      130.0      80.0  50.0   \n",
       "69996   160.0    82.0     1.0  32.031250      130.0      90.0  40.0   \n",
       "69997   158.0    60.0     1.0  24.034610      110.0      70.0  40.0   \n",
       "69998   175.0    85.0     0.0  27.755102      152.0      90.0  62.0   \n",
       "69999   169.0    71.0     1.0  24.859074      110.0      60.0  50.0   \n",
       "\n",
       "             pp%  heart_alarm  \n",
       "0      33.333333            0  \n",
       "1      30.000000            0  \n",
       "2      28.571429            0  \n",
       "3      25.000000            0  \n",
       "4      38.461538            0  \n",
       "...          ...          ...  \n",
       "69995  38.461538            0  \n",
       "69996  30.769231            0  \n",
       "69997  36.363636            0  \n",
       "69998  40.789474            0  \n",
       "69999  45.454545            0  \n",
       "\n",
       "[67496 rows x 17 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['heart_alarm'] = (df['pp%'] < 25)*1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    66176\n",
       "1     1320\n",
       "Name: heart_alarm, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['heart_alarm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8360</th>\n",
       "      <td>99334.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.568789</td>\n",
       "      <td>170.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>130.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52126</th>\n",
       "      <td>41109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>156.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.833005</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64426</th>\n",
       "      <td>14956.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>178.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.089888</td>\n",
       "      <td>110.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40442</th>\n",
       "      <td>20949.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.315537</td>\n",
       "      <td>152.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.536704</td>\n",
       "      <td>130.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61582</th>\n",
       "      <td>37565.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.343600</td>\n",
       "      <td>158.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.239224</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  cholesterol  gluc  smoke  alco  active  diabetes        age  \\\n",
       "8360   99334.0          0.0   0.0    0.0   0.0     1.0       1.0  61.568789   \n",
       "52126  41109.0          0.0   0.0    0.0   0.0     0.0       0.0  47.000000   \n",
       "64426  14956.0          0.0   0.0    0.0   0.0     1.0       0.0  48.000000   \n",
       "40442  20949.0          0.0   0.0    0.0   0.0     1.0       0.0  43.315537   \n",
       "61582  37565.0          0.0   0.0    0.0   0.0     1.0       0.0  42.343600   \n",
       "\n",
       "       height  weight  gender        BMI  diastolic  systolic    pp  \\\n",
       "8360    170.0    68.0     0.0  23.529412      130.0     110.0  20.0   \n",
       "52126   156.0    58.0     1.0  23.833005       90.0      70.0  20.0   \n",
       "64426   178.0    89.0     0.0  28.089888      110.0      90.0  20.0   \n",
       "40442   152.0    59.0     1.0  25.536704      130.0     100.0  30.0   \n",
       "61582   158.0    68.0     1.0  27.239224      100.0      80.0  20.0   \n",
       "\n",
       "             pp%  heart_alarm  \n",
       "8360   15.384615            1  \n",
       "52126  22.222222            1  \n",
       "64426  18.181818            1  \n",
       "40442  23.076923            1  \n",
       "61582  20.000000            1  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['heart_alarm'] == 1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>49973.089102</td>\n",
       "      <td>28847.572488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25033.500000</td>\n",
       "      <td>50007.500000</td>\n",
       "      <td>74874.500000</td>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.184144</td>\n",
       "      <td>0.340693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluc</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.113703</td>\n",
       "      <td>0.286707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoke</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.283283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alco</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.225909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>0.397445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.250948</td>\n",
       "      <td>0.433562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>53.157658</td>\n",
       "      <td>6.763708</td>\n",
       "      <td>29.563313</td>\n",
       "      <td>48.254620</td>\n",
       "      <td>53.927447</td>\n",
       "      <td>58.294319</td>\n",
       "      <td>64.889802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>164.430618</td>\n",
       "      <td>7.817413</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>74.205104</td>\n",
       "      <td>14.575911</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>0.476360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>27.473165</td>\n",
       "      <td>5.279666</td>\n",
       "      <td>12.254473</td>\n",
       "      <td>23.833005</td>\n",
       "      <td>26.298488</td>\n",
       "      <td>30.346074</td>\n",
       "      <td>69.827553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>127.462527</td>\n",
       "      <td>25.079666</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>81.613672</td>\n",
       "      <td>9.850456</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pp</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>45.848855</td>\n",
       "      <td>21.188590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pp%</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>35.470788</td>\n",
       "      <td>5.718729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>95.210728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_alarm</th>\n",
       "      <td>67496.0</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.138472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count          mean           std         min           25%  \\\n",
       "id           67496.0  49973.089102  28847.572488    0.000000  25033.500000   \n",
       "cholesterol  67496.0      0.184144      0.340693    0.000000      0.000000   \n",
       "gluc         67496.0      0.113703      0.286707    0.000000      0.000000   \n",
       "smoke        67496.0      0.087990      0.283283    0.000000      0.000000   \n",
       "alco         67496.0      0.053944      0.225909    0.000000      0.000000   \n",
       "active       67496.0      0.803381      0.397445    0.000000      1.000000   \n",
       "diabetes     67496.0      0.250948      0.433562    0.000000      0.000000   \n",
       "age          67496.0     53.157658      6.763708   29.563313     48.254620   \n",
       "height       67496.0    164.430618      7.817413  140.000000    159.000000   \n",
       "weight       67496.0     74.205104     14.575911   35.000000     65.000000   \n",
       "gender       67496.0      0.651935      0.476360    0.000000      0.000000   \n",
       "BMI          67496.0     27.473165      5.279666   12.254473     23.833005   \n",
       "diastolic    67496.0    127.462527     25.079666   60.000000    120.000000   \n",
       "systolic     67496.0     81.613672      9.850456   10.000000     80.000000   \n",
       "pp           67496.0     45.848855     21.188590    0.000000     40.000000   \n",
       "pp%          67496.0     35.470788      5.718729    0.000000     33.333333   \n",
       "heart_alarm  67496.0      0.019557      0.138472    0.000000      0.000000   \n",
       "\n",
       "                      50%           75%           max  \n",
       "id           50007.500000  74874.500000  99999.000000  \n",
       "cholesterol      0.000000      0.500000      1.000000  \n",
       "gluc             0.000000      0.000000      1.000000  \n",
       "smoke            0.000000      0.000000      1.000000  \n",
       "alco             0.000000      0.000000      1.000000  \n",
       "active           1.000000      1.000000      1.000000  \n",
       "diabetes         0.000000      1.000000      1.000000  \n",
       "age             53.927447     58.294319     64.889802  \n",
       "height         165.000000    170.000000    207.000000  \n",
       "weight          71.000000     82.000000    200.000000  \n",
       "gender           1.000000      1.000000      1.000000  \n",
       "BMI             26.298488     30.346074     69.827553  \n",
       "diastolic      120.000000    140.000000   1602.000000  \n",
       "systolic        80.000000     90.000000    200.000000  \n",
       "pp              40.000000     50.000000   1522.000000  \n",
       "pp%             33.333333     38.461538     95.210728  \n",
       "heart_alarm      0.000000      0.000000      1.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.001226</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>-0.006008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol</th>\n",
       "      <td>0.006560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>0.036581</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.453115</td>\n",
       "      <td>0.154354</td>\n",
       "      <td>-0.053847</td>\n",
       "      <td>0.144656</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>0.176078</td>\n",
       "      <td>0.131520</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>0.080727</td>\n",
       "      <td>0.068913</td>\n",
       "      <td>-0.003634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluc</th>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004886</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.562886</td>\n",
       "      <td>0.099142</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.108893</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0.121912</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.074189</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.039033</td>\n",
       "      <td>-0.003765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoke</th>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>-0.004886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339311</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.068955</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>0.197454</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>-0.338473</td>\n",
       "      <td>-0.030124</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.006743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alco</th>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.036581</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.339311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>0.054047</td>\n",
       "      <td>-0.029097</td>\n",
       "      <td>0.098736</td>\n",
       "      <td>0.068181</td>\n",
       "      <td>-0.171916</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.005112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>-0.009318</td>\n",
       "      <td>-0.009228</td>\n",
       "      <td>-0.016981</td>\n",
       "      <td>-0.005726</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>-0.002117</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>-0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.453115</td>\n",
       "      <td>0.562886</td>\n",
       "      <td>0.068955</td>\n",
       "      <td>0.054047</td>\n",
       "      <td>-0.271240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.274208</td>\n",
       "      <td>0.140675</td>\n",
       "      <td>0.370537</td>\n",
       "      <td>-0.225287</td>\n",
       "      <td>0.305926</td>\n",
       "      <td>0.235056</td>\n",
       "      <td>0.325363</td>\n",
       "      <td>0.126963</td>\n",
       "      <td>0.042512</td>\n",
       "      <td>0.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.154354</td>\n",
       "      <td>0.099142</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.029097</td>\n",
       "      <td>-0.009318</td>\n",
       "      <td>0.274208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.085318</td>\n",
       "      <td>0.055008</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.102328</td>\n",
       "      <td>0.140312</td>\n",
       "      <td>0.152045</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.096833</td>\n",
       "      <td>-0.032597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.053847</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.197454</td>\n",
       "      <td>0.098736</td>\n",
       "      <td>-0.009228</td>\n",
       "      <td>0.140675</td>\n",
       "      <td>-0.085318</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314788</td>\n",
       "      <td>-0.524954</td>\n",
       "      <td>-0.180802</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>-0.014583</td>\n",
       "      <td>-0.014267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.001226</td>\n",
       "      <td>0.144656</td>\n",
       "      <td>0.108893</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>0.068181</td>\n",
       "      <td>-0.016981</td>\n",
       "      <td>0.370537</td>\n",
       "      <td>0.055008</td>\n",
       "      <td>0.314788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.159332</td>\n",
       "      <td>0.872067</td>\n",
       "      <td>0.188869</td>\n",
       "      <td>0.257381</td>\n",
       "      <td>0.103899</td>\n",
       "      <td>0.060212</td>\n",
       "      <td>-0.011328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>-0.338473</td>\n",
       "      <td>-0.171916</td>\n",
       "      <td>-0.005726</td>\n",
       "      <td>-0.225287</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>-0.524954</td>\n",
       "      <td>-0.159332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105444</td>\n",
       "      <td>-0.044791</td>\n",
       "      <td>-0.068061</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.176078</td>\n",
       "      <td>0.121912</td>\n",
       "      <td>-0.030124</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>0.305926</td>\n",
       "      <td>0.102328</td>\n",
       "      <td>-0.180802</td>\n",
       "      <td>0.872067</td>\n",
       "      <td>0.105444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186523</td>\n",
       "      <td>0.248969</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>0.069828</td>\n",
       "      <td>-0.005011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic</th>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.131520</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.235056</td>\n",
       "      <td>0.140312</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.188869</td>\n",
       "      <td>-0.044791</td>\n",
       "      <td>0.186523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.560755</td>\n",
       "      <td>0.922948</td>\n",
       "      <td>0.444070</td>\n",
       "      <td>-0.096264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic</th>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.161210</td>\n",
       "      <td>0.074189</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.325363</td>\n",
       "      <td>0.152045</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.257381</td>\n",
       "      <td>-0.068061</td>\n",
       "      <td>0.248969</td>\n",
       "      <td>0.560755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198838</td>\n",
       "      <td>-0.219810</td>\n",
       "      <td>0.100740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pp</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.080727</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>-0.002117</td>\n",
       "      <td>0.126963</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.103899</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>0.922948</td>\n",
       "      <td>0.198838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.627807</td>\n",
       "      <td>-0.160775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pp%</th>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.068913</td>\n",
       "      <td>0.039033</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>-0.000939</td>\n",
       "      <td>0.042512</td>\n",
       "      <td>0.096833</td>\n",
       "      <td>-0.014583</td>\n",
       "      <td>0.060212</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>0.069828</td>\n",
       "      <td>0.444070</td>\n",
       "      <td>-0.219810</td>\n",
       "      <td>0.627807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.391498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_alarm</th>\n",
       "      <td>-0.006008</td>\n",
       "      <td>-0.003634</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.000663</td>\n",
       "      <td>0.011043</td>\n",
       "      <td>-0.032597</td>\n",
       "      <td>-0.014267</td>\n",
       "      <td>-0.011328</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>-0.005011</td>\n",
       "      <td>-0.096264</td>\n",
       "      <td>0.100740</td>\n",
       "      <td>-0.160775</td>\n",
       "      <td>-0.391498</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  cholesterol      gluc     smoke      alco    active  \\\n",
       "id           1.000000     0.006560  0.002305 -0.004180  0.001242  0.003760   \n",
       "cholesterol  0.006560     1.000000  0.453300  0.010188  0.036581  0.008565   \n",
       "gluc         0.002305     0.453300  1.000000 -0.004886  0.011325 -0.007937   \n",
       "smoke       -0.004180     0.010188 -0.004886  1.000000  0.339311  0.024834   \n",
       "alco         0.001242     0.036581  0.011325  0.339311  1.000000  0.025724   \n",
       "active       0.003760     0.008565 -0.007937  0.024834  0.025724  1.000000   \n",
       "diabetes     0.004734     0.453115  0.562886  0.068955  0.054047 -0.271240   \n",
       "age          0.002831     0.154354  0.099142 -0.046598 -0.029097 -0.009318   \n",
       "height      -0.003799    -0.053847 -0.019509  0.197454  0.098736 -0.009228   \n",
       "weight      -0.001226     0.144656  0.108893  0.069042  0.068181 -0.016981   \n",
       "gender      -0.002913     0.035613  0.020039 -0.338473 -0.171916 -0.005726   \n",
       "BMI          0.000242     0.176078  0.121912 -0.030124  0.019118 -0.013471   \n",
       "diastolic    0.000453     0.131520  0.066148  0.016737  0.026746 -0.000729   \n",
       "systolic     0.000972     0.161210  0.074189  0.025115  0.038655  0.002697   \n",
       "pp           0.000085     0.080727  0.043805  0.008135  0.013687 -0.002117   \n",
       "pp%          0.002118     0.068913  0.039033  0.006658  0.002531 -0.000939   \n",
       "heart_alarm -0.006008    -0.003634 -0.003765  0.006743  0.005112 -0.000663   \n",
       "\n",
       "             diabetes       age    height    weight    gender       BMI  \\\n",
       "id           0.004734  0.002831 -0.003799 -0.001226 -0.002913  0.000242   \n",
       "cholesterol  0.453115  0.154354 -0.053847  0.144656  0.035613  0.176078   \n",
       "gluc         0.562886  0.099142 -0.019509  0.108893  0.020039  0.121912   \n",
       "smoke        0.068955 -0.046598  0.197454  0.069042 -0.338473 -0.030124   \n",
       "alco         0.054047 -0.029097  0.098736  0.068181 -0.171916  0.019118   \n",
       "active      -0.271240 -0.009318 -0.009228 -0.016981 -0.005726 -0.013471   \n",
       "diabetes     1.000000  0.274208  0.140675  0.370537 -0.225287  0.305926   \n",
       "age          0.274208  1.000000 -0.085318  0.055008  0.022304  0.102328   \n",
       "height       0.140675 -0.085318  1.000000  0.314788 -0.524954 -0.180802   \n",
       "weight       0.370537  0.055008  0.314788  1.000000 -0.159332  0.872067   \n",
       "gender      -0.225287  0.022304 -0.524954 -0.159332  1.000000  0.105444   \n",
       "BMI          0.305926  0.102328 -0.180802  0.872067  0.105444  1.000000   \n",
       "diastolic    0.235056  0.140312  0.018714  0.188869 -0.044791  0.186523   \n",
       "systolic     0.325363  0.152045  0.035009  0.257381 -0.068061  0.248969   \n",
       "pp           0.126963  0.095395  0.005875  0.103899 -0.021375  0.105032   \n",
       "pp%          0.042512  0.096833 -0.014583  0.060212 -0.004605  0.069828   \n",
       "heart_alarm  0.011043 -0.032597 -0.014267 -0.011328  0.008186 -0.005011   \n",
       "\n",
       "             diastolic  systolic        pp       pp%  heart_alarm  \n",
       "id            0.000453  0.000972  0.000085  0.002118    -0.006008  \n",
       "cholesterol   0.131520  0.161210  0.080727  0.068913    -0.003634  \n",
       "gluc          0.066148  0.074189  0.043805  0.039033    -0.003765  \n",
       "smoke         0.016737  0.025115  0.008135  0.006658     0.006743  \n",
       "alco          0.026746  0.038655  0.013687  0.002531     0.005112  \n",
       "active       -0.000729  0.002697 -0.002117 -0.000939    -0.000663  \n",
       "diabetes      0.235056  0.325363  0.126963  0.042512     0.011043  \n",
       "age           0.140312  0.152045  0.095395  0.096833    -0.032597  \n",
       "height        0.018714  0.035009  0.005875 -0.014583    -0.014267  \n",
       "weight        0.188869  0.257381  0.103899  0.060212    -0.011328  \n",
       "gender       -0.044791 -0.068061 -0.021375 -0.004605     0.008186  \n",
       "BMI           0.186523  0.248969  0.105032  0.069828    -0.005011  \n",
       "diastolic     1.000000  0.560755  0.922948  0.444070    -0.096264  \n",
       "systolic      0.560755  1.000000  0.198838 -0.219810     0.100740  \n",
       "pp            0.922948  0.198838  1.000000  0.627807    -0.160775  \n",
       "pp%           0.444070 -0.219810  0.627807  1.000000    -0.391498  \n",
       "heart_alarm  -0.096264  0.100740 -0.160775 -0.391498     1.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>49894.047668</td>\n",
       "      <td>0.094792</td>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.076684</td>\n",
       "      <td>0.046877</td>\n",
       "      <td>0.865778</td>\n",
       "      <td>52.084167</td>\n",
       "      <td>163.794098</td>\n",
       "      <td>71.079021</td>\n",
       "      <td>0.714051</td>\n",
       "      <td>26.538284</td>\n",
       "      <td>124.050390</td>\n",
       "      <td>79.758614</td>\n",
       "      <td>44.291776</td>\n",
       "      <td>35.330073</td>\n",
       "      <td>0.018672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>50209.018774</td>\n",
       "      <td>0.450850</td>\n",
       "      <td>0.392520</td>\n",
       "      <td>0.121738</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>0.617133</td>\n",
       "      <td>56.361907</td>\n",
       "      <td>166.330559</td>\n",
       "      <td>83.536108</td>\n",
       "      <td>0.466525</td>\n",
       "      <td>30.263675</td>\n",
       "      <td>137.647367</td>\n",
       "      <td>87.150809</td>\n",
       "      <td>50.496558</td>\n",
       "      <td>35.890806</td>\n",
       "      <td>0.022199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  cholesterol      gluc     smoke      alco    active  \\\n",
       "diabetes                                                                      \n",
       "0.0       49894.047668     0.094792  0.020294  0.076684  0.046877  0.865778   \n",
       "1.0       50209.018774     0.450850  0.392520  0.121738  0.075038  0.617133   \n",
       "\n",
       "                age      height     weight    gender        BMI   diastolic  \\\n",
       "diabetes                                                                      \n",
       "0.0       52.084167  163.794098  71.079021  0.714051  26.538284  124.050390   \n",
       "1.0       56.361907  166.330559  83.536108  0.466525  30.263675  137.647367   \n",
       "\n",
       "           systolic         pp        pp%  heart_alarm  \n",
       "diabetes                                                \n",
       "0.0       79.758614  44.291776  35.330073     0.018672  \n",
       "1.0       87.150809  50.496558  35.890806     0.022199  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('diabetes').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>pp</th>\n",
       "      <th>pp%</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>28829.460011</td>\n",
       "      <td>0.242026</td>\n",
       "      <td>0.106291</td>\n",
       "      <td>0.266092</td>\n",
       "      <td>0.211377</td>\n",
       "      <td>0.340894</td>\n",
       "      <td>6.732974</td>\n",
       "      <td>7.521489</td>\n",
       "      <td>12.327721</td>\n",
       "      <td>0.451870</td>\n",
       "      <td>4.625906</td>\n",
       "      <td>19.533606</td>\n",
       "      <td>8.804110</td>\n",
       "      <td>16.352467</td>\n",
       "      <td>5.546075</td>\n",
       "      <td>0.135364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>28901.135136</td>\n",
       "      <td>0.439015</td>\n",
       "      <td>0.435961</td>\n",
       "      <td>0.326993</td>\n",
       "      <td>0.263461</td>\n",
       "      <td>0.486101</td>\n",
       "      <td>5.768963</td>\n",
       "      <td>8.357365</td>\n",
       "      <td>16.636469</td>\n",
       "      <td>0.498893</td>\n",
       "      <td>6.067179</td>\n",
       "      <td>35.058645</td>\n",
       "      <td>10.694292</td>\n",
       "      <td>31.017414</td>\n",
       "      <td>6.186746</td>\n",
       "      <td>0.147333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  cholesterol      gluc     smoke      alco    active  \\\n",
       "diabetes                                                                      \n",
       "0.0       28829.460011     0.242026  0.106291  0.266092  0.211377  0.340894   \n",
       "1.0       28901.135136     0.439015  0.435961  0.326993  0.263461  0.486101   \n",
       "\n",
       "               age    height     weight    gender       BMI  diastolic  \\\n",
       "diabetes                                                                 \n",
       "0.0       6.732974  7.521489  12.327721  0.451870  4.625906  19.533606   \n",
       "1.0       5.768963  8.357365  16.636469  0.498893  6.067179  35.058645   \n",
       "\n",
       "           systolic         pp       pp%  heart_alarm  \n",
       "diabetes                                               \n",
       "0.0        8.804110  16.352467  5.546075     0.135364  \n",
       "1.0       10.694292  31.017414  6.186746     0.147333  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('diabetes').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_pandas in c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.1 in c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages (from sklearn_pandas) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages (from sklearn_pandas) (1.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ksi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->sklearn_pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user sklearn_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ksi\\\\IT\\\\Toptal dijabetes Ivan'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version            \n",
      "---------------------------------- -------------------\n",
      "alabaster                          0.7.12             \n",
      "anaconda-client                    1.7.2              \n",
      "anaconda-navigator                 1.9.12             \n",
      "anaconda-project                   0.8.3              \n",
      "argh                               0.26.2             \n",
      "asn1crypto                         1.3.0              \n",
      "astroid                            2.3.3              \n",
      "astropy                            4.0                \n",
      "atomicwrites                       1.3.0              \n",
      "attrs                              19.3.0             \n",
      "autopep8                           1.4.4              \n",
      "Babel                              2.8.0              \n",
      "backcall                           0.1.0              \n",
      "backports.functools-lru-cache      1.6.1              \n",
      "backports.shutil-get-terminal-size 1.0.0              \n",
      "backports.tempfile                 1.0                \n",
      "backports.weakref                  1.0.post1          \n",
      "bcrypt                             3.1.7              \n",
      "beautifulsoup4                     4.8.2              \n",
      "bitarray                           1.2.1              \n",
      "bkcharts                           0.2                \n",
      "bleach                             3.1.0              \n",
      "bokeh                              1.4.0              \n",
      "boto                               2.49.0             \n",
      "Bottleneck                         1.3.2              \n",
      "certifi                            2019.11.28         \n",
      "cffi                               1.14.0             \n",
      "chardet                            3.0.4              \n",
      "Click                              7.0                \n",
      "cloudpickle                        1.3.0              \n",
      "clyent                             1.2.2              \n",
      "colorama                           0.4.3              \n",
      "comtypes                           1.1.7              \n",
      "conda                              4.8.2              \n",
      "conda-build                        3.18.11            \n",
      "conda-package-handling             1.6.0              \n",
      "conda-verify                       3.4.2              \n",
      "contextlib2                        0.6.0.post1        \n",
      "cryptography                       2.8                \n",
      "cycler                             0.10.0             \n",
      "Cython                             0.29.15            \n",
      "cytoolz                            0.10.1             \n",
      "dask                               2.11.0             \n",
      "decorator                          4.4.1              \n",
      "defusedxml                         0.6.0              \n",
      "diff-match-patch                   20181111           \n",
      "distributed                        2.11.0             \n",
      "docutils                           0.16               \n",
      "entrypoints                        0.3                \n",
      "et-xmlfile                         1.0.1              \n",
      "fastcache                          1.1.0              \n",
      "filelock                           3.0.12             \n",
      "flake8                             3.7.9              \n",
      "Flask                              1.1.1              \n",
      "fsspec                             0.6.2              \n",
      "future                             0.18.2             \n",
      "gevent                             1.4.0              \n",
      "glob2                              0.7                \n",
      "greenlet                           0.4.15             \n",
      "h5py                               2.10.0             \n",
      "HeapDict                           1.0.1              \n",
      "html5lib                           1.0.1              \n",
      "hypothesis                         5.5.4              \n",
      "idna                               2.8                \n",
      "imageio                            2.6.1              \n",
      "imagesize                          1.2.0              \n",
      "importlib-metadata                 1.5.0              \n",
      "intervaltree                       3.0.2              \n",
      "ipykernel                          5.1.4              \n",
      "ipython                            7.12.0             \n",
      "ipython-genutils                   0.2.0              \n",
      "ipywidgets                         7.5.1              \n",
      "isort                              4.3.21             \n",
      "itsdangerous                       1.1.0              \n",
      "jdcal                              1.4.1              \n",
      "jedi                               0.14.1             \n",
      "Jinja2                             2.11.1             \n",
      "joblib                             0.14.1             \n",
      "json5                              0.9.1              \n",
      "jsonschema                         3.2.0              \n",
      "jupyter                            1.0.0              \n",
      "jupyter-client                     5.3.4              \n",
      "jupyter-console                    6.1.0              \n",
      "jupyter-core                       4.6.1              \n",
      "jupyterlab                         1.2.6              \n",
      "jupyterlab-server                  1.0.6              \n",
      "keyring                            21.1.0             \n",
      "kiwisolver                         1.1.0              \n",
      "lazy-object-proxy                  1.4.3              \n",
      "libarchive-c                       2.8                \n",
      "llvmlite                           0.31.0             \n",
      "locket                             0.2.0              \n",
      "lxml                               4.5.0              \n",
      "MarkupSafe                         1.1.1              \n",
      "matplotlib                         3.1.3              \n",
      "mccabe                             0.6.1              \n",
      "menuinst                           1.4.16             \n",
      "mistune                            0.8.4              \n",
      "mkl-fft                            1.0.15             \n",
      "mkl-random                         1.1.0              \n",
      "mkl-service                        2.3.0              \n",
      "mock                               4.0.1              \n",
      "more-itertools                     8.2.0              \n",
      "mpmath                             1.1.0              \n",
      "msgpack                            0.6.1              \n",
      "multipledispatch                   0.6.0              \n",
      "navigator-updater                  0.2.1              \n",
      "nbconvert                          5.6.1              \n",
      "nbformat                           5.0.4              \n",
      "networkx                           2.4                \n",
      "nltk                               3.4.5              \n",
      "nose                               1.3.7              \n",
      "notebook                           6.0.3              \n",
      "numba                              0.48.0             \n",
      "numexpr                            2.7.1              \n",
      "numpy                              1.18.1             \n",
      "numpydoc                           0.9.2              \n",
      "olefile                            0.46               \n",
      "openpyxl                           3.0.3              \n",
      "packaging                          20.1               \n",
      "pandas                             1.3.5              \n",
      "pandocfilters                      1.4.2              \n",
      "paramiko                           2.7.1              \n",
      "parso                              0.5.2              \n",
      "partd                              1.1.0              \n",
      "path                               13.1.0             \n",
      "pathlib2                           2.3.5              \n",
      "pathtools                          0.1.2              \n",
      "patsy                              0.5.1              \n",
      "pep8                               1.7.1              \n",
      "pexpect                            4.8.0              \n",
      "pickleshare                        0.7.5              \n",
      "Pillow                             7.0.0              \n",
      "pip                                20.0.2             \n",
      "pkginfo                            1.5.0.1            \n",
      "pluggy                             0.13.1             \n",
      "ply                                3.11               \n",
      "prometheus-client                  0.7.1              \n",
      "prompt-toolkit                     3.0.3              \n",
      "psutil                             5.6.7              \n",
      "py                                 1.8.1              \n",
      "pycodestyle                        2.5.0              \n",
      "pycosat                            0.6.3              \n",
      "pycparser                          2.19               \n",
      "pycrypto                           2.6.1              \n",
      "pycurl                             7.43.0.5           \n",
      "pydocstyle                         4.0.1              \n",
      "pyflakes                           2.1.1              \n",
      "Pygments                           2.5.2              \n",
      "pylint                             2.4.4              \n",
      "PyNaCl                             1.3.0              \n",
      "pyodbc                             4.0.0-unsupported  \n",
      "pyOpenSSL                          19.1.0             \n",
      "pyparsing                          2.4.6              \n",
      "pyreadline                         2.1                \n",
      "pyrsistent                         0.15.7             \n",
      "PySocks                            1.7.1              \n",
      "pytest                             5.3.5              \n",
      "pytest-arraydiff                   0.3                \n",
      "pytest-astropy                     0.8.0              \n",
      "pytest-astropy-header              0.1.2              \n",
      "pytest-doctestplus                 0.5.0              \n",
      "pytest-openfiles                   0.4.0              \n",
      "pytest-remotedata                  0.3.2              \n",
      "python-dateutil                    2.8.1              \n",
      "python-jsonrpc-server              0.3.4              \n",
      "python-language-server             0.31.7             \n",
      "pytz                               2019.3             \n",
      "PyWavelets                         1.1.1              \n",
      "pywin32                            227                \n",
      "pywin32-ctypes                     0.2.0              \n",
      "pywinpty                           0.5.7              \n",
      "PyYAML                             5.3                \n",
      "pyzmq                              18.1.1             \n",
      "QDarkStyle                         2.8                \n",
      "QtAwesome                          0.6.1              \n",
      "qtconsole                          4.6.0              \n",
      "QtPy                               1.9.0              \n",
      "requests                           2.22.0             \n",
      "rope                               0.16.0             \n",
      "Rtree                              0.9.3              \n",
      "ruamel-yaml                        0.15.87            \n",
      "scikit-image                       0.16.2             \n",
      "scikit-learn                       1.0.2              \n",
      "scipy                              1.7.3              \n",
      "seaborn                            0.10.0             \n",
      "Send2Trash                         1.5.0              \n",
      "setuptools                         45.2.0.post20200210\n",
      "simplegeneric                      0.8.1              \n",
      "singledispatch                     3.4.0.3            \n",
      "six                                1.14.0             \n",
      "sklearn-pandas                     2.2.0              \n",
      "snowballstemmer                    2.0.0              \n",
      "sortedcollections                  1.1.2              \n",
      "sortedcontainers                   2.1.0              \n",
      "soupsieve                          1.9.5              \n",
      "Sphinx                             2.4.0              \n",
      "sphinxcontrib-applehelp            1.0.1              \n",
      "sphinxcontrib-devhelp              1.0.1              \n",
      "sphinxcontrib-htmlhelp             1.0.2              \n",
      "sphinxcontrib-jsmath               1.0.1              \n",
      "sphinxcontrib-qthelp               1.0.2              \n",
      "sphinxcontrib-serializinghtml      1.1.3              \n",
      "sphinxcontrib-websupport           1.2.0              \n",
      "spyder                             4.0.1              \n",
      "spyder-kernels                     1.8.1              \n",
      "SQLAlchemy                         1.3.13             \n",
      "statsmodels                        0.11.0             \n",
      "sympy                              1.5.1              \n",
      "tables                             3.6.1              \n",
      "tblib                              1.6.0              \n",
      "terminado                          0.8.3              \n",
      "testpath                           0.4.4              \n",
      "threadpoolctl                      3.1.0              \n",
      "toolz                              0.10.0             \n",
      "tornado                            6.0.3              \n",
      "tqdm                               4.42.1             \n",
      "traitlets                          4.3.3              \n",
      "ujson                              1.35               \n",
      "unicodecsv                         0.14.1             \n",
      "urllib3                            1.25.8             \n",
      "watchdog                           0.10.2             \n",
      "wcwidth                            0.1.8              \n",
      "webencodings                       0.5.1              \n",
      "Werkzeug                           1.0.0              \n",
      "wheel                              0.34.2             \n",
      "widgetsnbextension                 3.5.1              \n",
      "win-inet-pton                      1.1.0              \n",
      "win-unicode-console                0.5                \n",
      "wincertstore                       0.2                \n",
      "wrapt                              1.11.2             \n",
      "xlrd                               1.2.0              \n",
      "XlsxWriter                         1.2.7              \n",
      "xlwings                            0.17.1             \n",
      "xlwt                               1.3.0              \n",
      "xmltodict                          0.12.0             \n",
      "yapf                               0.28.0             \n",
      "zict                               1.0.0              \n",
      "zipp                               2.2.0              \n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.0.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model - The :mod:`sklearn.linear_model` module implements a variety of linear models.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _bayes\n",
      "    _cd_fast\n",
      "    _coordinate_descent\n",
      "    _glm (package)\n",
      "    _huber\n",
      "    _least_angle\n",
      "    _logistic\n",
      "    _omp\n",
      "    _passive_aggressive\n",
      "    _perceptron\n",
      "    _quantile\n",
      "    _ransac\n",
      "    _ridge\n",
      "    _sag\n",
      "    _sag_fast\n",
      "    _sgd_fast\n",
      "    _stochastic_gradient\n",
      "    _theil_sen\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MultiOutputMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.linear_model._stochastic_gradient.SGDOneClassSVM(sklearn.linear_model._stochastic_gradient.BaseSGD, sklearn.base.OutlierMixin)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._base.LinearModel(sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._coordinate_descent.LinearModelCV(sklearn.base.MultiOutputMixin, sklearn.linear_model._base.LinearModel, abc.ABC)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "    sklearn.linear_model._glm.glm.GeneralizedLinearRegressor(sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._glm.glm.GammaRegressor\n",
      "        sklearn.linear_model._glm.glm.PoissonRegressor\n",
      "        sklearn.linear_model._glm.glm.TweedieRegressor\n",
      "    sklearn.linear_model._ridge._BaseRidge(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "    sklearn.linear_model._ridge._BaseRidgeCV(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._ridge._RidgeClassifierMixin(sklearn.linear_model._base.LinearClassifierMixin)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._sgd_fast.Classification(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Hinge\n",
      "        sklearn.linear_model._sgd_fast.Log\n",
      "        sklearn.linear_model._sgd_fast.ModifiedHuber\n",
      "    sklearn.linear_model._sgd_fast.Regression(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Huber\n",
      "        sklearn.linear_model._sgd_fast.SquaredLoss\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGD(sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._stochastic_gradient.SGDOneClassSVM(sklearn.linear_model._stochastic_gradient.BaseSGD, sklearn.base.OutlierMixin)\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model._perceptron.Perceptron\n",
      "        sklearn.linear_model._stochastic_gradient.SGDClassifier\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model._stochastic_gradient.SGDRegressor\n",
      "    \n",
      "    class ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ARDRegression(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |  \n",
      "     |  threshold_lambda : float, default=10 000\n",
      "     |      Threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array-like of shape (n_features,)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  X_offset_ : float\n",
      "     |      If `normalize=True`, offset subtracted for centering data to a\n",
      "     |      zero mean.\n",
      "     |  \n",
      "     |  X_scale_ : float\n",
      "     |      If `normalize=True`, parameter used to scale data to a unit\n",
      "     |      standard deviation.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BayesianRidge : Bayesian ridge regression.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/linear_model/plot_ard.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      "     |  competition, ASHRAE Transactions, 1994.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  ARD is a little different than the slide: only dimensions/features for\n",
      "     |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      "     |  discarded.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ARDRegression()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers). Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  BayesianRidge(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ridge regression.\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model. See the Notes section for details on this\n",
      "     |  implementation and the optimization of the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations. Should be greater than or equal to 1.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  alpha_init : float, default=None\n",
      "     |      Initial value for alpha (precision of the noise).\n",
      "     |      If not set, alpha_init is 1/Var(y).\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  lambda_init : float, default=None\n",
      "     |      Initial value for lambda (precision of the weights).\n",
      "     |      If not set, lambda_init is 1.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the log marginal likelihood at each iteration of the\n",
      "     |      optimization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model.\n",
      "     |      The intercept is not treated as a probabilistic parameter\n",
      "     |      and thus has no associated variance. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     Estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     Estimated precision of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      Estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : array-like of shape (n_iter_+1,)\n",
      "     |      If computed_score is True, value of the log marginal likelihood (to be\n",
      "     |      maximized) at each iteration of the optimization. The array starts\n",
      "     |      with the value of the log marginal likelihood obtained for the initial\n",
      "     |      values of alpha and lambda and ends with the value obtained for the\n",
      "     |      estimated alpha and lambda.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  X_offset_ : float\n",
      "     |      If `normalize=True`, offset subtracted for centering data to a\n",
      "     |      zero mean.\n",
      "     |  \n",
      "     |  X_scale_ : float\n",
      "     |      If `normalize=True`, parameter used to scale data to a unit\n",
      "     |      standard deviation.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ARDRegression : Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There exist several strategies to perform Bayesian ridge regression. This\n",
      "     |  implementation is based on the algorithm described in Appendix A of\n",
      "     |  (Tipping, 2001) where updates of the regularization parameters are done as\n",
      "     |  suggested in (MacKay, 1992). Note that according to A New\n",
      "     |  View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
      "     |  update rules do not guarantee that the marginal likelihood is increasing\n",
      "     |  between two consecutive iterations of the optimization.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      "     |  Vol. 4, No. 3, 1992.\n",
      "     |  \n",
      "     |  M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
      "     |  Journal of Machine Learning Research, Vol. 1, 2001.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  BayesianRidge()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |             parameter *sample_weight* support to BayesianRidge.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
      "     |      solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  SGDRegressor : Implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier : Implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNet\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNet(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNet(random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [18.83816048 64.55968825]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  1.451...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [1.451...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. Internally, the `sample_weight` vector will be\n",
      "     |          rescaled to sum to `n_samples`.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Elastic Net model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      Float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``.\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If None alphas are set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  enet_path : Compute elastic net path with coordinate descent.\n",
      "     |  ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNetCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> print(regr.alpha_)\n",
      "     |  0.199...\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  0.398...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [0.398...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      "     |          Sample weights used for fitting and evaluation of the weighted\n",
      "     |          mean squared error of each cv-fold. Note that the cross validated\n",
      "     |          MSE that is finally used to find the best model is the unweighted\n",
      "     |          mean over the (weighted) MSEs of each test fold.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GammaRegressor(GeneralizedLinearRegressor)\n",
      "     |  GammaRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X * coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n",
      "     |  TweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.GammaRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [19, 26, 33, 30]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  GammaRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.773...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.072..., 0.066...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  2.896...\n",
      "     |  >>> clf.predict([[1, 0], [2, 8]])\n",
      "     |  array([19.483..., 35.795...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GammaRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |      Return the family of the regressor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Hinge(Classification)\n",
      "     |  Hinge loss for binary classification tasks with y in {-1,1}\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  threshold : float > 0.0\n",
      "     |      Margin threshold. When threshold=1.0, one gets the loss used by SVM.\n",
      "     |      When threshold=0.0, one gets the loss used by the Perceptron.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "     |  \n",
      "     |  py_loss(...)\n",
      "     |      Python version of `loss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x + intercept`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The loss evaluated at `p` and `y`.\n",
      "    \n",
      "    class Huber(Regression)\n",
      "     |  Huber regression loss\n",
      "     |  \n",
      "     |  Variant of the SquaredLoss that is robust to outliers (quadratic near zero,\n",
      "     |  linear in for large errors).\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Huber_Loss_Function\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Huber\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "     |  \n",
      "     |  py_loss(...)\n",
      "     |      Python version of `loss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x + intercept`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The loss evaluated at `p` and `y`.\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  HuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |  \n",
      "     |  Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n",
      "     |  to be optimized. The parameter sigma makes sure that if y is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale epsilon to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of X may be of different scales.\n",
      "     |  \n",
      "     |  This makes sure that the loss function is not heavily influenced by the\n",
      "     |  outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, greater than 1.0, default=1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Regularization parameter.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default=1e-05\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - X'w - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  outliers_ : array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, pg 172\n",
      "     |  .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n",
      "     |         https://statweb.stanford.edu/~owen/reports/hhu.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X, y, coef = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n",
      "     |  >>> X[:4] = rng.uniform(10, 20, (4, 2))\n",
      "     |  >>> y[:4] = rng.uniform(10, 20, 4)\n",
      "     |  >>> huber = HuberRegressor().fit(X, y)\n",
      "     |  >>> huber.score(X, y)\n",
      "     |  -7.284...\n",
      "     |  >>> huber.predict(X[:1,])\n",
      "     |  array([806.7200...])\n",
      "     |  >>> linear = LinearRegression().fit(X, y)\n",
      "     |  >>> print(\"True coefficients:\", coef)\n",
      "     |  True coefficients: [20.4923...  34.1698...]\n",
      "     |  >>> print(\"Huber coefficients:\", huber.coef_)\n",
      "     |  Huber coefficients: [17.7906... 31.0106...]\n",
      "     |  >>> print(\"Linear Regression coefficients:\", linear.coef_)\n",
      "     |  Linear Regression coefficients: [-1.9221...  7.0226...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `HuberRegressor` estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  Lars(*, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Least Angle Regression model a.k.a. LAR.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  n_nonzero_coefs : int, default=500\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of shape (n_alphas,) or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path: Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  LarsCV : Cross-validated Least Angle Regression model.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.Lars(n_nonzero_coefs=1, normalize=False)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  Lars(n_nonzero_coefs=1, normalize=False)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0. -1.11...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  LarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize='deprecated', precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |  \n",
      "     |  Cross-validated Least Angle Regression model.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of lists, the outer list length is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n",
      "     |  >>> reg = LarsCV(cv=5, normalize=False).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9996...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.2961...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([154.3996...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize='deprecated', precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Lasso(alpha=1.0, *, fit_intercept=True, normalize='deprecated', precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
      "     |      Readonly property derived from ``coef_``.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Regularization path using LARS.\n",
      "     |  lasso_path : Regularization path using Lasso.\n",
      "     |  LassoLars : Lasso Path along the regularization parameter usingLARS algorithm.\n",
      "     |  LassoCV : Lasso alpha parameter by cross-validation.\n",
      "     |  LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [0.85 0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. Internally, the `sample_weight` vector will be\n",
      "     |          rescaled to sum to `n_samples`.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  LassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Lasso linear model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If ``None`` alphas are set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      If positive, restrict regression coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |      algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |      path.\n",
      "     |  LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4951...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent.\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |          algorithm.\n",
      "     |      Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |      LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |      LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |          path.\n",
      "     |      LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |      sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "     |          transform signals into sparse linear combination of atoms from a fixed.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.linear_model import lasso_path\n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      "     |          Sample weights used for fitting and evaluation of the weighted\n",
      "     |          mean squared error of each cv-fold. Note that the cross validated\n",
      "     |          MSE that is finally used to find the best model is the unweighted\n",
      "     |          mean over the (weighted) MSEs of each test fold.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  LassoLars(alpha=1.0, *, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLars(alpha=0.01, normalize=False)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  LassoLars(alpha=0.01, normalize=False)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.         -0.955...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  LassoLarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize='deprecated', precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : bool or 'auto' , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  active_ : list of int\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The object solves the same problem as the LassoCV object. However,\n",
      "     |  unlike the LassoCV, it find the relevant alphas values by itself.\n",
      "     |  In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the LassoCV if only a small number of\n",
      "     |  features are selected compared to the total number, for instance if\n",
      "     |  there are very few samples compared to the number of features.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoLarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4.0, random_state=0)\n",
      "     |  >>> reg = LassoLarsCV(cv=5, normalize=False).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.3972...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4831...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize='deprecated', precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  LassoLarsIC(criterion='aic', *, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False, noise_variance=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion [2]_ and BIC is the Bayes\n",
      "     |  Information criterion [3]_. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso_lars_ic>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {'aic', 'bic'}, default='aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  noise_variance : float, default=None\n",
      "     |      The estimated noise variance of the data. If `None`, an unbiased\n",
      "     |      estimate is computed by an OLS model. However, it is only possible\n",
      "     |      in the case where `n_samples > n_features + fit_intercept`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If a list, it will be of length `n_targets`.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array-like of shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criterion is\n",
      "     |      chosen, as specified in [1]_.\n",
      "     |  \n",
      "     |  noise_variance_ : float\n",
      "     |      The estimated noise variance from the data used to compute the\n",
      "     |      criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The number of degrees of freedom is computed as in [1]_.\n",
      "     |  \n",
      "     |  To have more details regarding the mathematical formulation of the\n",
      "     |  AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n",
      "     |          \"On the degrees of freedom of the lasso.\"\n",
      "     |          The Annals of Statistics 35.5 (2007): 2173-2192.\n",
      "     |          <0712.0881>`\n",
      "     |  \n",
      "     |  .. [2] `Wikipedia entry on the Akaike information criterion\n",
      "     |          <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n",
      "     |  \n",
      "     |  .. [3] `Wikipedia entry on the Bayesian information criterion\n",
      "     |          <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLarsIC(criterion='bic', normalize=False)\n",
      "     |  >>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n",
      "     |  >>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  LassoLarsIC(criterion='bic', normalize=False)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.  -1.11...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', *, fit_intercept=True, verbose=False, normalize='deprecated', precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False, noise_variance=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      copy_X : bool, default=None\n",
      "     |          If provided, this parameter will override the choice\n",
      "     |          of copy_X made at instance creation.\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LassoLars:\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      "     |  LinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n",
      "     |  \n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      "     |  to minimize the residual sum of squares between the observed targets in\n",
      "     |  the dataset, and the targets predicted by the linear approximation.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         `normalize` was deprecated in version 1.0 and will be\n",
      "     |         removed in 1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This will only provide\n",
      "     |      speedup in case of sufficiently large problems, that is if firstly\n",
      "     |      `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
      "     |      to `True`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive. This\n",
      "     |      option is only supported for dense arrays.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  rank_ : int\n",
      "     |      Rank of matrix `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  singular_ : array of shape (min(X, y),)\n",
      "     |      Singular values of `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  intercept_ : float or array of shape (n_targets,)\n",
      "     |      Independent term in the linear model. Set to 0.0 if\n",
      "     |      `fit_intercept = False`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression addresses some of the\n",
      "     |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      "     |      size of the coefficients with l2 regularization.\n",
      "     |  Lasso : The Lasso is a linear model that estimates\n",
      "     |      sparse coefficients with l1 regularization.\n",
      "     |  ElasticNet : Elastic-Net is a linear regression\n",
      "     |      model trained with both l1 and l2 -norm regularization of the\n",
      "     |      coefficients.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      "     |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      "     |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      "     |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      "     |  >>> reg = LinearRegression().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  1.0\n",
      "     |  >>> reg.coef_\n",
      "     |  array([1., 2.])\n",
      "     |  >>> reg.intercept_\n",
      "     |  3.0...\n",
      "     |  >>> reg.predict(np.array([[3, 5]]))\n",
      "     |  array([16.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted Estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Log(Classification)\n",
      "     |  Logistic regression loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Log\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "     |  \n",
      "     |  py_loss(...)\n",
      "     |      Python version of `loss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x + intercept`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The loss evaluated at `p` and `y`.\n",
      "    \n",
      "    class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |  \n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "     |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "     |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "     |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "     |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "     |  that regularization is applied by default**. It can handle both dense\n",
      "     |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "     |  floats for optimal performance; any other input format will be converted\n",
      "     |  (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      "     |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      "     |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "     |  'saga' solver.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      "     |      Specify the norm of the penalty:\n",
      "     |  \n",
      "     |      - `'none'`: no penalty is added;\n",
      "     |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "     |      - `'l1'`: add a L1 penalty term;\n",
      "     |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         Some penalties may not work with some solvers. See the parameter\n",
      "     |         `solver` below, to know the compatibility between the penalty and\n",
      "     |         solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'*\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "     |      data. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "     |      To choose a solver, you might want to consider the following aspects:\n",
      "     |  \n",
      "     |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "     |            and 'saga' are faster for large ones;\n",
      "     |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "     |            'lbfgs' handle multinomial loss;\n",
      "     |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         The choice of the algorithm depends on the penalty chosen:\n",
      "     |         Supported penalties by solver:\n",
      "     |  \n",
      "     |         - 'newton-cg'   -   ['l2', 'none']\n",
      "     |         - 'lbfgs'       -   ['l2', 'none']\n",
      "     |         - 'liblinear'   -   ['l1', 'l2']\n",
      "     |         - 'sag'         -   ['l2', 'none']\n",
      "     |         - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |         features with approximately the same scale. You can\n",
      "     |         preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      "     |  \n",
      "     |      .. seealso::\n",
      "     |         Refer to the User Guide for more information regarding\n",
      "     |         :class:`LogisticRegression` and more specifically the\n",
      "     |         `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      "     |         summarazing solver/penalty supports.\n",
      "     |         <!--\n",
      "     |         # noqa: E501\n",
      "     |         -->\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used when parallelizing over classes if\n",
      "     |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "     |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "     |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors.\n",
      "     |      See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=None\n",
      "     |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "     |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "     |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "     |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "     |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "     |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "     |      outcome 0 (False).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log\"``).\n",
      "     |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "     |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "     |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "     |      SAGA: A Fast Incremental Gradient Method With Support\n",
      "     |      for Non-Strongly Convex Composite Objectives\n",
      "     |      https://arxiv.org/abs/1407.0202\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :])\n",
      "     |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "     |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.97...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |  \n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  Elastic-Net penalty is only supported by the saga solver.\n",
      "     |  \n",
      "     |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      "     |  is selected by the cross-validator\n",
      "     |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      "     |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : int or list of floats, default=10\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  cv : int or cross-validation generator, default=None\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      "     |      Specify the norm of the penalty:\n",
      "     |  \n",
      "     |      - `'l2'`: add a L2 penalty term (used by default);\n",
      "     |      - `'l1'`: add a L1 penalty term;\n",
      "     |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         Some penalties may not work with some solvers. See the parameter\n",
      "     |         `solver` below, to know the compatibility between the penalty and\n",
      "     |         solver.\n",
      "     |  \n",
      "     |  scoring : str or callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      "     |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      "     |      default scoring option used is 'accuracy'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "     |      To choose a solver, you might want to consider the following aspects:\n",
      "     |  \n",
      "     |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "     |            and 'saga' are faster for large ones;\n",
      "     |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "     |            'lbfgs' handle multinomial loss;\n",
      "     |          - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n",
      "     |            because it does not handle warm-starting. 'liblinear' is\n",
      "     |            limited to one-versus-rest schemes.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         The choice of the algorithm depends on the penalty chosen:\n",
      "     |  \n",
      "     |         - 'newton-cg'   -   ['l2']\n",
      "     |         - 'lbfgs'       -   ['l2']\n",
      "     |         - 'liblinear'   -   ['l1', 'l2']\n",
      "     |         - 'sag'         -   ['l2']\n",
      "     |         - 'saga'        -   ['elasticnet', 'l1', 'l2']\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "     |         with approximately the same scale. You can preprocess the data with\n",
      "     |         a scaler from :mod:`sklearn.preprocessing`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used during the cross-validation loop.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool, default=True\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      "     |      Note that this only applies to the solver and not the cross-validation\n",
      "     |      generator. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  l1_ratios : list of float, default=None\n",
      "     |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      "     |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      "     |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      "     |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      "     |      of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : ndarray of shape (n_cs)\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      "     |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      "     |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      "     |  \n",
      "     |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class. Each dict value\n",
      "     |      has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      "     |      ``penalty='elasticnet'``.\n",
      "     |  \n",
      "     |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of l1_ratio that maps to the best scores across every class. If\n",
      "     |      refit is set to False, then for each class, the best l1_ratio is the\n",
      "     |      average of the l1_ratio's that correspond to the best scores for each\n",
      "     |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      "     |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LogisticRegression : Logistic regression without tuning the\n",
      "     |      hyperparameter `C`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :]).shape\n",
      "     |  (2, 3)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.98...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted LogisticRegressionCV estimator.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Score using the `scoring` option on the given test data and labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Score of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ModifiedHuber(Classification)\n",
      "     |  Modified Huber loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  This is equivalent to quadratically smoothed SVM with gamma = 2.\n",
      "     |  \n",
      "     |  See T. Zhang 'Solving Large Scale Linear Prediction Problems Using\n",
      "     |  Stochastic Gradient Descent', ICML'04.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ModifiedHuber\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "     |  \n",
      "     |  py_loss(...)\n",
      "     |      Python version of `loss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x + intercept`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The loss evaluated at `p` and `y`.\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  MultiTaskElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n",
      "     |  \n",
      "     |  i.e. the sum of norms of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is\n",
      "     |      passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.45663524 0.45612256]\n",
      "     |   [0.45663524 0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.0872422 0.0872422]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``.\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      Best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  MultiTaskLassoCV : Multi-task Lasso model trained with L1/L2\n",
      "     |      mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.52875032 0.46958558]\n",
      "     |   [0.52875032 0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.00166409 0.00166409]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Training target variable. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns MultiTaskElasticNet instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  MultiTaskLasso(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : ndarray of shape (n_alphas,)\n",
      "     |      The dual gaps at the end of the optimization for each alpha.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
      "     |  MultiTaskLasso: Multi-task L1/L2 Lasso with built-in cross-validation.\n",
      "     |  MultiTaskElasticNet: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n",
      "     |  MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.         0.60809415]\n",
      "     |  [0.         0.94592424]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [-0.41888636 -0.87382323]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskLassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n",
      "     |      mixed-norm as regularizer.\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import MultiTaskLassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.metrics import r2_score\n",
      "     |  >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n",
      "     |  >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> r2_score(y, reg.predict(X))\n",
      "     |  0.9994...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.5713...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([[153.7971...,  94.9015...]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskLasso model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent.\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |          algorithm.\n",
      "     |      Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |      LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |      LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |          path.\n",
      "     |      LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |      sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "     |          transform signals into sparse linear combination of atoms from a fixed.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.linear_model import lasso_path\n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuit(*, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize='deprecated', precompute='auto')\n",
      "     |  \n",
      "     |  Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, default=None\n",
      "     |      Desired number of non-zero entries in the solution. If None (by\n",
      "     |      default) this value is set to 10% of n_features.\n",
      "     |  \n",
      "     |  tol : float, default=None\n",
      "     |      Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  precompute : 'auto' or bool, default='auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when :term:`n_targets` or\n",
      "     |      :term:`n_samples` is very large. Note that if you already have such\n",
      "     |      matrices, you can pass them directly to the fit method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      The number of non-zero coefficients in the solution. If\n",
      "     |      `n_nonzero_coefs` is None and `tol` is None this value is either set\n",
      "     |      to 10% of `n_features` or 1, whichever is greater.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "     |  orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit\n",
      "     |      problems using only the Gram matrix X.T * X and the product X.T * y.\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n",
      "     |  Lars : Least Angle Regression model a.k.a. LAR.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  sklearn.decomposition.sparse_encode : Generic sparse coding.\n",
      "     |      Each column of the result is the solution to a Lasso problem.\n",
      "     |  OrthogonalMatchingPursuitCV : Cross-validated\n",
      "     |      Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuit(normalize=False).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize='deprecated', precompute='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuitCV(*, copy=True, fit_intercept=True, normalize='deprecated', max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0. It will default\n",
      "     |          to False in 1.2 and be removed in 1.4.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "     |  orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit\n",
      "     |      problems using only the Gram matrix X.T * X and the product X.T * y.\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n",
      "     |  Lars : Least Angle Regression model a.k.a. LAR.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n",
      "     |  LarsCV : Cross-validated Least Angle Regression model.\n",
      "     |  LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.\n",
      "     |  sklearn.decomposition.sparse_encode : Generic sparse coding.\n",
      "     |      Each column of the result is the solution to a Lasso problem.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=100, n_informative=10,\n",
      "     |  ...                        noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuitCV(cv=5, normalize=False).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.n_nonzero_coefs_\n",
      "     |  10\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True, fit_intercept=True, normalize='deprecated', max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  PassiveAggressiveClassifier(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Classifier.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  loss : str, default=\"hinge\"\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  loss_function_ : callable\n",
      "     |      Loss function used by the algorithm.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression.\n",
      "     |  Perceptron : Linear perceptron classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PassiveAggressiveClassifier(random_state=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.26642044 0.45070924 0.67251877 0.64185414]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [1.84127814]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model._stochastic_gradient.BaseSGDRegressor)\n",
      "     |  PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Regressor.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  loss : str, default=\"epsilon_insensitive\"\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDRegressor : Linear model fitted by minimizing a regularized\n",
      "     |      empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  PassiveAggressiveRegressor(max_iter=100, random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [20.48736655 34.18818427 67.59122734 87.94731329]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-0.02306214]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-0.02306214]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Perceptron(*, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |  \n",
      "     |  Linear perceptron classifier.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : {'l2','l1','elasticnet'}, default=None\n",
      "     |      The penalty (aka regularization term) to be used.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n",
      "     |      `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n",
      "     |      Only used if `penalty='elasticnet'`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  eta0 : float, default=1\n",
      "     |      Constant by which the updates are multiplied.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution. See\n",
      "     |      :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  loss_function_ : concreteÂ LossFunction\n",
      "     |      The function that determines the loss, or difference between the\n",
      "     |      output of the algorithm and the target values.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.linear_model.SGDClassifier : Linear classifiers\n",
      "     |      (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  ``Perceptron`` is a classification algorithm which shares the same\n",
      "     |  underlying implementation with ``SGDClassifier``. In fact,\n",
      "     |  ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.linear_model import Perceptron\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Perceptron()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.939...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence, early stopping, and\n",
      "     |      learning rate adjustments should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PoissonRegressor(GeneralizedLinearRegressor)\n",
      "     |  PoissonRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Poisson distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  ----------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.PoissonRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [12, 17, 22, 21]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PoissonRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.990...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.121..., 0.158...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  2.088...\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([10.676..., 21.875...])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  ----------\n",
      "     |  GeneralizedLinearRegressor : Generalized Linear Model with a Poisson\n",
      "     |      distribution.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PoissonRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |      Return the string `'poisson'`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  QuantileRegressor(*, quantile=0.5, alpha=1.0, fit_intercept=True, solver='interior-point', solver_options=None)\n",
      "     |  \n",
      "     |  Linear regression model that predicts conditional quantiles.\n",
      "     |  \n",
      "     |  The linear :class:`QuantileRegressor` optimizes the pinball loss for a\n",
      "     |  desired `quantile` and is robust to outliers.\n",
      "     |  \n",
      "     |  This model uses an L1 regularization like\n",
      "     |  :class:`~sklearn.linear_model.Lasso`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <quantile_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  quantile : float, default=0.5\n",
      "     |      The quantile that the model tries to predict. It must be strictly\n",
      "     |      between 0 and 1. If 0.5 (default), the model predicts the 50%\n",
      "     |      quantile, i.e. the median.\n",
      "     |  \n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization constant that multiplies the L1 penalty term.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether or not to fit the intercept.\n",
      "     |  \n",
      "     |  solver : {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='interior-point'\n",
      "     |      Method used by :func:`scipy.optimize.linprog` to solve the linear\n",
      "     |      programming formulation. Note that the highs methods are recommended\n",
      "     |      for usage with `scipy>=1.6.0` because they are the fastest ones.\n",
      "     |  \n",
      "     |  solver_options : dict, default=None\n",
      "     |      Additional parameters passed to :func:`scipy.optimize.linprog` as\n",
      "     |      options. If `None` and if `solver='interior-point'`, then\n",
      "     |      `{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the\n",
      "     |      sake of stability.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the features.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      The intercept of the model, aka bias term.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations performed by the solver.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Lasso : The Lasso is a linear model that estimates sparse coefficients\n",
      "     |      with l1 regularization.\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import QuantileRegressor\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 2\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\n",
      "     |  >>> np.mean(y <= reg.predict(X))\n",
      "     |  0.8\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      QuantileRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, quantile=0.5, alpha=1.0, fit_intercept=True, solver='interior-point', solver_options=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "     |  RANSACRegressor(base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)\n",
      "     |  \n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |       * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |       * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |         which is used for the stop criterion defined by `stop_score`.\n",
      "     |         Additionally, the score is used to decide which of two equally\n",
      "     |         large consensus sets is chosen as the better one.\n",
      "     |       * `predict(X)`: Returns predicted values using the linear model,\n",
      "     |         which is used to compute residual error using loss function.\n",
      "     |  \n",
      "     |      If `base_estimator` is None, then\n",
      "     |      :class:`~sklearn.linear_model.LinearRegression` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), default=None\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0])` for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `base_estimator`. By default a\n",
      "     |      ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\n",
      "     |      dependent upon the model, so if a `base_estimator` other than\n",
      "     |      :class:`linear_model.LinearRegression` is used, the user is\n",
      "     |      encouraged to provide a value.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |         Not setting `min_samples` explicitly will raise an error in version\n",
      "     |         1.2 for models other than\n",
      "     |         :class:`~sklearn.linear_model.LinearRegression`. To keep the old\n",
      "     |         default behavior, set `min_samples=X.shape[1] + 1` explicitly.\n",
      "     |  \n",
      "     |  residual_threshold : float, default=None\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`. Points whose residuals are\n",
      "     |      strictly equal to the threshold are considered as inliers.\n",
      "     |  \n",
      "     |  is_data_valid : callable, default=None\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, default=None\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, default=100\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  max_skips : int, default=np.inf\n",
      "     |      Maximum number of iterations that can be skipped due to finding zero\n",
      "     |      inliers or invalid data defined by ``is_data_valid`` or invalid models\n",
      "     |      defined by ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  stop_n_inliers : int, default=np.inf\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, default=np.inf\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], default=0.99\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  loss : str, callable, default='absolute_error'\n",
      "     |      String inputs, 'absolute_error' and 'squared_error' are supported which\n",
      "     |      find the absolute error and squared error per sample respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the i-th value of the array corresponding to the loss\n",
      "     |      on ``X[i]``.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``,\n",
      "     |      then this sample is classified as an outlier.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          The loss 'squared_loss' was deprecated in v1.0 and will be removed\n",
      "     |          in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          The loss 'absolute_loss' was deprecated in v1.0 and will be removed\n",
      "     |          in version 1.2. Use `loss='absolute_error'` which is equivalent.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The generator used to initialize the centers.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Best fitted model (copy of the `base_estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  n_skips_no_inliers_ : int\n",
      "     |      Number of iterations skipped due to finding zero inliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_data_ : int\n",
      "     |      Number of iterations skipped due to invalid data defined by\n",
      "     |      ``is_data_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_model_ : int\n",
      "     |      Number of iterations skipped due to an invalid model defined by\n",
      "     |      ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\n",
      "     |  .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RANSACRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9885...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.9417...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and base_estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `RANSACRegressor` estimator.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Return the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : (array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      "     |  Ridge(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      "     |  \n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "     |      assumed to be specific to the targets. Hence they must correspond in\n",
      "     |      number.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to fit the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and\n",
      "     |          will be removed in 1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |      For 'lbfgs' solver, the default value is 15000.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its improved, unbiased version named SAGA. Both methods also use an\n",
      "     |        iterative procedure, and are often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "     |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "     |        is True.\n",
      "     |  \n",
      "     |      All last six solvers support both dense and sparse data. However, only\n",
      "     |      'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept`\n",
      "     |      is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |      Only 'lbfgs' solver is supported in this case.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         `random_state` to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      "     |      combines ridge regression with the kernel trick.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Ridge()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidgeCV)\n",
      "     |  RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      "     |  \n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs efficient Leave-One-Out Cross-Validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |      If using Leave-One-Out cross-validation, alphas must be positive.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      "     |          1.2.\n",
      "     |  \n",
      "     |  scoring : str, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |      If None, the negative mean squared error if cv is 'auto' or None\n",
      "     |      (i.e. when using leave-one-out cross-validation), and r2 score\n",
      "     |      otherwise.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n",
      "     |      :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Leave-One-Out Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n",
      "     |          'svd' : force use of singular value decomposition of X when X is\n",
      "     |              dense, eigenvalue decomposition of X^T.X when X is sparse.\n",
      "     |          'eigen' : force computation via eigendecomposition of X.X^T\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending on the shape of the training data.\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |  alpha_per_target : bool, default=False\n",
      "     |      Flag indicating whether to optimize the alpha value (picked from the\n",
      "     |      `alphas` parameter list) for each target separately (for multi-output\n",
      "     |      settings: multiple prediction targets). When set to `True`, after\n",
      "     |      fitting, the `alpha_` attribute will contain a value for each target.\n",
      "     |      When set to `False`, a single alpha is used for all targets.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (only available if\n",
      "     |      ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n",
      "     |      called, this attribute will contain the mean squared errors if\n",
      "     |      `scoring is None` otherwise it will contain standardized per point\n",
      "     |      prediction values.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float or ndarray of shape (n_targets,)\n",
      "     |      Estimated regularization parameter, or, if ``alpha_per_target=True``,\n",
      "     |      the estimated regularization parameter for each target.\n",
      "     |  \n",
      "     |  best_score_ : float or ndarray of shape (n_targets,)\n",
      "     |      Score of base estimator with best alpha, or, if\n",
      "     |      ``alpha_per_target=True``, a score for each target.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n",
      "     |  RidgeClassifierCV : Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.5166...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data. If using GCV, will be cast to float64\n",
      "     |          if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When sample_weight is provided, the selected hyperparameter may depend\n",
      "     |      on whether we use leave-one-out cross-validation (cv=None or cv='auto')\n",
      "     |      or another form of cross-validation, because only leave-one-out\n",
      "     |      cross-validation takes the sample weights into account when computing\n",
      "     |      the validation score.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifier(_RidgeClassifierMixin, _BaseRidge)\n",
      "     |  RidgeClassifier(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', positive=False, random_state=None)\n",
      "     |  \n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  This classifier first converts the target values into ``{-1, 1}`` and\n",
      "     |  then treats the problem as a regression task (multi-output regression in\n",
      "     |  the multiclass case).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and\n",
      "     |          will be removed in 1.2.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its unbiased and more flexible version named SAGA. Both methods\n",
      "     |        use an iterative procedure, and are often faster than other solvers\n",
      "     |        when both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |        .. versionadded:: 0.19\n",
      "     |           SAGA solver.\n",
      "     |  \n",
      "     |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "     |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "     |        is True.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |      Only 'lbfgs' solver is supported in this case.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifier\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifier().fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9595...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      _RidgeClassifierMixin\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', positive=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to RidgeClassifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of the estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, spare matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to predict the targets.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Vector or matrix containing the predictions. In binary and\n",
      "     |          multiclass problems, this is a vector containing `n_samples`. In\n",
      "     |          a multilabel problem, it returns a matrix of shape\n",
      "     |          `(n_samples, n_outputs)`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifierCV(_RidgeClassifierMixin, _BaseRidgeCV)\n",
      "     |  RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |  \n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs Leave-One-Out Cross-Validation. Currently,\n",
      "     |  only the n_features > n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          ``normalize`` was deprecated in version 1.0 and\n",
      "     |          will be removed in 1.2.\n",
      "     |  \n",
      "     |  scoring : str, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (only if ``store_cv_values=True`` and\n",
      "     |      ``cv=None``). After ``fit()`` has been called, this attribute will\n",
      "     |      contain the mean squared errors if `scoring is None` otherwise it\n",
      "     |      will contain standardized per point prediction values.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Score of base estimator with best alpha.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9630...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      _RidgeClassifierMixin\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features. When using GCV,\n",
      "     |          will be cast to float64 if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, spare matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to predict the targets.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Vector or matrix containing the predictions. In binary and\n",
      "     |          multiclass problems, this is a vector containing `n_samples`. In\n",
      "     |          a multilabel problem, it returns a matrix of shape\n",
      "     |          `(n_samples, n_outputs)`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier)\n",
      "     |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning via the `partial_fit` method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='hinge'\n",
      "     |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      "     |      linear SVM.\n",
      "     |  \n",
      "     |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      "     |      'squared_hinge', 'perceptron', or a regression loss: 'squared_error',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |  \n",
      "     |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "     |      'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |      outliers as well as probability estimates.\n",
      "     |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      The other losses are designed for regression but can be useful in\n",
      "     |      classification as well; see\n",
      "     |      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          The loss 'squared_loss' was deprecated in v1.0 and will be removed\n",
      "     |          in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='optimal'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : float, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : float, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit`` will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights across all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  loss_function_ : concrete ``LossFunction``\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.LinearSVC : Linear support vector classification.\n",
      "     |  LogisticRegression : Logistic regression.\n",
      "     |  Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "     |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "     |      penalty=None)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDClassifier\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> clf = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdclassifier', SGDClassifier())])\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      https://dl.acm.org/doi/pdf/10.1145/775047.775151\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence, early stopping, and\n",
      "     |      learning rate adjustments should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SGDOneClassSVM(BaseSGD, sklearn.base.OutlierMixin)\n",
      "     |  SGDOneClassSVM(nu=0.5, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Solves linear One-Class SVM using Stochastic Gradient Descent.\n",
      "     |  \n",
      "     |  This implementation is meant to be used with a kernel approximation\n",
      "     |  technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results\n",
      "     |  similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by\n",
      "     |  default.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd_online_one_class_svm>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, default=0.5\n",
      "     |      The nu parameter of the One Class SVM: an upper bound on the\n",
      "     |      fraction of training errors and a lower bound of the fraction of\n",
      "     |      support vectors. Should be in the interval (0, 1]. By default 0.5\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`. Defaults to 1000.\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to 1e-3.\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  learning_rate : {'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'\n",
      "     |      The learning rate schedule to use with `fit`. (If using `partial_fit`,\n",
      "     |      learning rate must be controlled directly).\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |  eta0 : float, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : float, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  offset_ : ndarray of shape (1,)\n",
      "     |      Offset used to define the decision function from the raw scores.\n",
      "     |      We have the relation: decision_function = score_samples - offset.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  loss_function_ : concrete ``LossFunction``\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This estimator has a linear complexity in the number of training samples\n",
      "     |  and is thus better suited than the `sklearn.svm.OneClassSVM`\n",
      "     |  implementation for datasets with a large number of training samples (say\n",
      "     |  > 10,000).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> clf = linear_model.SGDOneClassSVM(random_state=42)\n",
      "     |  >>> clf.fit(X)\n",
      "     |  SGDOneClassSVM(random_state=42)\n",
      "     |  \n",
      "     |  >>> print(clf.predict([[4, 4]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDOneClassSVM\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Signed distance to the separating hyperplane.\n",
      "     |      \n",
      "     |      Signed distance is positive for an inlier and negative for an\n",
      "     |      outlier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dec : array-like, shape (n_samples,)\n",
      "     |          Decision function values of the samples.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, coef_init=None, offset_init=None, sample_weight=None)\n",
      "     |      Fit linear One-Class SVM with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      This solves an equivalent optimization problem of the\n",
      "     |      One-Class SVM primal optimization problem and returns a weight vector\n",
      "     |      w and an offset rho such that the decision function is given by\n",
      "     |      <w, x> - rho.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      offset_init : array, shape (n_classes,)\n",
      "     |          The initial offset to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit linear One-Class SVM with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Return labels (1 inlier, -1 outlier) of the samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape (n_samples,)\n",
      "     |          Labels of the samples.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Raw scoring function of the samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score_samples : array-like, shape (n_samples,)\n",
      "     |          Unshiffted scoring function values of the samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  loss_functions = {'hinge': (<class 'sklearn.linear_model._sgd_fast.Hin...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor)\n",
      "     |  SGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='squared_error'\n",
      "     |      The loss function to be used. The possible values are 'squared_error',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |  \n",
      "     |      The 'squared_error' refers to the ordinary least squares fit.\n",
      "     |      'huber' modifies 'squared_error' to focus less on getting outliers\n",
      "     |      correct by switching from squared to linear loss past a distance of\n",
      "     |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      "     |      linear past that; this is the loss function used in SVR.\n",
      "     |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      "     |      a tolerance of epsilon.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          The loss 'squared_loss' was deprecated in v1.0 and will be removed\n",
      "     |          in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='invscaling'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : float, default=0.01\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.01.\n",
      "     |  \n",
      "     |  power_t : float, default=0.25\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights across all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  Lars : Least Angle Regression model.\n",
      "     |  Lasso : Linear Model trained with L1 prior as regularizer.\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  Ridge : Linear least squares with l2 regularization.\n",
      "     |  sklearn.svm.SVR : Epsilon-Support Vector Regression.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDRegressor\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> reg = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdregressor', SGDRegressor())])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features,), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (1,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `SGDRegressor` estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SquaredLoss(Regression)\n",
      "     |  Squared loss traditional used in linear regression.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredLoss\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  py_dloss(...)\n",
      "     |      Python version of `dloss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "     |  \n",
      "     |  py_loss(...)\n",
      "     |      Python version of `loss` for testing.\n",
      "     |      \n",
      "     |      Pytest needs a python function and can't use cdef functions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, `p = w^T x + intercept`.\n",
      "     |      y : double\n",
      "     |          The true value (aka target).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The loss evaluated at `p` and `y`.\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_subpopulation : int, default=1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed.\n",
      "     |  \n",
      "     |  n_subsamples : int, default=None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      A random number generator instance to define the state of the random\n",
      "     |      permutations generator. Pass an int for reproducible output across\n",
      "     |      multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import TheilSenRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9884...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.5871...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |          Fitted `TheilSenRegressor` estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination of the prediction.\n",
      "     |      \n",
      "     |      The coefficient of determination :math:`R^2` is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class TweedieRegressor(GeneralizedLinearRegressor)\n",
      "     |  TweedieRegressor(*, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  This estimator can be used to model different GLMs depending on the\n",
      "     |  ``power`` parameter, which determines the underlying distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  power : float, default=0\n",
      "     |          The power determines the underlying target distribution according\n",
      "     |          to the following table:\n",
      "     |  \n",
      "     |          +-------+------------------------+\n",
      "     |          | Power | Distribution           |\n",
      "     |          +=======+========================+\n",
      "     |          | 0     | Normal                 |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 1     | Poisson                |\n",
      "     |          +-------+------------------------+\n",
      "     |          | (1,2) | Compound Poisson Gamma |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 2     | Gamma                  |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 3     | Inverse Gaussian       |\n",
      "     |          +-------+------------------------+\n",
      "     |  \n",
      "     |          For ``0 < power < 1``, no distribution exists.\n",
      "     |  \n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  link : {'auto', 'identity', 'log'}, default='auto'\n",
      "     |      The link function of the GLM, i.e. mapping from linear predictor\n",
      "     |      `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n",
      "     |      the link depending on the chosen family as follows:\n",
      "     |  \n",
      "     |      - 'identity' for Normal distribution\n",
      "     |      - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n",
      "     |  GammaRegressor : Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  ----------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.TweedieRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [2, 3.5, 5, 5.5]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  TweedieRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.839...\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.599..., 0.299...])\n",
      "     |  >>> clf.intercept_\n",
      "     |  1.600...\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([2.500..., 4.599...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TweedieRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |      Return the family of the regressor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent.\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "            Target values.\n",
      "        \n",
      "        l1_ratio : float, default=0.5\n",
      "            Number between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If set to False, the input validation checks are skipped (including the\n",
      "            Gram matrix when provided). It is assumed that they are handled\n",
      "            by the caller.\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "        MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "        ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "        ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "    \n",
      "    lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : None or array-like of shape (n_samples, n_features)\n",
      "            Input data. Note that if X is None then the Gram matrix must be\n",
      "            specified, i.e., cannot be None or False.\n",
      "        \n",
      "        y : None or array-like of shape (n_samples,)\n",
      "            Input targets.\n",
      "        \n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets),             default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        Gram : None, 'auto', array-like of shape (n_features, n_features),             default=None\n",
      "            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the ``tol`` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path_gram\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        lars_path in the sufficient stats mode [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Xy = np.dot(X.T, y).\n",
      "        \n",
      "        Gram : array-like of shape (n_features, n_features)\n",
      "            Gram = np.dot(X.T * X).\n",
      "        \n",
      "        n_samples : int or float\n",
      "            Equivalent size of sample.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the ``tol`` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent.\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "            Target values.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "            algorithm.\n",
      "        Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "        LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "        LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "            path.\n",
      "        LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "        sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "            transform signals into sparse linear combination of atoms from a fixed.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.linear_model import lasso_path\n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[0.         0.         0.46874778]\n",
      "         [0.2159048  0.4425765  0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[0.         0.         0.46915237]\n",
      "         [0.2159048  0.4425765  0.23668876]]\n",
      "    \n",
      "    orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : 'auto' or bool, default=False\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp_gram\n",
      "        lars_path\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : ndarray of shape (n_features, n_features)\n",
      "            Gram matrix of the input data: X.T * X.\n",
      "        \n",
      "        Xy : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by X: X.T * y.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        norms_squared : array-like of shape (n_targets,), default=None\n",
      "            Squared L2 norms of the lines of y. Required if tol is not None.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            Whether the gram matrix must be copied by the algorithm. A false\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, default=True\n",
      "            Whether the covariance vector Xy must be copied by the algorithm.\n",
      "            If False, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp\n",
      "        lars_path\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, positive=False, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n",
      "            Training data\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Target values\n",
      "        \n",
      "        alpha : float or array-like of shape (n_targets,)\n",
      "            Regularization strength; must be a positive float. Regularization\n",
      "            improves the conditioning of the problem and reduces the variance of\n",
      "            the estimates. Larger values specify stronger regularization.\n",
      "            Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "            :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "            :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "            assumed to be specific to the targets. Hence they must correspond in\n",
      "            number.\n",
      "        \n",
      "        sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "            Individual weights for each sample. If given a float, every sample\n",
      "            will have the same weight. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. More stable for singular matrices than 'cholesky'.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "              procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "              its improved, unbiased version named SAGA. Both methods also use an\n",
      "              iterative procedure, and are often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' and\n",
      "              'saga' fast convergence is only guaranteed on features with\n",
      "              approximately the same scale. You can preprocess the data with a\n",
      "              scaler from sklearn.preprocessing.\n",
      "        \n",
      "            - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "              `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "              is True.\n",
      "        \n",
      "            All last six solvers support both dense and sparse data. However, only\n",
      "            'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept`\n",
      "            is True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "            .. versionadded:: 0.19\n",
      "               SAGA solver.\n",
      "        \n",
      "        max_iter : int, default=None\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n",
      "            1000. For 'lbfgs' solver, the default value is 15000.\n",
      "        \n",
      "        tol : float, default=1e-3\n",
      "            Precision of the solution.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            When set to ``True``, forces the coefficients to be positive.\n",
      "            Only 'lbfgs' solver is supported in this case.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "            See :term:`Glossary <random_state>` for details.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : bool, default=False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or ndarray of shape (n_targets,)\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages\\sklearn\\linear_model\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "help(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.preprocessing in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.preprocessing\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.preprocessing` module includes scaling, centering,\n",
      "    normalization, binarization methods.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _csr_polynomial_expansion\n",
      "    _data\n",
      "    _discretization\n",
      "    _encoders\n",
      "    _function_transformer\n",
      "    _label\n",
      "    _polynomial\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.preprocessing._data.Binarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.KernelCenterer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.MaxAbsScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.MinMaxScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.Normalizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.PowerTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.QuantileTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.RobustScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._discretization.KBinsDiscretizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._function_transformer.FunctionTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.LabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.MultiLabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._polynomial.PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._polynomial.SplineTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.preprocessing._data.Binarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.KernelCenterer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.MaxAbsScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.MinMaxScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.Normalizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.PowerTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.QuantileTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.RobustScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._discretization.KBinsDiscretizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._function_transformer.FunctionTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.LabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._label.MultiLabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._polynomial.PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._polynomial.SplineTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base._OneToOneFeatureMixin(builtins.object)\n",
      "        sklearn.preprocessing._data.MaxAbsScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.MinMaxScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.PowerTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.QuantileTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.RobustScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._data.StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.preprocessing._encoders._BaseEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.preprocessing._encoders.OneHotEncoder\n",
      "        sklearn.preprocessing._encoders.OrdinalEncoder\n",
      "    \n",
      "    class Binarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Binarizer(*, threshold=0.0, copy=True)\n",
      "     |  \n",
      "     |  Binarize data (set feature values to 0 or 1) according to a threshold.\n",
      "     |  \n",
      "     |  Values greater than the threshold map to 1, while values less than\n",
      "     |  or equal to the threshold map to 0. With the default threshold of 0,\n",
      "     |  only positive values map to 1.\n",
      "     |  \n",
      "     |  Binarization is a common operation on text count data where the\n",
      "     |  analyst can decide to only consider the presence or absence of a\n",
      "     |  feature rather than a quantified number of occurrences for instance.\n",
      "     |  \n",
      "     |  It can also be used as a pre-processing step for estimators that\n",
      "     |  consider boolean random variables (e.g. modelled using the Bernoulli\n",
      "     |  distribution in a Bayesian setting).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  threshold : float, default=0.0\n",
      "     |      Feature values below or equal to this are replaced by 0, above it by 1.\n",
      "     |      Threshold may not be less than 0 for operations on sparse matrices.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace binarization and avoid a copy (if\n",
      "     |      the input is already a numpy array or a scipy.sparse CSR matrix).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  binarize : Equivalent function without the estimator API.\n",
      "     |  KBinsDiscretizer : Bin continuous data into intervals.\n",
      "     |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  If the input is a sparse matrix, only the non-zero values are subject\n",
      "     |  to update by the Binarizer class.\n",
      "     |  \n",
      "     |  This estimator is stateless (besides constructor parameters), the\n",
      "     |  fit method does nothing but is useful when used in a pipeline.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import Binarizer\n",
      "     |  >>> X = [[ 1., -1.,  2.],\n",
      "     |  ...      [ 2.,  0.,  0.],\n",
      "     |  ...      [ 0.,  1., -1.]]\n",
      "     |  >>> transformer = Binarizer().fit(X)  # fit does nothing.\n",
      "     |  >>> transformer\n",
      "     |  Binarizer()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[1., 0., 1.],\n",
      "     |         [1., 0., 0.],\n",
      "     |         [0., 1., 0.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Binarizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, threshold=0.0, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Do nothing and return the estimator unchanged.\n",
      "     |      \n",
      "     |      This method is just there to implement the usual API and hence\n",
      "     |      work in pipelines.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |  \n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Binarize each element of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to binarize, element by element.\n",
      "     |          scipy.sparse matrices should be in CSR format to avoid an\n",
      "     |          un-necessary copy.\n",
      "     |      \n",
      "     |      copy : bool\n",
      "     |          Copy the input X or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class FunctionTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  FunctionTransformer(func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None)\n",
      "     |  \n",
      "     |  Constructs a transformer from an arbitrary callable.\n",
      "     |  \n",
      "     |  A FunctionTransformer forwards its X (and optionally y) arguments to a\n",
      "     |  user-defined function or function object and returns the result of this\n",
      "     |  function. This is useful for stateless transformations such as taking the\n",
      "     |  log of frequencies, doing custom scaling, etc.\n",
      "     |  \n",
      "     |  Note: If a lambda is used as the function, then the resulting\n",
      "     |  transformer will not be pickleable.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <function_transformer>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  func : callable, default=None\n",
      "     |      The callable to use for the transformation. This will be passed\n",
      "     |      the same arguments as transform, with args and kwargs forwarded.\n",
      "     |      If func is None, then func will be the identity function.\n",
      "     |  \n",
      "     |  inverse_func : callable, default=None\n",
      "     |      The callable to use for the inverse transformation. This will be\n",
      "     |      passed the same arguments as inverse transform, with args and\n",
      "     |      kwargs forwarded. If inverse_func is None, then inverse_func\n",
      "     |      will be the identity function.\n",
      "     |  \n",
      "     |  validate : bool, default=False\n",
      "     |      Indicate that the input X array should be checked before calling\n",
      "     |      ``func``. The possibilities are:\n",
      "     |  \n",
      "     |      - If False, there is no input validation.\n",
      "     |      - If True, then X will be converted to a 2-dimensional NumPy array or\n",
      "     |        sparse matrix. If the conversion is not possible an exception is\n",
      "     |        raised.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default of ``validate`` changed from True to False.\n",
      "     |  \n",
      "     |  accept_sparse : bool, default=False\n",
      "     |      Indicate that func accepts a sparse matrix as input. If validate is\n",
      "     |      False, this has no effect. Otherwise, if accept_sparse is false,\n",
      "     |      sparse matrix inputs will cause an exception to be raised.\n",
      "     |  \n",
      "     |  check_inverse : bool, default=True\n",
      "     |     Whether to check that or ``func`` followed by ``inverse_func`` leads to\n",
      "     |     the original inputs. It can be used for a sanity check, raising a\n",
      "     |     warning when the condition is not fulfilled.\n",
      "     |  \n",
      "     |     .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  kw_args : dict, default=None\n",
      "     |      Dictionary of additional keyword arguments to pass to func.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  inv_kw_args : dict, default=None\n",
      "     |      Dictionary of additional keyword arguments to pass to inverse_func.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Defined only when\n",
      "     |      `validate=True`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `validate=True`\n",
      "     |      and `X` has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MaxAbsScaler : Scale each feature by its maximum absolute value.\n",
      "     |  StandardScaler : Standardize features by removing the mean and\n",
      "     |      scaling to unit variance.\n",
      "     |  LabelBinarizer : Binarize labels in a one-vs-all fashion.\n",
      "     |  MultiLabelBinarizer : Transform between iterable of iterables\n",
      "     |      and a multilabel format.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import FunctionTransformer\n",
      "     |  >>> transformer = FunctionTransformer(np.log1p)\n",
      "     |  >>> X = np.array([[0, 1], [2, 3]])\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[0.       , 0.6931...],\n",
      "     |         [1.0986..., 1.3862...]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FunctionTransformer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self)\n",
      "     |      Return True since FunctionTransfomer is stateless.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit transformer by checking X.\n",
      "     |      \n",
      "     |      If ``validate`` is ``True``, ``X`` will be checked.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input array.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          FunctionTransformer class instance.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Transform X using the inverse function.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : array-like, shape (n_samples, n_features)\n",
      "     |          Transformed input.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X using the forward function.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Input array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : array-like, shape (n_samples, n_features)\n",
      "     |          Transformed input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KBinsDiscretizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
      "     |  \n",
      "     |  Bin continuous data into intervals.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_discretization>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_bins : int or array-like of shape (n_features,), default=5\n",
      "     |      The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n",
      "     |  \n",
      "     |  encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'\n",
      "     |      Method used to encode the transformed result.\n",
      "     |  \n",
      "     |      - 'onehot': Encode the transformed result with one-hot encoding\n",
      "     |        and return a sparse matrix. Ignored features are always\n",
      "     |        stacked to the right.\n",
      "     |      - 'onehot-dense': Encode the transformed result with one-hot encoding\n",
      "     |        and return a dense array. Ignored features are always\n",
      "     |        stacked to the right.\n",
      "     |      - 'ordinal': Return the bin identifier encoded as an integer value.\n",
      "     |  \n",
      "     |  strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'\n",
      "     |      Strategy used to define the widths of the bins.\n",
      "     |  \n",
      "     |      - 'uniform': All bins in each feature have identical widths.\n",
      "     |      - 'quantile': All bins in each feature have the same number of points.\n",
      "     |      - 'kmeans': Values in each bin have the same nearest center of a 1D\n",
      "     |        k-means cluster.\n",
      "     |  \n",
      "     |  dtype : {np.float32, np.float64}, default=None\n",
      "     |      The desired data-type for the output. If None, output dtype is\n",
      "     |      consistent with input dtype. Only np.float32 and np.float64 are\n",
      "     |      supported.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  bin_edges_ : ndarray of ndarray of shape (n_features,)\n",
      "     |      The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n",
      "     |      Ignored features will have empty arrays.\n",
      "     |  \n",
      "     |  n_bins_ : ndarray of shape (n_features,), dtype=np.int_\n",
      "     |      Number of bins per feature. Bins whose width are too small\n",
      "     |      (i.e., <= 1e-8) are removed with a warning.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Binarizer : Class used to bin values as ``0`` or\n",
      "     |      ``1`` based on a parameter ``threshold``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  In bin edges for feature ``i``, the first and last values are used only for\n",
      "     |  ``inverse_transform``. During transform, bin edges are extended to::\n",
      "     |  \n",
      "     |    np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n",
      "     |  \n",
      "     |  You can combine ``KBinsDiscretizer`` with\n",
      "     |  :class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess\n",
      "     |  part of the features.\n",
      "     |  \n",
      "     |  ``KBinsDiscretizer`` might produce constant features (e.g., when\n",
      "     |  ``encode = 'onehot'`` and certain bins do not contain any data).\n",
      "     |  These features can be removed with feature selection algorithms\n",
      "     |  (e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import KBinsDiscretizer\n",
      "     |  >>> X = [[-2, 1, -4,   -1],\n",
      "     |  ...      [-1, 2, -3, -0.5],\n",
      "     |  ...      [ 0, 3, -2,  0.5],\n",
      "     |  ...      [ 1, 4, -1,    2]]\n",
      "     |  >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
      "     |  >>> est.fit(X)\n",
      "     |  KBinsDiscretizer(...)\n",
      "     |  >>> Xt = est.transform(X)\n",
      "     |  >>> Xt  # doctest: +SKIP\n",
      "     |  array([[ 0., 0., 0., 0.],\n",
      "     |         [ 1., 1., 1., 0.],\n",
      "     |         [ 2., 2., 2., 1.],\n",
      "     |         [ 2., 2., 2., 2.]])\n",
      "     |  \n",
      "     |  Sometimes it may be useful to convert the data back into the original\n",
      "     |  feature space. The ``inverse_transform`` function converts the binned\n",
      "     |  data into the original feature space. Each value will be equal to the mean\n",
      "     |  of the two bin edges.\n",
      "     |  \n",
      "     |  >>> est.bin_edges_[0]\n",
      "     |  array([-2., -1.,  0.,  1.])\n",
      "     |  >>> est.inverse_transform(Xt)\n",
      "     |  array([[-1.5,  1.5, -3.5, -0.5],\n",
      "     |         [-0.5,  2.5, -2.5, -0.5],\n",
      "     |         [ 0.5,  3.5, -1.5,  0.5],\n",
      "     |         [ 0.5,  3.5, -1.5,  1.5]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KBinsDiscretizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Data to be discretized.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  inverse_transform(self, Xt)\n",
      "     |      Transform discretized data back to original feature space.\n",
      "     |      \n",
      "     |      Note that this function does not regenerate the original data\n",
      "     |      due to discretization rounding.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xt : array-like of shape (n_samples, n_features)\n",
      "     |          Transformed data in the binned space.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xinv : ndarray, dtype={np.float32, np.float64}\n",
      "     |          Data in the original feature space.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Discretize the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Data to be discretized.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix}, dtype={np.float32, np.float64}\n",
      "     |          Data in the binned space. Will be a sparse matrix if\n",
      "     |          `self.encode='onehot'` and ndarray otherwise.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KernelCenterer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Center an arbitrary kernel matrix :math:`K`.\n",
      "     |  \n",
      "     |  Let define a kernel :math:`K` such that:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      K(X, Y) = \\phi(X) . \\phi(Y)^{T}\n",
      "     |  \n",
      "     |  :math:`\\phi(X)` is a function mapping of rows of :math:`X` to a\n",
      "     |  Hilbert space and :math:`K` is of shape `(n_samples, n_samples)`.\n",
      "     |  \n",
      "     |  This class allows to compute :math:`\\tilde{K}(X, Y)` such that:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\tilde{K(X, Y)} = \\tilde{\\phi}(X) . \\tilde{\\phi}(Y)^{T}\n",
      "     |  \n",
      "     |  :math:`\\tilde{\\phi}(X)` is the centered mapped data in the Hilbert\n",
      "     |  space.\n",
      "     |  \n",
      "     |  `KernelCenterer` centers the features without explicitly computing the\n",
      "     |  mapping :math:`\\phi(\\cdot)`. Working with centered kernels is sometime\n",
      "     |  expected when dealing with algebra computation such as eigendecomposition\n",
      "     |  for :class:`~sklearn.decomposition.KernelPCA` for instance.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <kernel_centering>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  K_fit_rows_ : ndarray of shape (n_samples,)\n",
      "     |      Average of each column of kernel matrix.\n",
      "     |  \n",
      "     |  K_fit_all_ : float\n",
      "     |      Average of kernel matrix.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.kernel_approximation.Nystroem : Approximate a kernel map\n",
      "     |      using a subset of the training data.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] `SchÃ¶lkopf, Bernhard, Alexander Smola, and Klaus-Robert MÃ¼ller.\n",
      "     |     \"Nonlinear component analysis as a kernel eigenvalue problem.\"\n",
      "     |     Neural computation 10.5 (1998): 1299-1319.\n",
      "     |     <https://www.mlpack.org/papers/kpca.pdf>`_\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import KernelCenterer\n",
      "     |  >>> from sklearn.metrics.pairwise import pairwise_kernels\n",
      "     |  >>> X = [[ 1., -2.,  2.],\n",
      "     |  ...      [ -2.,  1.,  3.],\n",
      "     |  ...      [ 4.,  1., -2.]]\n",
      "     |  >>> K = pairwise_kernels(X, metric='linear')\n",
      "     |  >>> K\n",
      "     |  array([[  9.,   2.,  -2.],\n",
      "     |         [  2.,  14., -13.],\n",
      "     |         [ -2., -13.,  21.]])\n",
      "     |  >>> transformer = KernelCenterer().fit(K)\n",
      "     |  >>> transformer\n",
      "     |  KernelCenterer()\n",
      "     |  >>> transformer.transform(K)\n",
      "     |  array([[  5.,   0.,  -5.],\n",
      "     |         [  0.,  14., -14.],\n",
      "     |         [ -5., -14.,  19.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KernelCenterer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, K, y=None)\n",
      "     |      Fit KernelCenterer.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      K : ndarray of shape (n_samples, n_samples)\n",
      "     |          Kernel matrix.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  transform(self, K, copy=True)\n",
      "     |      Center kernel matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      K : ndarray of shape (n_samples1, n_samples2)\n",
      "     |          Kernel matrix.\n",
      "     |      \n",
      "     |      copy : bool, default=True\n",
      "     |          Set to False to perform inplace computation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      K_new : ndarray of shape (n_samples1, n_samples2)\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  LabelBinarizer(*, neg_label=0, pos_label=1, sparse_output=False)\n",
      "     |  \n",
      "     |  Binarize labels in a one-vs-all fashion.\n",
      "     |  \n",
      "     |  Several regression and binary classification algorithms are\n",
      "     |  available in scikit-learn. A simple way to extend these algorithms\n",
      "     |  to the multi-class classification case is to use the so-called\n",
      "     |  one-vs-all scheme.\n",
      "     |  \n",
      "     |  At learning time, this simply consists in learning one regressor\n",
      "     |  or binary classifier per class. In doing so, one needs to convert\n",
      "     |  multi-class labels to binary labels (belong or does not belong\n",
      "     |  to the class). LabelBinarizer makes this process easy with the\n",
      "     |  transform method.\n",
      "     |  \n",
      "     |  At prediction time, one assigns the class for which the corresponding\n",
      "     |  model gave the greatest confidence. LabelBinarizer makes this easy\n",
      "     |  with the inverse_transform method.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  neg_label : int, default=0\n",
      "     |      Value with which negative labels must be encoded.\n",
      "     |  \n",
      "     |  pos_label : int, default=1\n",
      "     |      Value with which positive labels must be encoded.\n",
      "     |  \n",
      "     |  sparse_output : bool, default=False\n",
      "     |      True if the returned array from transform is desired to be in sparse\n",
      "     |      CSR format.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Holds the label for each class.\n",
      "     |  \n",
      "     |  y_type_ : str\n",
      "     |      Represents the type of the target data as evaluated by\n",
      "     |      utils.multiclass.type_of_target. Possible type are 'continuous',\n",
      "     |      'continuous-multioutput', 'binary', 'multiclass',\n",
      "     |      'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n",
      "     |  \n",
      "     |  sparse_input_ : bool\n",
      "     |      True if the input data to transform is given as a sparse matrix, False\n",
      "     |      otherwise.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  label_binarize : Function to perform the transform operation of\n",
      "     |      LabelBinarizer with fixed classes.\n",
      "     |  OneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n",
      "     |      scheme.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import preprocessing\n",
      "     |  >>> lb = preprocessing.LabelBinarizer()\n",
      "     |  >>> lb.fit([1, 2, 6, 4, 2])\n",
      "     |  LabelBinarizer()\n",
      "     |  >>> lb.classes_\n",
      "     |  array([1, 2, 4, 6])\n",
      "     |  >>> lb.transform([1, 6])\n",
      "     |  array([[1, 0, 0, 0],\n",
      "     |         [0, 0, 0, 1]])\n",
      "     |  \n",
      "     |  Binary targets transform to a column vector\n",
      "     |  \n",
      "     |  >>> lb = preprocessing.LabelBinarizer()\n",
      "     |  >>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\n",
      "     |  array([[1],\n",
      "     |         [0],\n",
      "     |         [0],\n",
      "     |         [1]])\n",
      "     |  \n",
      "     |  Passing a 2D matrix for multilabel classification\n",
      "     |  \n",
      "     |  >>> import numpy as np\n",
      "     |  >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\n",
      "     |  LabelBinarizer()\n",
      "     |  >>> lb.classes_\n",
      "     |  array([0, 1, 2])\n",
      "     |  >>> lb.transform([0, 1, 2, 1])\n",
      "     |  array([[1, 0, 0],\n",
      "     |         [0, 1, 0],\n",
      "     |         [0, 0, 1],\n",
      "     |         [0, 1, 0]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LabelBinarizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, neg_label=0, pos_label=1, sparse_output=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, y)\n",
      "     |      Fit label binarizer.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      "     |          represents multilabel classification.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  fit_transform(self, y)\n",
      "     |      Fit label binarizer/transform multi-class labels to binary labels.\n",
      "     |      \n",
      "     |      The output of transform is sometimes referred to as\n",
      "     |      the 1-of-K coding scheme.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)\n",
      "     |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      "     |          represents multilabel classification. Sparse matrix can be\n",
      "     |          CSR, CSC, COO, DOK, or LIL.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "     |          Shape will be (n_samples, 1) for binary problems. Sparse matrix\n",
      "     |          will be of CSR format.\n",
      "     |  \n",
      "     |  inverse_transform(self, Y, threshold=None)\n",
      "     |      Transform binary labels back to multi-class labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "     |          Target values. All sparse matrices are converted to CSR before\n",
      "     |          inverse transformation.\n",
      "     |      \n",
      "     |      threshold : float, default=None\n",
      "     |          Threshold used in the binary and multi-label cases.\n",
      "     |      \n",
      "     |          Use 0 when ``Y`` contains the output of decision_function\n",
      "     |          (classifier).\n",
      "     |          Use 0.5 when ``Y`` contains the output of predict_proba.\n",
      "     |      \n",
      "     |          If None, the threshold is assumed to be half way between\n",
      "     |          neg_label and pos_label.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,)\n",
      "     |          Target values. Sparse matrix will be of CSR format.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      In the case when the binary labels are fractional\n",
      "     |      (probabilistic), inverse_transform chooses the class with the\n",
      "     |      greatest value. Typically, this allows to use the output of a\n",
      "     |      linear model's decision_function method directly as the input\n",
      "     |      of inverse_transform.\n",
      "     |  \n",
      "     |  transform(self, y)\n",
      "     |      Transform multi-class labels to binary labels.\n",
      "     |      \n",
      "     |      The output of transform is sometimes referred to by some authors as\n",
      "     |      the 1-of-K coding scheme.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : {array, sparse matrix} of shape (n_samples,) or                 (n_samples, n_classes)\n",
      "     |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      "     |          represents multilabel classification. Sparse matrix can be\n",
      "     |          CSR, CSC, COO, DOK, or LIL.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "     |          Shape will be (n_samples, 1) for binary problems. Sparse matrix\n",
      "     |          will be of CSR format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Encode target labels with value between 0 and n_classes-1.\n",
      "     |  \n",
      "     |  This transformer should be used to encode target values, *i.e.* `y`, and\n",
      "     |  not the input `X`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.12\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Holds the label for each class.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OrdinalEncoder : Encode categorical features using an ordinal encoding\n",
      "     |      scheme.\n",
      "     |  OneHotEncoder : Encode categorical features as a one-hot numeric array.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  `LabelEncoder` can be used to normalize labels.\n",
      "     |  \n",
      "     |  >>> from sklearn import preprocessing\n",
      "     |  >>> le = preprocessing.LabelEncoder()\n",
      "     |  >>> le.fit([1, 2, 2, 6])\n",
      "     |  LabelEncoder()\n",
      "     |  >>> le.classes_\n",
      "     |  array([1, 2, 6])\n",
      "     |  >>> le.transform([1, 1, 2, 6])\n",
      "     |  array([0, 0, 1, 2]...)\n",
      "     |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      "     |  array([1, 1, 2, 6])\n",
      "     |  \n",
      "     |  It can also be used to transform non-numerical labels (as long as they are\n",
      "     |  hashable and comparable) to numerical labels.\n",
      "     |  \n",
      "     |  >>> le = preprocessing.LabelEncoder()\n",
      "     |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      "     |  LabelEncoder()\n",
      "     |  >>> list(le.classes_)\n",
      "     |  ['amsterdam', 'paris', 'tokyo']\n",
      "     |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
      "     |  array([2, 2, 1]...)\n",
      "     |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      "     |  ['tokyo', 'tokyo', 'paris']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LabelEncoder\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fit(self, y)\n",
      "     |      Fit label encoder.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |          Fitted label encoder.\n",
      "     |  \n",
      "     |  fit_transform(self, y)\n",
      "     |      Fit label encoder and return encoded labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Encoded labels.\n",
      "     |  \n",
      "     |  inverse_transform(self, y)\n",
      "     |      Transform labels back to original encoding.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Original encoding.\n",
      "     |  \n",
      "     |  transform(self, y)\n",
      "     |      Transform labels to normalized encoding.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Labels as normalized encodings.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MaxAbsScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  MaxAbsScaler(*, copy=True)\n",
      "     |  \n",
      "     |  Scale each feature by its maximum absolute value.\n",
      "     |  \n",
      "     |  This estimator scales and translates each feature individually such\n",
      "     |  that the maximal absolute value of each feature in the\n",
      "     |  training set will be 1.0. It does not shift/center the data, and\n",
      "     |  thus does not destroy any sparsity.\n",
      "     |  \n",
      "     |  This scaler can also be applied to sparse CSR or CSC matrices.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "     |      is already a numpy array).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scale_ : ndarray of shape (n_features,)\n",
      "     |      Per feature relative scaling of the data.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |  \n",
      "     |  max_abs_ : ndarray of shape (n_features,)\n",
      "     |      Per feature maximum absolute value.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_seen_ : int\n",
      "     |      The number of samples processed by the estimator. Will be reset on\n",
      "     |      new calls to fit, but increments across ``partial_fit`` calls.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  maxabs_scale : Equivalent function without the estimator API.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import MaxAbsScaler\n",
      "     |  >>> X = [[ 1., -1.,  2.],\n",
      "     |  ...      [ 2.,  0.,  0.],\n",
      "     |  ...      [ 0.,  1., -1.]]\n",
      "     |  >>> transformer = MaxAbsScaler().fit(X)\n",
      "     |  >>> transformer\n",
      "     |  MaxAbsScaler()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[ 0.5, -1. ,  1. ],\n",
      "     |         [ 1. ,  0. ,  0. ],\n",
      "     |         [ 0. ,  1. , -0.5]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxAbsScaler\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the maximum absolute value to be used for later scaling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the per-feature minimum and maximum\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data that should be transformed back.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Online computation of max absolute value of X for later scaling.\n",
      "     |      \n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Scale the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data that should be scaled.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MinMaxScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      "     |  \n",
      "     |  Transform features by scaling each feature to a given range.\n",
      "     |  \n",
      "     |  This estimator scales and translates each feature individually such\n",
      "     |  that it is in the given range on the training set, e.g. between\n",
      "     |  zero and one.\n",
      "     |  \n",
      "     |  The transformation is given by::\n",
      "     |  \n",
      "     |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "     |      X_scaled = X_std * (max - min) + min\n",
      "     |  \n",
      "     |  where min, max = feature_range.\n",
      "     |  \n",
      "     |  This transformation is often used as an alternative to zero mean,\n",
      "     |  unit variance scaling.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  feature_range : tuple (min, max), default=(0, 1)\n",
      "     |      Desired range of transformed data.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace row normalization and avoid a\n",
      "     |      copy (if the input is already a numpy array).\n",
      "     |  \n",
      "     |  clip : bool, default=False\n",
      "     |      Set to True to clip transformed values of held-out data to\n",
      "     |      provided `feature range`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  min_ : ndarray of shape (n_features,)\n",
      "     |      Per feature adjustment for minimum. Equivalent to\n",
      "     |      ``min - X.min(axis=0) * self.scale_``\n",
      "     |  \n",
      "     |  scale_ : ndarray of shape (n_features,)\n",
      "     |      Per feature relative scaling of the data. Equivalent to\n",
      "     |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |  \n",
      "     |  data_min_ : ndarray of shape (n_features,)\n",
      "     |      Per feature minimum seen in the data\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_min_*\n",
      "     |  \n",
      "     |  data_max_ : ndarray of shape (n_features,)\n",
      "     |      Per feature maximum seen in the data\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_max_*\n",
      "     |  \n",
      "     |  data_range_ : ndarray of shape (n_features,)\n",
      "     |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *data_range_*\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  n_samples_seen_ : int\n",
      "     |      The number of samples processed by the estimator.\n",
      "     |      It will be reset on new calls to fit, but increments across\n",
      "     |      ``partial_fit`` calls.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  minmax_scale : Equivalent function without the estimator API.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      "     |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      "     |  >>> scaler = MinMaxScaler()\n",
      "     |  >>> print(scaler.fit(data))\n",
      "     |  MinMaxScaler()\n",
      "     |  >>> print(scaler.data_max_)\n",
      "     |  [ 1. 18.]\n",
      "     |  >>> print(scaler.transform(data))\n",
      "     |  [[0.   0.  ]\n",
      "     |   [0.25 0.25]\n",
      "     |   [0.5  0.5 ]\n",
      "     |   [1.   1.  ]]\n",
      "     |  >>> print(scaler.transform([[2, 2]]))\n",
      "     |  [[1.5 0. ]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MinMaxScaler\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the minimum and maximum to be used for later scaling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to compute the per-feature minimum and maximum\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Undo the scaling of X according to feature_range.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data that will be transformed. It cannot be sparse.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Online computation of min and max on X for later scaling.\n",
      "     |      \n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Scale features of X according to feature_range.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data that will be transformed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiLabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  MultiLabelBinarizer(*, classes=None, sparse_output=False)\n",
      "     |  \n",
      "     |  Transform between iterable of iterables and a multilabel format.\n",
      "     |  \n",
      "     |  Although a list of sets or tuples is a very intuitive format for multilabel\n",
      "     |  data, it is unwieldy to process. This transformer converts between this\n",
      "     |  intuitive format and the supported multilabel format: a (samples x classes)\n",
      "     |  binary matrix indicating the presence of a class label.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  classes : array-like of shape (n_classes,), default=None\n",
      "     |      Indicates an ordering for the class labels.\n",
      "     |      All entries should be unique (cannot contain duplicate classes).\n",
      "     |  \n",
      "     |  sparse_output : bool, default=False\n",
      "     |      Set to True if output binary array is desired in CSR sparse format.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      A copy of the `classes` parameter when provided.\n",
      "     |      Otherwise it corresponds to the sorted set of classes found\n",
      "     |      when fitting.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OneHotEncoder : Encode categorical features using a one-hot aka one-of-K\n",
      "     |      scheme.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import MultiLabelBinarizer\n",
      "     |  >>> mlb = MultiLabelBinarizer()\n",
      "     |  >>> mlb.fit_transform([(1, 2), (3,)])\n",
      "     |  array([[1, 1, 0],\n",
      "     |         [0, 0, 1]])\n",
      "     |  >>> mlb.classes_\n",
      "     |  array([1, 2, 3])\n",
      "     |  \n",
      "     |  >>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\n",
      "     |  array([[0, 1, 1],\n",
      "     |         [1, 0, 0]])\n",
      "     |  >>> list(mlb.classes_)\n",
      "     |  ['comedy', 'sci-fi', 'thriller']\n",
      "     |  \n",
      "     |  A common mistake is to pass in a list, which leads to the following issue:\n",
      "     |  \n",
      "     |  >>> mlb = MultiLabelBinarizer()\n",
      "     |  >>> mlb.fit(['sci-fi', 'thriller', 'comedy'])\n",
      "     |  MultiLabelBinarizer()\n",
      "     |  >>> mlb.classes_\n",
      "     |  array(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',\n",
      "     |      'y'], dtype=object)\n",
      "     |  \n",
      "     |  To correct this, the list of labels should be passed in as:\n",
      "     |  \n",
      "     |  >>> mlb = MultiLabelBinarizer()\n",
      "     |  >>> mlb.fit([['sci-fi', 'thriller', 'comedy']])\n",
      "     |  MultiLabelBinarizer()\n",
      "     |  >>> mlb.classes_\n",
      "     |  array(['comedy', 'sci-fi', 'thriller'], dtype=object)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiLabelBinarizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, classes=None, sparse_output=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, y)\n",
      "     |      Fit the label sets binarizer, storing :term:`classes_`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : iterable of iterables\n",
      "     |          A set of labels (any orderable and hashable object) for each\n",
      "     |          sample. If the `classes` parameter is set, `y` will not be\n",
      "     |          iterated.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  fit_transform(self, y)\n",
      "     |      Fit the label sets binarizer and transform the given label sets.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : iterable of iterables\n",
      "     |          A set of labels (any orderable and hashable object) for each\n",
      "     |          sample. If the `classes` parameter is set, `y` will not be\n",
      "     |          iterated.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_indicator : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "     |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]`\n",
      "     |          is in `y[i]`, and 0 otherwise. Sparse matrix will be of CSR\n",
      "     |          format.\n",
      "     |  \n",
      "     |  inverse_transform(self, yt)\n",
      "     |      Transform the given indicator matrix into label sets.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      yt : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "     |          A matrix containing only 1s ands 0s.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : list of tuples\n",
      "     |          The set of labels for each sample such that `y[i]` consists of\n",
      "     |          `classes_[j]` for each `yt[i, j] == 1`.\n",
      "     |  \n",
      "     |  transform(self, y)\n",
      "     |      Transform the given label sets.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y : iterable of iterables\n",
      "     |          A set of labels (any orderable and hashable object) for each\n",
      "     |          sample. If the `classes` parameter is set, `y` will not be\n",
      "     |          iterated.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n",
      "     |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n",
      "     |          `y[i]`, and 0 otherwise.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Normalizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Normalizer(norm='l2', *, copy=True)\n",
      "     |  \n",
      "     |  Normalize samples individually to unit norm.\n",
      "     |  \n",
      "     |  Each sample (i.e. each row of the data matrix) with at least one\n",
      "     |  non zero component is rescaled independently of other samples so\n",
      "     |  that its norm (l1, l2 or inf) equals one.\n",
      "     |  \n",
      "     |  This transformer is able to work both with dense numpy arrays and\n",
      "     |  scipy.sparse matrix (use CSR format if you want to avoid the burden of\n",
      "     |  a copy / conversion).\n",
      "     |  \n",
      "     |  Scaling inputs to unit norms is a common operation for text\n",
      "     |  classification or clustering for instance. For instance the dot\n",
      "     |  product of two l2-normalized TF-IDF vectors is the cosine similarity\n",
      "     |  of the vectors and is the base similarity metric for the Vector\n",
      "     |  Space Model commonly used by the Information Retrieval community.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  norm : {'l1', 'l2', 'max'}, default='l2'\n",
      "     |      The norm to use to normalize each non zero sample. If norm='max'\n",
      "     |      is used, values will be rescaled by the maximum of the absolute\n",
      "     |      values.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace row normalization and avoid a\n",
      "     |      copy (if the input is already a numpy array or a scipy.sparse\n",
      "     |      CSR matrix).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  normalize : Equivalent function without the estimator API.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This estimator is stateless (besides constructor parameters), the\n",
      "     |  fit method does nothing but is useful when used in a pipeline.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import Normalizer\n",
      "     |  >>> X = [[4, 1, 2, 2],\n",
      "     |  ...      [1, 3, 9, 3],\n",
      "     |  ...      [5, 7, 5, 1]]\n",
      "     |  >>> transformer = Normalizer().fit(X)  # fit does nothing.\n",
      "     |  >>> transformer\n",
      "     |  Normalizer()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[0.8, 0.2, 0.4, 0.4],\n",
      "     |         [0.1, 0.3, 0.9, 0.3],\n",
      "     |         [0.5, 0.7, 0.5, 0.1]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Normalizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, norm='l2', *, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Do nothing and return the estimator unchanged.\n",
      "     |      \n",
      "     |      This method is just there to implement the usual API and hence\n",
      "     |      work in pipelines.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to estimate the normalization parameters.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |  \n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Scale each non zero row of X to unit norm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to normalize, row by row. scipy.sparse matrices should be\n",
      "     |          in CSR format to avoid an un-necessary copy.\n",
      "     |      \n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input X or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OneHotEncoder(_BaseEncoder)\n",
      "     |  OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      "     |  \n",
      "     |  Encode categorical features as a one-hot numeric array.\n",
      "     |  \n",
      "     |  The input to this transformer should be an array-like of integers or\n",
      "     |  strings, denoting the values taken on by categorical (discrete) features.\n",
      "     |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      "     |  encoding scheme. This creates a binary column for each category and\n",
      "     |  returns a sparse matrix or dense array (depending on the ``sparse``\n",
      "     |  parameter)\n",
      "     |  \n",
      "     |  By default, the encoder derives the categories based on the unique values\n",
      "     |  in each feature. Alternatively, you can also specify the `categories`\n",
      "     |  manually.\n",
      "     |  \n",
      "     |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      "     |  estimators, notably linear models and SVMs with the standard kernels.\n",
      "     |  \n",
      "     |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  categories : 'auto' or a list of array-like, default='auto'\n",
      "     |      Categories (unique values) per feature:\n",
      "     |  \n",
      "     |      - 'auto' : Determine categories automatically from the training data.\n",
      "     |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      "     |        column. The passed categories should not mix strings and numeric\n",
      "     |        values within a single feature, and should be sorted in case of\n",
      "     |        numeric values.\n",
      "     |  \n",
      "     |      The used categories can be found in the ``categories_`` attribute.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  drop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None\n",
      "     |      Specifies a methodology to use to drop one of the categories per\n",
      "     |      feature. This is useful in situations where perfectly collinear\n",
      "     |      features cause problems, such as when feeding the resulting data\n",
      "     |      into a neural network or an unregularized regression.\n",
      "     |  \n",
      "     |      However, dropping one category breaks the symmetry of the original\n",
      "     |      representation and can therefore induce a bias in downstream models,\n",
      "     |      for instance for penalized linear classification or regression models.\n",
      "     |  \n",
      "     |      - None : retain all features (the default).\n",
      "     |      - 'first' : drop the first category in each feature. If only one\n",
      "     |        category is present, the feature will be dropped entirely.\n",
      "     |      - 'if_binary' : drop the first category in each feature with two\n",
      "     |        categories. Features with 1 or more than 2 categories are\n",
      "     |        left intact.\n",
      "     |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      "     |        should be dropped.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |         The parameter `drop` was added in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.23\n",
      "     |         The option `drop='if_binary'` was added in 0.23.\n",
      "     |  \n",
      "     |  sparse : bool, default=True\n",
      "     |      Will return sparse matrix if set True else will return an array.\n",
      "     |  \n",
      "     |  dtype : number type, default=float\n",
      "     |      Desired dtype of output.\n",
      "     |  \n",
      "     |  handle_unknown : {'error', 'ignore'}, default='error'\n",
      "     |      Whether to raise an error or ignore if an unknown categorical feature\n",
      "     |      is present during transform (default is to raise). When this parameter\n",
      "     |      is set to 'ignore' and an unknown category is encountered during\n",
      "     |      transform, the resulting one-hot encoded columns for this feature\n",
      "     |      will be all zeros. In the inverse transform, an unknown category\n",
      "     |      will be denoted as None.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  categories_ : list of arrays\n",
      "     |      The categories of each feature determined during fitting\n",
      "     |      (in order of the features in X and corresponding with the output\n",
      "     |      of ``transform``). This includes the category specified in ``drop``\n",
      "     |      (if any).\n",
      "     |  \n",
      "     |  drop_idx_ : array of shape (n_features,)\n",
      "     |      - ``drop_idx_[i]`` isÂ the index in ``categories_[i]`` of the category\n",
      "     |        to be dropped for each feature.\n",
      "     |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      "     |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      "     |        feature isn't binary.\n",
      "     |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      "     |        retained.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.23\n",
      "     |         Added the possibility to contain `None` values.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OrdinalEncoder : Performs an ordinal (integer)\n",
      "     |    encoding of the categorical features.\n",
      "     |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      "     |    dictionary items (also handles string-valued features).\n",
      "     |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      "     |    encoding of dictionary items or strings.\n",
      "     |  LabelBinarizer : Binarizes labels in a one-vs-all\n",
      "     |    fashion.\n",
      "     |  MultiLabelBinarizer : Transforms between iterable of\n",
      "     |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      "     |    matrix indicating the presence of a class label.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Given a dataset with two features, we let the encoder find the unique\n",
      "     |  values per feature and transform the data to a binary one-hot encoding.\n",
      "     |  \n",
      "     |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      "     |  \n",
      "     |  One can discard categories not seen during `fit`:\n",
      "     |  \n",
      "     |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      "     |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      "     |  >>> enc.fit(X)\n",
      "     |  OneHotEncoder(handle_unknown='ignore')\n",
      "     |  >>> enc.categories_\n",
      "     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "     |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      "     |  array([[1., 0., 1., 0., 0.],\n",
      "     |         [0., 1., 0., 0., 0.]])\n",
      "     |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      "     |  array([['Male', 1],\n",
      "     |         [None, 2]], dtype=object)\n",
      "     |  >>> enc.get_feature_names_out(['gender', 'group'])\n",
      "     |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)\n",
      "     |  \n",
      "     |  One can always drop the first column for each feature:\n",
      "     |  \n",
      "     |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      "     |  >>> drop_enc.categories_\n",
      "     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "     |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      "     |  array([[0., 0., 0.],\n",
      "     |         [1., 1., 0.]])\n",
      "     |  \n",
      "     |  Or drop a column for feature only having 2 categories:\n",
      "     |  \n",
      "     |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      "     |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      "     |  array([[0., 1., 0., 0.],\n",
      "     |         [1., 0., 1., 0.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotEncoder\n",
      "     |      _BaseEncoder\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit OneHotEncoder to X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to determine the categories of each feature.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted encoder.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit OneHotEncoder to X, then transform X.\n",
      "     |      \n",
      "     |      Equivalent to fit(X).transform(X) but more convenient.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to encode.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      "     |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      "     |          returned.\n",
      "     |  \n",
      "     |  get_feature_names(self, input_features=None)\n",
      "     |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "     |      \n",
      "     |      Return feature names for output features.\n",
      "     |      \n",
      "     |          Parameters\n",
      "     |          ----------\n",
      "     |          input_features : list of str of shape (n_features,)\n",
      "     |              String names for input features if available. By default,\n",
      "     |              \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      "     |      \n",
      "     |          Returns\n",
      "     |          -------\n",
      "     |          output_feature_names : ndarray of shape (n_output_features,)\n",
      "     |              Array of feature names.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Convert the data back to the original representation.\n",
      "     |      \n",
      "     |      When unknown categories are encountered (all zeros in the\n",
      "     |      one-hot encoding), ``None`` is used to represent this category. If the\n",
      "     |      feature with the unknown category has a dropped category, the dropped\n",
      "     |      category will be its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      "     |          The transformed data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : ndarray of shape (n_samples, n_features)\n",
      "     |          Inverse transformed array.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X using one-hot encoding.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to encode.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      "     |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      "     |          returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrdinalEncoder(_BaseEncoder)\n",
      "     |  OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None)\n",
      "     |  \n",
      "     |  Encode categorical features as an integer array.\n",
      "     |  \n",
      "     |  The input to this transformer should be an array-like of integers or\n",
      "     |  strings, denoting the values taken on by categorical (discrete) features.\n",
      "     |  The features are converted to ordinal integers. This results in\n",
      "     |  a single column of integers (0 to n_categories - 1) per feature.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  categories : 'auto' or a list of array-like, default='auto'\n",
      "     |      Categories (unique values) per feature:\n",
      "     |  \n",
      "     |      - 'auto' : Determine categories automatically from the training data.\n",
      "     |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      "     |        column. The passed categories should not mix strings and numeric\n",
      "     |        values, and should be sorted in case of numeric values.\n",
      "     |  \n",
      "     |      The used categories can be found in the ``categories_`` attribute.\n",
      "     |  \n",
      "     |  dtype : number type, default np.float64\n",
      "     |      Desired dtype of output.\n",
      "     |  \n",
      "     |  handle_unknown : {'error', 'use_encoded_value'}, default='error'\n",
      "     |      When set to 'error' an error will be raised in case an unknown\n",
      "     |      categorical feature is present during transform. When set to\n",
      "     |      'use_encoded_value', the encoded value of unknown categories will be\n",
      "     |      set to the value given for the parameter `unknown_value`. In\n",
      "     |      :meth:`inverse_transform`, an unknown category will be denoted as None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  unknown_value : int or np.nan, default=None\n",
      "     |      When the parameter handle_unknown is set to 'use_encoded_value', this\n",
      "     |      parameter is required and will set the encoded value of unknown\n",
      "     |      categories. It has to be distinct from the values used to encode any of\n",
      "     |      the categories in `fit`. If set to np.nan, the `dtype` parameter must\n",
      "     |      be a float dtype.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  categories_ : list of arrays\n",
      "     |      The categories of each feature determined during ``fit`` (in order of\n",
      "     |      the features in X and corresponding with the output of ``transform``).\n",
      "     |      This does not include categories that weren't seen during ``fit``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  OneHotEncoder : Performs a one-hot encoding of categorical features.\n",
      "     |  LabelEncoder : Encodes target labels with values between 0 and\n",
      "     |      ``n_classes-1``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Given a dataset with two features, we let the encoder find the unique\n",
      "     |  values per feature and transform the data to an ordinal encoding.\n",
      "     |  \n",
      "     |  >>> from sklearn.preprocessing import OrdinalEncoder\n",
      "     |  >>> enc = OrdinalEncoder()\n",
      "     |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      "     |  >>> enc.fit(X)\n",
      "     |  OrdinalEncoder()\n",
      "     |  >>> enc.categories_\n",
      "     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      "     |  >>> enc.transform([['Female', 3], ['Male', 1]])\n",
      "     |  array([[0., 2.],\n",
      "     |         [1., 0.]])\n",
      "     |  \n",
      "     |  >>> enc.inverse_transform([[1, 0], [0, 1]])\n",
      "     |  array([['Male', 1],\n",
      "     |         ['Female', 2]], dtype=object)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrdinalEncoder\n",
      "     |      _BaseEncoder\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the OrdinalEncoder to X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to determine the categories of each feature.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted encoder.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Convert the data back to the original representation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_encoded_features)\n",
      "     |          The transformed data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : ndarray of shape (n_samples, n_features)\n",
      "     |          Inverse transformed array.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform X to ordinal codes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to encode.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_out : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      "     |  \n",
      "     |  Generate polynomial and interaction features.\n",
      "     |  \n",
      "     |  Generate a new feature matrix consisting of all polynomial combinations\n",
      "     |  of the features with degree less than or equal to the specified degree.\n",
      "     |  For example, if an input sample is two dimensional and of the form\n",
      "     |  [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <polynomial_features>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  degree : int or tuple (min_degree, max_degree), default=2\n",
      "     |      If a single int is given, it specifies the maximal degree of the\n",
      "     |      polynomial features. If a tuple `(min_degree, max_degree)` is passed,\n",
      "     |      then `min_degree` is the minimum and `max_degree` is the maximum\n",
      "     |      polynomial degree of the generated features. Note that `min_degree=0`\n",
      "     |      and `min_degree=1` are equivalent as outputting the degree zero term is\n",
      "     |      determined by `include_bias`.\n",
      "     |  \n",
      "     |  interaction_only : bool, default=False\n",
      "     |      If `True`, only interaction features are produced: features that are\n",
      "     |      products of at most `degree` *distinct* input features, i.e. terms with\n",
      "     |      power of 2 or higher of the same input feature are excluded:\n",
      "     |  \n",
      "     |          - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc.\n",
      "     |          - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.\n",
      "     |  \n",
      "     |  include_bias : bool, default=True\n",
      "     |      If `True` (default), then include a bias column, the feature in which\n",
      "     |      all polynomial powers are zero (i.e. a column of ones - acts as an\n",
      "     |      intercept term in a linear model).\n",
      "     |  \n",
      "     |  order : {'C', 'F'}, default='C'\n",
      "     |      Order of output array in the dense case. `'F'` order is faster to\n",
      "     |      compute, but may slow down subsequent estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  powers_ : ndarray of shape (`n_output_features_`, `n_features_in_`)\n",
      "     |      `powers_[i, j]` is the exponent of the jth input in the ith output.\n",
      "     |  \n",
      "     |  n_input_features_ : int\n",
      "     |      The total number of input features.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.0\n",
      "     |          This attribute is deprecated in 1.0 and will be removed in 1.2.\n",
      "     |          Refer to `n_features_in_` instead.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_output_features_ : int\n",
      "     |      The total number of polynomial output features. The number of output\n",
      "     |      features is computed by iterating over all suitably sized combinations\n",
      "     |      of input features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SplineTransformer : Transformer that generates univariate B-spline bases\n",
      "     |      for features.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Be aware that the number of features in the output array scales\n",
      "     |  polynomially in the number of features of the input array, and\n",
      "     |  exponentially in the degree. High degrees can cause overfitting.\n",
      "     |  \n",
      "     |  See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import PolynomialFeatures\n",
      "     |  >>> X = np.arange(6).reshape(3, 2)\n",
      "     |  >>> X\n",
      "     |  array([[0, 1],\n",
      "     |         [2, 3],\n",
      "     |         [4, 5]])\n",
      "     |  >>> poly = PolynomialFeatures(2)\n",
      "     |  >>> poly.fit_transform(X)\n",
      "     |  array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
      "     |         [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
      "     |         [ 1.,  4.,  5., 16., 20., 25.]])\n",
      "     |  >>> poly = PolynomialFeatures(interaction_only=True)\n",
      "     |  >>> poly.fit_transform(X)\n",
      "     |  array([[ 1.,  0.,  1.,  0.],\n",
      "     |         [ 1.,  2.,  3.,  6.],\n",
      "     |         [ 1.,  4.,  5., 20.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PolynomialFeatures\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute number of output features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |  \n",
      "     |  get_feature_names(self, input_features=None)\n",
      "     |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "     |      \n",
      "     |      Return feature names for output features.\n",
      "     |      \n",
      "     |          Parameters\n",
      "     |          ----------\n",
      "     |          input_features : list of str of shape (n_features,), default=None\n",
      "     |              String names for input features if available. By default,\n",
      "     |              \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      "     |      \n",
      "     |          Returns\n",
      "     |          -------\n",
      "     |          output_feature_names : list of str of shape (n_output_features,)\n",
      "     |              Transformed feature names.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features is None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform data to polynomial features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data to transform, row by row.\n",
      "     |      \n",
      "     |          Prefer CSR over CSC for sparse input (for speed), but CSC is\n",
      "     |          required if the degree is 4 or higher. If the degree is less than\n",
      "     |          4 and the input format is CSC, it will be converted to CSR, have\n",
      "     |          its polynomial features generated, then converted back to CSC.\n",
      "     |      \n",
      "     |          If the degree is 2 or 3, the method described in \"Leveraging\n",
      "     |          Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n",
      "     |          Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n",
      "     |          used, which is much faster than the method used on CSC input. For\n",
      "     |          this reason, a CSC input will be converted to CSR, and the output\n",
      "     |          will be converted back to CSC prior to being returned, hence the\n",
      "     |          preference of CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      XP : {ndarray, sparse matrix} of shape (n_samples, NP)\n",
      "     |          The matrix of features, where `NP` is the number of polynomial\n",
      "     |          features generated from the combination of inputs. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          `csr_matrix`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  n_input_features_\n",
      "     |      DEPRECATED: The attribute `n_input_features_` was deprecated in version 1.0 and will be removed in 1.2.\n",
      "     |  \n",
      "     |  powers_\n",
      "     |      Exponent for each of the inputs in the output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PowerTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  PowerTransformer(method='yeo-johnson', *, standardize=True, copy=True)\n",
      "     |  \n",
      "     |  Apply a power transform featurewise to make data more Gaussian-like.\n",
      "     |  \n",
      "     |  Power transforms are a family of parametric, monotonic transformations\n",
      "     |  that are applied to make data more Gaussian-like. This is useful for\n",
      "     |  modeling issues related to heteroscedasticity (non-constant variance),\n",
      "     |  or other situations where normality is desired.\n",
      "     |  \n",
      "     |  Currently, PowerTransformer supports the Box-Cox transform and the\n",
      "     |  Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      "     |  minimizing skewness is estimated through maximum likelihood.\n",
      "     |  \n",
      "     |  Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      "     |  supports both positive or negative data.\n",
      "     |  \n",
      "     |  By default, zero-mean, unit-variance normalization is applied to the\n",
      "     |  transformed data.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n",
      "     |      The power transform method. Available methods are:\n",
      "     |  \n",
      "     |      - 'yeo-johnson' [1]_, works with positive and negative values\n",
      "     |      - 'box-cox' [2]_, only works with strictly positive values\n",
      "     |  \n",
      "     |  standardize : bool, default=True\n",
      "     |      Set to True to apply zero-mean, unit-variance normalization to the\n",
      "     |      transformed output.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace computation during transformation.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  lambdas_ : ndarray of float of shape (n_features,)\n",
      "     |      The parameters of the power transformation for the selected features.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  power_transform : Equivalent function without the estimator API.\n",
      "     |  \n",
      "     |  QuantileTransformer : Maps data to a standard normal distribution with\n",
      "     |      the parameter `output_distribution='normal'`.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      "     |  in ``transform``.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n",
      "     |         improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n",
      "     |         (2000).\n",
      "     |  \n",
      "     |  .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n",
      "     |         of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import PowerTransformer\n",
      "     |  >>> pt = PowerTransformer()\n",
      "     |  >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      "     |  >>> print(pt.fit(data))\n",
      "     |  PowerTransformer()\n",
      "     |  >>> print(pt.lambdas_)\n",
      "     |  [ 1.386... -3.100...]\n",
      "     |  >>> print(pt.transform(data))\n",
      "     |  [[-1.316... -0.707...]\n",
      "     |   [ 0.209... -0.707...]\n",
      "     |   [ 1.106...  1.414...]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PowerTransformer\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, method='yeo-johnson', *, standardize=True, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Estimate the optimal parameter lambda for each feature.\n",
      "     |      \n",
      "     |      The optimal lambda parameter for minimizing skewness is estimated on\n",
      "     |      each feature independently using maximum likelihood.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to estimate the optimal transformation parameters.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit `PowerTransformer` to `X`, then transform `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data used to estimate the optimal transformation parameters\n",
      "     |          and to be transformed using a power transformation.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray of shape (n_samples, n_features)\n",
      "     |          Transformed data.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Apply the inverse power transformation using the fitted lambdas.\n",
      "     |      \n",
      "     |      The inverse of the Box-Cox transformation is given by::\n",
      "     |      \n",
      "     |          if lambda_ == 0:\n",
      "     |              X = exp(X_trans)\n",
      "     |          else:\n",
      "     |              X = (X_trans * lambda_ + 1) ** (1 / lambda_)\n",
      "     |      \n",
      "     |      The inverse of the Yeo-Johnson transformation is given by::\n",
      "     |      \n",
      "     |          if X >= 0 and lambda_ == 0:\n",
      "     |              X = exp(X_trans) - 1\n",
      "     |          elif X >= 0 and lambda_ != 0:\n",
      "     |              X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
      "     |          elif X < 0 and lambda_ != 2:\n",
      "     |              X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
      "     |          elif X < 0 and lambda_ == 2:\n",
      "     |              X = 1 - exp(-X_trans)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The transformed data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          The original data.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Apply the power transform to each feature using the fitted lambdas.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to be transformed using a power transformation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_trans : ndarray of shape (n_samples, n_features)\n",
      "     |          The transformed data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class QuantileTransformer(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  QuantileTransformer(*, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      "     |  \n",
      "     |  Transform features using quantiles information.\n",
      "     |  \n",
      "     |  This method transforms the features to follow a uniform or a normal\n",
      "     |  distribution. Therefore, for a given feature, this transformation tends\n",
      "     |  to spread out the most frequent values. It also reduces the impact of\n",
      "     |  (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      "     |  \n",
      "     |  The transformation is applied on each feature independently. First an\n",
      "     |  estimate of the cumulative distribution function of a feature is\n",
      "     |  used to map the original values to a uniform distribution. The obtained\n",
      "     |  values are then mapped to the desired output distribution using the\n",
      "     |  associated quantile function. Features values of new/unseen data that fall\n",
      "     |  below or above the fitted range will be mapped to the bounds of the output\n",
      "     |  distribution. Note that this transform is non-linear. It may distort linear\n",
      "     |  correlations between variables measured at the same scale but renders\n",
      "     |  variables measured at different scales more directly comparable.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_quantiles : int, default=1000 or n_samples\n",
      "     |      Number of quantiles to be computed. It corresponds to the number\n",
      "     |      of landmarks used to discretize the cumulative distribution function.\n",
      "     |      If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      "     |      to the number of samples as a larger number of quantiles does not give\n",
      "     |      a better approximation of the cumulative distribution function\n",
      "     |      estimator.\n",
      "     |  \n",
      "     |  output_distribution : {'uniform', 'normal'}, default='uniform'\n",
      "     |      Marginal distribution for the transformed data. The choices are\n",
      "     |      'uniform' (default) or 'normal'.\n",
      "     |  \n",
      "     |  ignore_implicit_zeros : bool, default=False\n",
      "     |      Only applies to sparse matrices. If True, the sparse entries of the\n",
      "     |      matrix are discarded to compute the quantile statistics. If False,\n",
      "     |      these entries are treated as zeros.\n",
      "     |  \n",
      "     |  subsample : int, default=1e5\n",
      "     |      Maximum number of samples used to estimate the quantiles for\n",
      "     |      computational efficiency. Note that the subsampling procedure may\n",
      "     |      differ for value-identical sparse and dense matrices.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for subsampling and smoothing\n",
      "     |      noise.\n",
      "     |      Please see ``subsample`` for more details.\n",
      "     |      Pass an int for reproducible results across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      Set to False to perform inplace transformation and avoid a copy (if the\n",
      "     |      input is already a numpy array).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_quantiles_ : int\n",
      "     |      The actual number of quantiles used to discretize the cumulative\n",
      "     |      distribution function.\n",
      "     |  \n",
      "     |  quantiles_ : ndarray of shape (n_quantiles, n_features)\n",
      "     |      The values corresponding the quantiles of reference.\n",
      "     |  \n",
      "     |  references_ : ndarray of shape (n_quantiles, )\n",
      "     |      Quantiles of references.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  quantile_transform : Equivalent function without the estimator API.\n",
      "     |  PowerTransformer : Perform mapping to a normal distribution using a power\n",
      "     |      transform.\n",
      "     |  StandardScaler : Perform standardization that is faster, but less robust\n",
      "     |      to outliers.\n",
      "     |  RobustScaler : Perform robust standardization that removes the influence\n",
      "     |      of outliers but does not put outliers and inliers on the same scale.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import QuantileTransformer\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      "     |  >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
      "     |  >>> qt.fit_transform(X)\n",
      "     |  array([...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      QuantileTransformer\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the quantiles used for transforming.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |         Fitted transformer.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Back-projection to the original space.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          The projected data.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Feature-wise transformation of the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis. If a sparse\n",
      "     |          matrix is provided, it will be converted into a sparse\n",
      "     |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      "     |          nonnegative if `ignore_implicit_zeros` is False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The projected data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RobustScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  RobustScaler(*, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "     |  \n",
      "     |  Scale features using statistics that are robust to outliers.\n",
      "     |  \n",
      "     |  This Scaler removes the median and scales the data according to\n",
      "     |  the quantile range (defaults to IQR: Interquartile Range).\n",
      "     |  The IQR is the range between the 1st quartile (25th quantile)\n",
      "     |  and the 3rd quartile (75th quantile).\n",
      "     |  \n",
      "     |  Centering and scaling happen independently on each feature by\n",
      "     |  computing the relevant statistics on the samples in the training\n",
      "     |  set. Median and interquartile range are then stored to be used on\n",
      "     |  later data using the :meth:`transform` method.\n",
      "     |  \n",
      "     |  Standardization of a dataset is a common requirement for many\n",
      "     |  machine learning estimators. Typically this is done by removing the mean\n",
      "     |  and scaling to unit variance. However, outliers can often influence the\n",
      "     |  sample mean / variance in a negative way. In such cases, the median and\n",
      "     |  the interquartile range often give better results.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  with_centering : bool, default=True\n",
      "     |      If `True`, center the data before scaling.\n",
      "     |      This will cause :meth:`transform` to raise an exception when attempted\n",
      "     |      on sparse matrices, because centering them entails building a dense\n",
      "     |      matrix which in common use cases is likely to be too large to fit in\n",
      "     |      memory.\n",
      "     |  \n",
      "     |  with_scaling : bool, default=True\n",
      "     |      If `True`, scale the data to interquartile range.\n",
      "     |  \n",
      "     |  quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0)\n",
      "     |      Quantile range used to calculate `scale_`. By default this is equal to\n",
      "     |      the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n",
      "     |      quantile.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  copy : bool, default=True\n",
      "     |      If `False`, try to avoid a copy and do inplace scaling instead.\n",
      "     |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      "     |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "     |      returned.\n",
      "     |  \n",
      "     |  unit_variance : bool, default=False\n",
      "     |      If `True`, scale data so that normally distributed features have a\n",
      "     |      variance of 1. In general, if the difference between the x-values of\n",
      "     |      `q_max` and `q_min` for a standard normal distribution is greater\n",
      "     |      than 1, the dataset will be scaled down. If less than 1, the dataset\n",
      "     |      will be scaled up.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  center_ : array of floats\n",
      "     |      The median value for each feature in the training set.\n",
      "     |  \n",
      "     |  scale_ : array of floats\n",
      "     |      The (scaled) interquartile range for each feature in the training set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_* attribute.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  robust_scale : Equivalent function without the estimator API.\n",
      "     |  sklearn.decomposition.PCA : Further removes the linear correlation across\n",
      "     |      features with 'whiten=True'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Median\n",
      "     |  https://en.wikipedia.org/wiki/Interquartile_range\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import RobustScaler\n",
      "     |  >>> X = [[ 1., -2.,  2.],\n",
      "     |  ...      [ -2.,  1.,  3.],\n",
      "     |  ...      [ 4.,  1., -2.]]\n",
      "     |  >>> transformer = RobustScaler().fit(X)\n",
      "     |  >>> transformer\n",
      "     |  RobustScaler()\n",
      "     |  >>> transformer.transform(X)\n",
      "     |  array([[ 0. , -2. ,  0. ],\n",
      "     |         [-1. ,  0. ,  0.4],\n",
      "     |         [ 1. ,  0. , -1.6]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RobustScaler\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Compute the median and quantiles to be used for scaling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the median and quantiles\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The rescaled data to be transformed back.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Center and scale the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the specified axis.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SplineTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  SplineTransformer(n_knots=5, degree=3, *, knots='uniform', extrapolation='constant', include_bias=True, order='C')\n",
      "     |  \n",
      "     |  Generate univariate B-spline bases for features.\n",
      "     |  \n",
      "     |  Generate a new feature matrix consisting of\n",
      "     |  `n_splines=n_knots + degree - 1` (`n_knots - 1` for\n",
      "     |  `extrapolation=\"periodic\"`) spline basis functions\n",
      "     |  (B-splines) of polynomial order=`degree` for each feature.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <spline_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_knots : int, default=5\n",
      "     |      Number of knots of the splines if `knots` equals one of\n",
      "     |      {'uniform', 'quantile'}. Must be larger or equal 2. Ignored if `knots`\n",
      "     |      is array-like.\n",
      "     |  \n",
      "     |  degree : int, default=3\n",
      "     |      The polynomial degree of the spline basis. Must be a non-negative\n",
      "     |      integer.\n",
      "     |  \n",
      "     |  knots : {'uniform', 'quantile'} or array-like of shape         (n_knots, n_features), default='uniform'\n",
      "     |      Set knot positions such that first knot <= features <= last knot.\n",
      "     |  \n",
      "     |      - If 'uniform', `n_knots` number of knots are distributed uniformly\n",
      "     |        from min to max values of the features.\n",
      "     |      - If 'quantile', they are distributed uniformly along the quantiles of\n",
      "     |        the features.\n",
      "     |      - If an array-like is given, it directly specifies the sorted knot\n",
      "     |        positions including the boundary knots. Note that, internally,\n",
      "     |        `degree` number of knots are added before the first knot, the same\n",
      "     |        after the last knot.\n",
      "     |  \n",
      "     |  extrapolation : {'error', 'constant', 'linear', 'continue', 'periodic'},         default='constant'\n",
      "     |      If 'error', values outside the min and max values of the training\n",
      "     |      features raises a `ValueError`. If 'constant', the value of the\n",
      "     |      splines at minimum and maximum value of the features is used as\n",
      "     |      constant extrapolation. If 'linear', a linear extrapolation is used.\n",
      "     |      If 'continue', the splines are extrapolated as is, i.e. option\n",
      "     |      `extrapolate=True` in :class:`scipy.interpolate.BSpline`. If\n",
      "     |      'periodic', periodic splines with a periodicity equal to the distance\n",
      "     |      between the first and last knot are used. Periodic splines enforce\n",
      "     |      equal function values and derivatives at the first and last knot.\n",
      "     |      For example, this makes it possible to avoid introducing an arbitrary\n",
      "     |      jump between Dec 31st and Jan 1st in spline features derived from a\n",
      "     |      naturally periodic \"day-of-year\" input feature. In this case it is\n",
      "     |      recommended to manually set the knot values to control the period.\n",
      "     |  \n",
      "     |  include_bias : bool, default=True\n",
      "     |      If True (default), then the last spline element inside the data range\n",
      "     |      of a feature is dropped. As B-splines sum to one over the spline basis\n",
      "     |      functions for each data point, they implicitly include a bias term,\n",
      "     |      i.e. a column of ones. It acts as an intercept term in a linear models.\n",
      "     |  \n",
      "     |  order : {'C', 'F'}, default='C'\n",
      "     |      Order of output array. 'F' order is faster to compute, but may slow\n",
      "     |      down subsequent estimators.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  bsplines_ : list of shape (n_features,)\n",
      "     |      List of BSplines objects, one for each feature.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      The total number of input features.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_features_out_ : int\n",
      "     |      The total number of output features, which is computed as\n",
      "     |      `n_features * n_splines`, where `n_splines` is\n",
      "     |      the number of bases elements of the B-splines,\n",
      "     |      `n_knots + degree - 1` for non-periodic splines and\n",
      "     |      `n_knots - 1` for periodic ones.\n",
      "     |      If `include_bias=False`, then it is only\n",
      "     |      `n_features * (n_splines - 1)`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KBinsDiscretizer : Transformer that bins continuous data into intervals.\n",
      "     |  \n",
      "     |  PolynomialFeatures : Transformer that generates polynomial and interaction\n",
      "     |      features.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  High degrees and a high number of knots can cause overfitting.\n",
      "     |  \n",
      "     |  See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.preprocessing import SplineTransformer\n",
      "     |  >>> X = np.arange(6).reshape(6, 1)\n",
      "     |  >>> spline = SplineTransformer(degree=2, n_knots=3)\n",
      "     |  >>> spline.fit_transform(X)\n",
      "     |  array([[0.5 , 0.5 , 0.  , 0.  ],\n",
      "     |         [0.18, 0.74, 0.08, 0.  ],\n",
      "     |         [0.02, 0.66, 0.32, 0.  ],\n",
      "     |         [0.  , 0.32, 0.66, 0.02],\n",
      "     |         [0.  , 0.08, 0.74, 0.18],\n",
      "     |         [0.  , 0.  , 0.5 , 0.5 ]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SplineTransformer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_knots=5, degree=3, *, knots='uniform', extrapolation='constant', include_bias=True, order='C')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute knot positions of splines.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default = None\n",
      "     |          Individual weights for each sample. Used to calculate quantiles if\n",
      "     |          `knots=\"quantile\"`. For `knots=\"uniform\"`, zero weighted\n",
      "     |          observations are ignored for finding the min and max of `X`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted transformer.\n",
      "     |  \n",
      "     |  get_feature_names(self, input_features=None)\n",
      "     |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "     |      \n",
      "     |      Return feature names for output features.\n",
      "     |      \n",
      "     |          Parameters\n",
      "     |          ----------\n",
      "     |          input_features : list of str of shape (n_features,), default=None\n",
      "     |              String names for input features if available. By default,\n",
      "     |              \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      "     |      \n",
      "     |          Returns\n",
      "     |          -------\n",
      "     |          output_feature_names : list of str of shape (n_output_features,)\n",
      "     |              Transformed feature names.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform each feature data to B-splines.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The data to transform.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      XBS : ndarray of shape (n_samples, n_features * n_splines)\n",
      "     |          The matrix of features, where n_splines is the number of bases\n",
      "     |          elements of the B-splines, n_knots + degree - 1.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      "     |  \n",
      "     |  Standardize features by removing the mean and scaling to unit variance.\n",
      "     |  \n",
      "     |  The standard score of a sample `x` is calculated as:\n",
      "     |  \n",
      "     |      z = (x - u) / s\n",
      "     |  \n",
      "     |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "     |  and `s` is the standard deviation of the training samples or one if\n",
      "     |  `with_std=False`.\n",
      "     |  \n",
      "     |  Centering and scaling happen independently on each feature by computing\n",
      "     |  the relevant statistics on the samples in the training set. Mean and\n",
      "     |  standard deviation are then stored to be used on later data using\n",
      "     |  :meth:`transform`.\n",
      "     |  \n",
      "     |  Standardization of a dataset is a common requirement for many\n",
      "     |  machine learning estimators: they might behave badly if the\n",
      "     |  individual features do not more or less look like standard normally\n",
      "     |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "     |  \n",
      "     |  For instance many elements used in the objective function of\n",
      "     |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      "     |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "     |  all features are centered around 0 and have variance in the same\n",
      "     |  order. If a feature has a variance that is orders of magnitude larger\n",
      "     |  that others, it might dominate the objective function and make the\n",
      "     |  estimator unable to learn from other features correctly as expected.\n",
      "     |  \n",
      "     |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "     |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      If False, try to avoid a copy and do inplace scaling instead.\n",
      "     |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      "     |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "     |      returned.\n",
      "     |  \n",
      "     |  with_mean : bool, default=True\n",
      "     |      If True, center the data before scaling.\n",
      "     |      This does not work (and will raise an exception) when attempted on\n",
      "     |      sparse matrices, because centering them entails building a dense\n",
      "     |      matrix which in common use cases is likely to be too large to fit in\n",
      "     |      memory.\n",
      "     |  \n",
      "     |  with_std : bool, default=True\n",
      "     |      If True, scale the data to unit variance (or equivalently,\n",
      "     |      unit standard deviation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scale_ : ndarray of shape (n_features,) or None\n",
      "     |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      "     |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      "     |      variance is zero, we can't achieve unit variance, and the data is left\n",
      "     |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      "     |      when `with_std=False`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *scale_*\n",
      "     |  \n",
      "     |  mean_ : ndarray of shape (n_features,) or None\n",
      "     |      The mean value for each feature in the training set.\n",
      "     |      Equal to ``None`` when ``with_mean=False``.\n",
      "     |  \n",
      "     |  var_ : ndarray of shape (n_features,) or None\n",
      "     |      The variance for each feature in the training set. Used to compute\n",
      "     |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      "     |      The number of samples processed by the estimator for each feature.\n",
      "     |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      "     |      integer, otherwise it will be an array of dtype int. If\n",
      "     |      `sample_weights` are used it will be a float (if no missing data)\n",
      "     |      or an array of dtype float that sums the weights seen so far.\n",
      "     |      Will be reset on new calls to fit, but increments across\n",
      "     |      ``partial_fit`` calls.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  scale : Equivalent function without the estimator API.\n",
      "     |  \n",
      "     |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      "     |      correlation across features with 'whiten=True'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "     |  transform.\n",
      "     |  \n",
      "     |  We use a biased estimator for the standard deviation, equivalent to\n",
      "     |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "     |  affect model performance.\n",
      "     |  \n",
      "     |  For a comparison of the different scalers, transformers, and normalizers,\n",
      "     |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      "     |  >>> scaler = StandardScaler()\n",
      "     |  >>> print(scaler.fit(data))\n",
      "     |  StandardScaler()\n",
      "     |  >>> print(scaler.mean_)\n",
      "     |  [0.5 0.5]\n",
      "     |  >>> print(scaler.transform(data))\n",
      "     |  [[-1. -1.]\n",
      "     |   [-1. -1.]\n",
      "     |   [ 1.  1.]\n",
      "     |   [ 1.  1.]]\n",
      "     |  >>> print(scaler.transform([[2, 2]]))\n",
      "     |  [[3. 3.]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StandardScaler\n",
      "     |      sklearn.base._OneToOneFeatureMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Compute the mean and std to be used for later scaling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.24\n",
      "     |             parameter *sample_weight* support to StandardScaler.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  inverse_transform(self, X, copy=None)\n",
      "     |      Scale back the data to the original representation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis.\n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input X or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Online computation of mean and std on X for later scaling.\n",
      "     |      \n",
      "     |      All of X is processed as a single batch. This is intended for cases\n",
      "     |      when :meth:`fit` is not feasible due to very large number of\n",
      "     |      `n_samples` or because X is read from a continuous stream.\n",
      "     |      \n",
      "     |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      "     |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      "     |      for computing the sample variance: Analysis and recommendations.\"\n",
      "     |      The American Statistician 37.3 (1983): 242-247:\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data used to compute the mean and standard deviation\n",
      "     |          used for later scaling along the features axis.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.24\n",
      "     |             parameter *sample_weight* support to StandardScaler.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted scaler.\n",
      "     |  \n",
      "     |  transform(self, X, copy=None)\n",
      "     |      Perform standardization by centering and scaling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      "     |          The data used to scale along the features axis.\n",
      "     |      copy : bool, default=None\n",
      "     |          Copy the input X or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Same as input features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    add_dummy_feature(X, value=1.0)\n",
      "        Augment dataset with an additional dummy feature.\n",
      "        \n",
      "        This is useful for fitting an intercept term with implementations which\n",
      "        cannot otherwise fit it directly.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Data.\n",
      "        \n",
      "        value : float\n",
      "            Value to use for the dummy feature.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)\n",
      "            Same data with dummy feature added as first column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import add_dummy_feature\n",
      "        >>> add_dummy_feature([[0, 1], [1, 0]])\n",
      "        array([[1., 0., 1.],\n",
      "               [1., 1., 0.]])\n",
      "    \n",
      "    binarize(X, *, threshold=0.0, copy=True)\n",
      "        Boolean thresholding of array-like or scipy.sparse matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to binarize, element by element.\n",
      "            scipy.sparse matrices should be in CSR or CSC format to avoid an\n",
      "            un-necessary copy.\n",
      "        \n",
      "        threshold : float, default=0.0\n",
      "            Feature values below or equal to this are replaced by 0, above it by 1.\n",
      "            Threshold may not be less than 0 for operations on sparse matrices.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            set to False to perform inplace binarization and avoid a copy\n",
      "            (if the input is already a numpy array or a scipy.sparse CSR / CSC\n",
      "            matrix and if axis is 1).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        Binarizer : Performs binarization using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False)\n",
      "        Binarize labels in a one-vs-all fashion.\n",
      "        \n",
      "        Several regression and binary classification algorithms are\n",
      "        available in scikit-learn. A simple way to extend these algorithms\n",
      "        to the multi-class classification case is to use the so-called\n",
      "        one-vs-all scheme.\n",
      "        \n",
      "        This function makes it possible to compute this transformation for a\n",
      "        fixed set of class labels known ahead of time.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array-like\n",
      "            Sequence of integer labels or multilabel data to encode.\n",
      "        \n",
      "        classes : array-like of shape (n_classes,)\n",
      "            Uniquely holds the label for each class.\n",
      "        \n",
      "        neg_label : int, default=0\n",
      "            Value with which negative labels must be encoded.\n",
      "        \n",
      "        pos_label : int, default=1\n",
      "            Value with which positive labels must be encoded.\n",
      "        \n",
      "        sparse_output : bool, default=False,\n",
      "            Set to true if output binary array is desired in CSR sparse format.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "            Shape will be (n_samples, 1) for binary problems. Sparse matrix will\n",
      "            be of CSR format.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.preprocessing import label_binarize\n",
      "        >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n",
      "        array([[1, 0, 0, 0],\n",
      "               [0, 0, 0, 1]])\n",
      "        \n",
      "        The class ordering is preserved:\n",
      "        \n",
      "        >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n",
      "        array([[1, 0, 0, 0],\n",
      "               [0, 1, 0, 0]])\n",
      "        \n",
      "        Binary targets transform to a column vector\n",
      "        \n",
      "        >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n",
      "        array([[1],\n",
      "               [0],\n",
      "               [0],\n",
      "               [1]])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        LabelBinarizer : Class used to wrap the functionality of label_binarize and\n",
      "            allow for fitting to classes independently of the transform operation.\n",
      "    \n",
      "    maxabs_scale(X, *, axis=0, copy=True)\n",
      "        Scale each feature to the [-1, 1] range without breaking the sparsity.\n",
      "        \n",
      "        This estimator scales each feature individually such\n",
      "        that the maximal absolute value of each feature in the\n",
      "        training set will be 1.0.\n",
      "        \n",
      "        This scaler can also be applied to sparse CSR or CSC matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data.\n",
      "        \n",
      "        axis : int, default=0\n",
      "            axis used to scale along. If 0, independently scale each feature,\n",
      "            otherwise (if 1) scale each sample.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "            is already a numpy array).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        .. warning:: Risk of data leak\n",
      "        \n",
      "            Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.MaxAbsScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MaxAbsScaler : Performs scaling to the [-1, 1] range using\n",
      "            the Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "        and maintained during the data transformation.\n",
      "        \n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True)\n",
      "        Transform features by scaling each feature to a given range.\n",
      "        \n",
      "        This estimator scales and translates each feature individually such\n",
      "        that it is in the given range on the training set, i.e. between\n",
      "        zero and one.\n",
      "        \n",
      "        The transformation is given by (when ``axis=0``)::\n",
      "        \n",
      "            X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "            X_scaled = X_std * (max - min) + min\n",
      "        \n",
      "        where min, max = feature_range.\n",
      "        \n",
      "        The transformation is calculated as (when ``axis=0``)::\n",
      "        \n",
      "           X_scaled = scale * X + min - X.min(axis=0) * scale\n",
      "           where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
      "        \n",
      "        This transformation is often used as an alternative to zero mean,\n",
      "        unit variance scaling.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           *minmax_scale* function interface\n",
      "           to :class:`~sklearn.preprocessing.MinMaxScaler`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data.\n",
      "        \n",
      "        feature_range : tuple (min, max), default=(0, 1)\n",
      "            Desired range of transformed data.\n",
      "        \n",
      "        axis : int, default=0\n",
      "            Axis used to scale along. If 0, independently scale each feature,\n",
      "            otherwise (if 1) scale each sample.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "            is already a numpy array).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : ndarray of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        .. warning:: Risk of data leak\n",
      "        \n",
      "            Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.MinMaxScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MinMaxScaler : Performs scaling to a given range using the Transformer\n",
      "            API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False)\n",
      "        Scale input vectors individually to unit norm (vector length).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to normalize, element by element.\n",
      "            scipy.sparse matrices should be in CSR format to avoid an\n",
      "            un-necessary copy.\n",
      "        \n",
      "        norm : {'l1', 'l2', 'max'}, default='l2'\n",
      "            The norm to use to normalize each non zero sample (or each non-zero\n",
      "            feature if axis is 0).\n",
      "        \n",
      "        axis : {0, 1}, default=1\n",
      "            axis used to normalize the data along. If 1, independently normalize\n",
      "            each sample, otherwise (if 0) normalize each feature.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            set to False to perform inplace row normalization and avoid a\n",
      "            copy (if the input is already a numpy array or a scipy.sparse\n",
      "            CSR matrix and if axis is 1).\n",
      "        \n",
      "        return_norm : bool, default=False\n",
      "            whether to return the computed norms\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            Normalized input X.\n",
      "        \n",
      "        norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n",
      "            An array of norms along given axis for X.\n",
      "            When X is sparse, a NotImplementedError will be raised\n",
      "            for norm 'l1' or 'l2'.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        Normalizer : Performs normalization using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    power_transform(X, method='yeo-johnson', *, standardize=True, copy=True)\n",
      "        Power transforms are a family of parametric, monotonic transformations\n",
      "        that are applied to make data more Gaussian-like. This is useful for\n",
      "        modeling issues related to heteroscedasticity (non-constant variance),\n",
      "        or other situations where normality is desired.\n",
      "        \n",
      "        Currently, power_transform supports the Box-Cox transform and the\n",
      "        Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      "        minimizing skewness is estimated through maximum likelihood.\n",
      "        \n",
      "        Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      "        supports both positive or negative data.\n",
      "        \n",
      "        By default, zero-mean, unit-variance normalization is applied to the\n",
      "        transformed data.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to be transformed using a power transformation.\n",
      "        \n",
      "        method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n",
      "            The power transform method. Available methods are:\n",
      "        \n",
      "            - 'yeo-johnson' [1]_, works with positive and negative values\n",
      "            - 'box-cox' [2]_, only works with strictly positive values\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "                The default value of the `method` parameter changed from\n",
      "                'box-cox' to 'yeo-johnson' in 0.23.\n",
      "        \n",
      "        standardize : bool, default=True\n",
      "            Set to True to apply zero-mean, unit-variance normalization to the\n",
      "            transformed output.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Set to False to perform inplace computation during transformation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_trans : ndarray of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.preprocessing import power_transform\n",
      "        >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      "        >>> print(power_transform(data, method='box-cox'))\n",
      "        [[-1.332... -0.707...]\n",
      "         [ 0.256... -0.707...]\n",
      "         [ 1.076...  1.414...]]\n",
      "        \n",
      "        .. warning:: Risk of data leak.\n",
      "            Do not use :func:`~sklearn.preprocessing.power_transform` unless you\n",
      "            know what you are doing. A common mistake is to apply it to the entire\n",
      "            data *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.PowerTransformer` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),\n",
      "            LogisticRegression())`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        PowerTransformer : Equivalent transformation with the\n",
      "            Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "        \n",
      "        quantile_transform : Maps data to a standard normal distribution with\n",
      "            the parameter `output_distribution='normal'`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      "        in ``transform``.\n",
      "        \n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n",
      "               improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n",
      "               (2000).\n",
      "        \n",
      "        .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n",
      "               of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      "    \n",
      "    quantile_transform(X, *, axis=0, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      "        Transform features using quantiles information.\n",
      "        \n",
      "        This method transforms the features to follow a uniform or a normal\n",
      "        distribution. Therefore, for a given feature, this transformation tends\n",
      "        to spread out the most frequent values. It also reduces the impact of\n",
      "        (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      "        \n",
      "        The transformation is applied on each feature independently. First an\n",
      "        estimate of the cumulative distribution function of a feature is\n",
      "        used to map the original values to a uniform distribution. The obtained\n",
      "        values are then mapped to the desired output distribution using the\n",
      "        associated quantile function. Features values of new/unseen data that fall\n",
      "        below or above the fitted range will be mapped to the bounds of the output\n",
      "        distribution. Note that this transform is non-linear. It may distort linear\n",
      "        correlations between variables measured at the same scale but renders\n",
      "        variables measured at different scales more directly comparable.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform.\n",
      "        \n",
      "        axis : int, default=0\n",
      "            Axis used to compute the means and standard deviations along. If 0,\n",
      "            transform each feature, otherwise (if 1) transform each sample.\n",
      "        \n",
      "        n_quantiles : int, default=1000 or n_samples\n",
      "            Number of quantiles to be computed. It corresponds to the number\n",
      "            of landmarks used to discretize the cumulative distribution function.\n",
      "            If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      "            to the number of samples as a larger number of quantiles does not give\n",
      "            a better approximation of the cumulative distribution function\n",
      "            estimator.\n",
      "        \n",
      "        output_distribution : {'uniform', 'normal'}, default='uniform'\n",
      "            Marginal distribution for the transformed data. The choices are\n",
      "            'uniform' (default) or 'normal'.\n",
      "        \n",
      "        ignore_implicit_zeros : bool, default=False\n",
      "            Only applies to sparse matrices. If True, the sparse entries of the\n",
      "            matrix are discarded to compute the quantile statistics. If False,\n",
      "            these entries are treated as zeros.\n",
      "        \n",
      "        subsample : int, default=1e5\n",
      "            Maximum number of samples used to estimate the quantiles for\n",
      "            computational efficiency. Note that the subsampling procedure may\n",
      "            differ for value-identical sparse and dense matrices.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for subsampling and smoothing\n",
      "            noise.\n",
      "            Please see ``subsample`` for more details.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Set to False to perform inplace transformation and avoid a copy (if the\n",
      "            input is already a numpy array). If True, a copy of `X` is transformed,\n",
      "            leaving the original `X` unchanged\n",
      "        \n",
      "            ..versionchanged:: 0.23\n",
      "                The default value of `copy` changed from False to True in 0.23.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.preprocessing import quantile_transform\n",
      "        >>> rng = np.random.RandomState(0)\n",
      "        >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      "        >>> quantile_transform(X, n_quantiles=10, random_state=0, copy=True)\n",
      "        array([...])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        QuantileTransformer : Performs quantile-based scaling using the\n",
      "            Transformer API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "        power_transform : Maps data to a normal distribution using a\n",
      "            power transformation.\n",
      "        scale : Performs standardization that is faster, but less robust\n",
      "            to outliers.\n",
      "        robust_scale : Performs robust standardization that removes the influence\n",
      "            of outliers but does not put outliers and inliers on the same scale.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "        transform.\n",
      "        \n",
      "        .. warning:: Risk of data leak\n",
      "        \n",
      "            Do not use :func:`~sklearn.preprocessing.quantile_transform` unless\n",
      "            you know what you are doing. A common mistake is to apply it\n",
      "            to the entire data *before* splitting into training and\n",
      "            test sets. This will bias the model evaluation because\n",
      "            information would have leaked from the test set to the\n",
      "            training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.QuantileTransformer` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking:`pipe = make_pipeline(QuantileTransformer(),\n",
      "            LogisticRegression())`.\n",
      "        \n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    robust_scale(X, *, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)\n",
      "        Standardize a dataset along any axis.\n",
      "        \n",
      "        Center to the median and component wise scale\n",
      "        according to the interquartile range.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_sample, n_features)\n",
      "            The data to center and scale.\n",
      "        \n",
      "        axis : int, default=0\n",
      "            Axis used to compute the medians and IQR along. If 0,\n",
      "            independently scale each feature, otherwise (if 1) scale\n",
      "            each sample.\n",
      "        \n",
      "        with_centering : bool, default=True\n",
      "            If `True`, center the data before scaling.\n",
      "        \n",
      "        with_scaling : bool, default=True\n",
      "            If `True`, scale the data to unit variance (or equivalently,\n",
      "            unit standard deviation).\n",
      "        \n",
      "        quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,        default=(25.0, 75.0)\n",
      "            Quantile range used to calculate `scale_`. By default this is equal to\n",
      "            the IQR, i.e., `q_min` is the first quantile and `q_max` is the third\n",
      "            quantile.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Set to `False` to perform inplace row normalization and avoid a\n",
      "            copy (if the input is already a numpy array or a scipy.sparse\n",
      "            CSR matrix and if axis is 1).\n",
      "        \n",
      "        unit_variance : bool, default=False\n",
      "            If `True`, scale data so that normally distributed features have a\n",
      "            variance of 1. In general, if the difference between the x-values of\n",
      "            `q_max` and `q_min` for a standard normal distribution is greater\n",
      "            than 1, the dataset will be scaled down. If less than 1, the dataset\n",
      "            will be scaled up.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This implementation will refuse to center scipy.sparse matrices\n",
      "        since it would make them non-sparse and would potentially crash the\n",
      "        program with memory exhaustion problems.\n",
      "        \n",
      "        Instead the caller is expected to either set explicitly\n",
      "        `with_centering=False` (in that case, only variance scaling will be\n",
      "        performed on the features of the CSR matrix) or to call `X.toarray()`\n",
      "        if he/she expects the materialized dense array to fit in memory.\n",
      "        \n",
      "        To avoid memory copy the caller should pass a CSR matrix.\n",
      "        \n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "        \n",
      "        .. warning:: Risk of data leak\n",
      "        \n",
      "            Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.RobustScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        RobustScaler : Performs centering and scaling using the Transformer API\n",
      "            (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    scale(X, *, axis=0, with_mean=True, with_std=True, copy=True)\n",
      "        Standardize a dataset along any axis.\n",
      "        \n",
      "        Center to the mean and component wise scale to unit variance.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to center and scale.\n",
      "        \n",
      "        axis : int, default=0\n",
      "            axis used to compute the means and standard deviations along. If 0,\n",
      "            independently standardize each feature, otherwise (if 1) standardize\n",
      "            each sample.\n",
      "        \n",
      "        with_mean : bool, default=True\n",
      "            If True, center the data before scaling.\n",
      "        \n",
      "        with_std : bool, default=True\n",
      "            If True, scale the data to unit variance (or equivalently,\n",
      "            unit standard deviation).\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            set to False to perform inplace row normalization and avoid a\n",
      "            copy (if the input is already a numpy array or a scipy.sparse\n",
      "            CSC matrix and if axis is 1).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "            The transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This implementation will refuse to center scipy.sparse matrices\n",
      "        since it would make them non-sparse and would potentially crash the\n",
      "        program with memory exhaustion problems.\n",
      "        \n",
      "        Instead the caller is expected to either set explicitly\n",
      "        `with_mean=False` (in that case, only variance scaling will be\n",
      "        performed on the features of the CSC matrix) or to call `X.toarray()`\n",
      "        if he/she expects the materialized dense array to fit in memory.\n",
      "        \n",
      "        To avoid memory copy the caller should pass a CSC matrix.\n",
      "        \n",
      "        NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "        and maintained during the data transformation.\n",
      "        \n",
      "        We use a biased estimator for the standard deviation, equivalent to\n",
      "        `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "        affect model performance.\n",
      "        \n",
      "        For a comparison of the different scalers, transformers, and normalizers,\n",
      "        see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "        <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "        \n",
      "        .. warning:: Risk of data leak\n",
      "        \n",
      "            Do not use :func:`~sklearn.preprocessing.scale` unless you know\n",
      "            what you are doing. A common mistake is to apply it to the entire data\n",
      "            *before* splitting into training and test sets. This will bias the\n",
      "            model evaluation because information would have leaked from the test\n",
      "            set to the training set.\n",
      "            In general, we recommend using\n",
      "            :class:`~sklearn.preprocessing.StandardScaler` within a\n",
      "            :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "            leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        StandardScaler : Performs scaling to unit variance using the Transformer\n",
      "            API (e.g. as part of a preprocessing\n",
      "            :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Binarizer', 'FunctionTransformer', 'KBinsDiscretizer', 'Ke...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ksi\\appdata\\roaming\\python\\python37\\site-packages\\sklearn\\preprocessing\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "help(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance.\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : bool, default=True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : bool, default=True\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : bool, default=True\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray of shape (n_features,) or None\n",
      " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      " |      variance is zero, we can't achieve unit variance, and the data is left\n",
      " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      " |      when `with_std=False`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,) or None\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_features,) or None\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array of dtype int. If\n",
      " |      `sample_weights` are used it will be a float (if no missing data)\n",
      " |      or an array of dtype float that sums the weights seen so far.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      " |      correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base._OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'diabetes',\n",
       "       'age', 'height', 'weight', 'gender', 'BMI', 'diastolic', 'systolic',\n",
       "       'pp', 'pp%', 'heart_alarm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scale = ['age', 'height', 'weight', 'BMI', 'diastolic', 'systolic']\n",
    "df0 = df[features_scale]\n",
    "mapper = DataFrameMapper([(df0.columns, StandardScaler())])\n",
    "scaled_features = mapper.fit_transform(df0.copy(), 4)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=df0.index, columns=df0.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.159482</td>\n",
       "      <td>0.072836</td>\n",
       "      <td>1.083638</td>\n",
       "      <td>1.057781</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>-0.163818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.956680</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-1.660636</td>\n",
       "      <td>-1.595035</td>\n",
       "      <td>-1.095020</td>\n",
       "      <td>-1.179007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.608291</td>\n",
       "      <td>-1.078449</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>1.866560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.910403</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>1.015031</td>\n",
       "      <td>1.219660</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>0.851371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.416056</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.032450</td>\n",
       "      <td>1.735802</td>\n",
       "      <td>1.975527</td>\n",
       "      <td>0.953733</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>1.016491</td>\n",
       "      <td>-0.566767</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.851371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.342923</td>\n",
       "      <td>-0.822608</td>\n",
       "      <td>-0.974568</td>\n",
       "      <td>-0.651287</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-1.179007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.415785</td>\n",
       "      <td>1.352041</td>\n",
       "      <td>0.740604</td>\n",
       "      <td>0.053401</td>\n",
       "      <td>0.978388</td>\n",
       "      <td>0.851371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.762555</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>-0.219892</td>\n",
       "      <td>-0.495128</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-2.194196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight       BMI  diastolic  systolic\n",
       "0      1.159482  0.072836  1.083638  1.057781  -0.297555 -0.163818\n",
       "1     -1.956680 -0.310926 -1.660636 -1.595035  -1.095020 -1.179007\n",
       "2      1.608291 -1.078449 -0.014072  0.555809   0.499910  1.866560\n",
       "3     -0.910403 -0.310926  1.015031  1.219660  -0.297555  0.851371\n",
       "4     -0.416056 -0.310926 -0.151285 -0.007259   0.101177 -0.163818\n",
       "...         ...       ...       ...       ...        ...       ...\n",
       "69995  0.032450  1.735802  1.975527  0.953733   0.101177 -0.163818\n",
       "69996  1.016491 -0.566767  0.534783  0.863335   0.101177  0.851371\n",
       "69997  0.342923 -0.822608 -0.974568 -0.651287  -0.696287 -1.179007\n",
       "69998  0.415785  1.352041  0.740604  0.053401   0.978388  0.851371\n",
       "69999 -0.762555  0.584518 -0.219892 -0.495128  -0.696287 -2.194196\n",
       "\n",
       "[67496 rows x 6 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.749600e+04</td>\n",
       "      <td>6.749600e+04</td>\n",
       "      <td>6.749600e+04</td>\n",
       "      <td>6.749600e+04</td>\n",
       "      <td>6.749600e+04</td>\n",
       "      <td>6.749600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.024131e-16</td>\n",
       "      <td>1.346114e-16</td>\n",
       "      <td>9.153797e-17</td>\n",
       "      <td>9.435473e-16</td>\n",
       "      <td>2.870121e-16</td>\n",
       "      <td>-2.229921e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.488400e+00</td>\n",
       "      <td>-3.125177e+00</td>\n",
       "      <td>-2.689739e+00</td>\n",
       "      <td>-2.882532e+00</td>\n",
       "      <td>-2.689949e+00</td>\n",
       "      <td>-7.270141e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.249092e-01</td>\n",
       "      <td>-6.946874e-01</td>\n",
       "      <td>-6.315333e-01</td>\n",
       "      <td>-6.894729e-01</td>\n",
       "      <td>-2.975551e-01</td>\n",
       "      <td>-1.638182e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.138125e-01</td>\n",
       "      <td>7.283562e-02</td>\n",
       "      <td>-2.198921e-01</td>\n",
       "      <td>-2.224924e-01</td>\n",
       "      <td>-2.975551e-01</td>\n",
       "      <td>-1.638182e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.594501e-01</td>\n",
       "      <td>7.124381e-01</td>\n",
       "      <td>5.347833e-01</td>\n",
       "      <td>5.441501e-01</td>\n",
       "      <td>4.999096e-01</td>\n",
       "      <td>8.513707e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.734585e+00</td>\n",
       "      <td>5.445497e+00</td>\n",
       "      <td>8.630392e+00</td>\n",
       "      <td>8.022231e+00</td>\n",
       "      <td>5.879458e+01</td>\n",
       "      <td>1.201845e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        height        weight           BMI     diastolic  \\\n",
       "count  6.749600e+04  6.749600e+04  6.749600e+04  6.749600e+04  6.749600e+04   \n",
       "mean  -9.024131e-16  1.346114e-16  9.153797e-17  9.435473e-16  2.870121e-16   \n",
       "std    1.000007e+00  1.000007e+00  1.000007e+00  1.000007e+00  1.000007e+00   \n",
       "min   -3.488400e+00 -3.125177e+00 -2.689739e+00 -2.882532e+00 -2.689949e+00   \n",
       "25%   -7.249092e-01 -6.946874e-01 -6.315333e-01 -6.894729e-01 -2.975551e-01   \n",
       "50%    1.138125e-01  7.283562e-02 -2.198921e-01 -2.224924e-01 -2.975551e-01   \n",
       "75%    7.594501e-01  7.124381e-01  5.347833e-01  5.441501e-01  4.999096e-01   \n",
       "max    1.734585e+00  5.445497e+00  8.630392e+00  8.022231e+00  5.879458e+01   \n",
       "\n",
       "           systolic  \n",
       "count  6.749600e+04  \n",
       "mean  -2.229921e-16  \n",
       "std    1.000007e+00  \n",
       "min   -7.270141e+00  \n",
       "25%   -1.638182e-01  \n",
       "50%   -1.638182e-01  \n",
       "75%    8.513707e-01  \n",
       "max    1.201845e+01  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>gender</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.159482</td>\n",
       "      <td>0.072836</td>\n",
       "      <td>1.083638</td>\n",
       "      <td>1.057781</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.956680</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-1.660636</td>\n",
       "      <td>-1.595035</td>\n",
       "      <td>-1.095020</td>\n",
       "      <td>-1.179007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.608291</td>\n",
       "      <td>-1.078449</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>1.866560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.910403</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>1.015031</td>\n",
       "      <td>1.219660</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.416056</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.032450</td>\n",
       "      <td>1.735802</td>\n",
       "      <td>1.975527</td>\n",
       "      <td>0.953733</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>1.016491</td>\n",
       "      <td>-0.566767</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.342923</td>\n",
       "      <td>-0.822608</td>\n",
       "      <td>-0.974568</td>\n",
       "      <td>-0.651287</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-1.179007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.415785</td>\n",
       "      <td>1.352041</td>\n",
       "      <td>0.740604</td>\n",
       "      <td>0.053401</td>\n",
       "      <td>0.978388</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.762555</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>-0.219892</td>\n",
       "      <td>-0.495128</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-2.194196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight       BMI  diastolic  systolic  \\\n",
       "0      1.159482  0.072836  1.083638  1.057781  -0.297555 -0.163818   \n",
       "1     -1.956680 -0.310926 -1.660636 -1.595035  -1.095020 -1.179007   \n",
       "2      1.608291 -1.078449 -0.014072  0.555809   0.499910  1.866560   \n",
       "3     -0.910403 -0.310926  1.015031  1.219660  -0.297555  0.851371   \n",
       "4     -0.416056 -0.310926 -0.151285 -0.007259   0.101177 -0.163818   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "69995  0.032450  1.735802  1.975527  0.953733   0.101177 -0.163818   \n",
       "69996  1.016491 -0.566767  0.534783  0.863335   0.101177  0.851371   \n",
       "69997  0.342923 -0.822608 -0.974568 -0.651287  -0.696287 -1.179007   \n",
       "69998  0.415785  1.352041  0.740604  0.053401   0.978388  0.851371   \n",
       "69999 -0.762555  0.584518 -0.219892 -0.495128  -0.696287 -2.194196   \n",
       "\n",
       "       cholesterol  gluc  smoke  alco  active  diabetes  gender  heart_alarm  \n",
       "0              0.0   0.5    0.0   0.0     0.0       1.0     0.0            0  \n",
       "1              0.0   0.0    0.0   0.0     1.0       0.0     1.0            0  \n",
       "2              0.0   0.0    0.0   0.0     0.0       1.0     1.0            0  \n",
       "3              0.0   0.0    0.0   0.0     1.0       0.0     1.0            0  \n",
       "4              0.0   0.0    0.0   0.0     0.0       0.0     0.0            0  \n",
       "...            ...   ...    ...   ...     ...       ...     ...          ...  \n",
       "69995          0.0   0.0    0.0   0.0     1.0       0.0     0.0            0  \n",
       "69996          0.0   0.0    0.0   0.0     1.0       0.0     1.0            0  \n",
       "69997          0.0   0.0    1.0   0.0     1.0       0.0     1.0            0  \n",
       "69998          0.5   0.0    1.0   0.0     1.0       1.0     0.0            0  \n",
       "69999          0.0   0.0    0.0   0.0     1.0       0.0     1.0            0  \n",
       "\n",
       "[67496 rows x 14 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df[['cholesterol', 'gluc', 'smoke', 'alco', 'active', 'diabetes', 'gender', 'heart_alarm']] = df[['cholesterol', 'gluc', 'smoke', 'alco', 'active', 'diabetes', 'gender', 'heart_alarm']]\n",
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height', 'weight', 'BMI', 'diastolic', 'systolic',\n",
       "       'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'diabetes', 'gender',\n",
       "       'heart_alarm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scaled_features_df[['age', 'height', 'weight', 'BMI', 'diastolic', 'systolic', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'gender', 'heart_alarm']]\n",
    "y = scaled_features_df['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>gender</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.159482</td>\n",
       "      <td>0.072836</td>\n",
       "      <td>1.083638</td>\n",
       "      <td>1.057781</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.956680</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-1.660636</td>\n",
       "      <td>-1.595035</td>\n",
       "      <td>-1.095020</td>\n",
       "      <td>-1.179007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.608291</td>\n",
       "      <td>-1.078449</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>0.555809</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>1.866560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.910403</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>1.015031</td>\n",
       "      <td>1.219660</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.416056</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.032450</td>\n",
       "      <td>1.735802</td>\n",
       "      <td>1.975527</td>\n",
       "      <td>0.953733</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>1.016491</td>\n",
       "      <td>-0.566767</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.342923</td>\n",
       "      <td>-0.822608</td>\n",
       "      <td>-0.974568</td>\n",
       "      <td>-0.651287</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-1.179007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.415785</td>\n",
       "      <td>1.352041</td>\n",
       "      <td>0.740604</td>\n",
       "      <td>0.053401</td>\n",
       "      <td>0.978388</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.762555</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>-0.219892</td>\n",
       "      <td>-0.495128</td>\n",
       "      <td>-0.696287</td>\n",
       "      <td>-2.194196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67496 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight       BMI  diastolic  systolic  \\\n",
       "0      1.159482  0.072836  1.083638  1.057781  -0.297555 -0.163818   \n",
       "1     -1.956680 -0.310926 -1.660636 -1.595035  -1.095020 -1.179007   \n",
       "2      1.608291 -1.078449 -0.014072  0.555809   0.499910  1.866560   \n",
       "3     -0.910403 -0.310926  1.015031  1.219660  -0.297555  0.851371   \n",
       "4     -0.416056 -0.310926 -0.151285 -0.007259   0.101177 -0.163818   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "69995  0.032450  1.735802  1.975527  0.953733   0.101177 -0.163818   \n",
       "69996  1.016491 -0.566767  0.534783  0.863335   0.101177  0.851371   \n",
       "69997  0.342923 -0.822608 -0.974568 -0.651287  -0.696287 -1.179007   \n",
       "69998  0.415785  1.352041  0.740604  0.053401   0.978388  0.851371   \n",
       "69999 -0.762555  0.584518 -0.219892 -0.495128  -0.696287 -2.194196   \n",
       "\n",
       "       cholesterol  gluc  smoke  alco  active  gender  heart_alarm  \n",
       "0              0.0   0.5    0.0   0.0     0.0     0.0            0  \n",
       "1              0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "2              0.0   0.0    0.0   0.0     0.0     1.0            0  \n",
       "3              0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "4              0.0   0.0    0.0   0.0     0.0     0.0            0  \n",
       "...            ...   ...    ...   ...     ...     ...          ...  \n",
       "69995          0.0   0.0    0.0   0.0     1.0     0.0            0  \n",
       "69996          0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "69997          0.0   0.0    1.0   0.0     1.0     1.0            0  \n",
       "69998          0.5   0.0    1.0   0.0     1.0     0.0            0  \n",
       "69999          0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "\n",
       "[67496 rows x 13 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        0.0\n",
       "2        1.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "69995    0.0\n",
       "69996    0.0\n",
       "69997    0.0\n",
       "69998    1.0\n",
       "69999    0.0\n",
       "Name: diabetes, Length: 67496, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.14, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>gender</th>\n",
       "      <th>heart_alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37668</th>\n",
       "      <td>0.420237</td>\n",
       "      <td>0.072836</td>\n",
       "      <td>-0.905961</td>\n",
       "      <td>-0.959781</td>\n",
       "      <td>0.898642</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14421</th>\n",
       "      <td>1.027825</td>\n",
       "      <td>1.224120</td>\n",
       "      <td>-0.631533</td>\n",
       "      <td>-1.137204</td>\n",
       "      <td>-1.493752</td>\n",
       "      <td>-2.194196</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27870</th>\n",
       "      <td>0.372068</td>\n",
       "      <td>-1.334290</td>\n",
       "      <td>0.191749</td>\n",
       "      <td>0.945970</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23122</th>\n",
       "      <td>-1.264897</td>\n",
       "      <td>0.712438</td>\n",
       "      <td>-0.631533</td>\n",
       "      <td>-0.943592</td>\n",
       "      <td>2.094839</td>\n",
       "      <td>2.881749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16985</th>\n",
       "      <td>-0.466857</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>-0.357106</td>\n",
       "      <td>-0.627761</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38571</th>\n",
       "      <td>0.067262</td>\n",
       "      <td>-1.334290</td>\n",
       "      <td>0.397570</td>\n",
       "      <td>1.185564</td>\n",
       "      <td>1.297374</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6487</th>\n",
       "      <td>-1.043478</td>\n",
       "      <td>0.456597</td>\n",
       "      <td>-0.151285</td>\n",
       "      <td>-0.371799</td>\n",
       "      <td>-0.297555</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56910</th>\n",
       "      <td>0.272388</td>\n",
       "      <td>-0.310926</td>\n",
       "      <td>0.054535</td>\n",
       "      <td>0.209256</td>\n",
       "      <td>0.101177</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.124539</td>\n",
       "      <td>0.328677</td>\n",
       "      <td>-0.357106</td>\n",
       "      <td>-0.517504</td>\n",
       "      <td>1.297374</td>\n",
       "      <td>-0.163818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>-0.772674</td>\n",
       "      <td>-0.694687</td>\n",
       "      <td>0.671997</td>\n",
       "      <td>1.089730</td>\n",
       "      <td>0.898642</td>\n",
       "      <td>0.851371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58046 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight       BMI  diastolic  systolic  \\\n",
       "37668  0.420237  0.072836 -0.905961 -0.959781   0.898642  0.851371   \n",
       "14421  1.027825  1.224120 -0.631533 -1.137204  -1.493752 -2.194196   \n",
       "27870  0.372068 -1.334290  0.191749  0.945970   0.499910 -0.163818   \n",
       "23122 -1.264897  0.712438 -0.631533 -0.943592   2.094839  2.881749   \n",
       "16985 -0.466857  0.584518 -0.357106 -0.627761  -0.297555 -0.163818   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "38571  0.067262 -1.334290  0.397570  1.185564   1.297374  0.851371   \n",
       "6487  -1.043478  0.456597 -0.151285 -0.371799  -0.297555 -0.163818   \n",
       "56910  0.272388 -0.310926  0.054535  0.209256   0.101177 -0.163818   \n",
       "899    0.124539  0.328677 -0.357106 -0.517504   1.297374 -0.163818   \n",
       "16373 -0.772674 -0.694687  0.671997  1.089730   0.898642  0.851371   \n",
       "\n",
       "       cholesterol  gluc  smoke  alco  active  gender  heart_alarm  \n",
       "37668          0.0   0.0    0.0   0.0     1.0     0.0            0  \n",
       "14421          0.5   0.0    0.0   0.0     1.0     1.0            0  \n",
       "27870          0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "23122          0.0   0.0    0.0   0.0     1.0     0.0            0  \n",
       "16985          0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "...            ...   ...    ...   ...     ...     ...          ...  \n",
       "38571          0.0   0.0    0.0   0.0     1.0     1.0            0  \n",
       "6487           0.0   0.5    0.0   0.0     1.0     1.0            0  \n",
       "56910          0.5   0.0    1.0   0.0     1.0     0.0            0  \n",
       "899            1.0   0.0    0.0   0.0     1.0     0.0            0  \n",
       "16373          0.0   0.0    0.0   0.0     0.0     1.0            0  \n",
       "\n",
       "[58046 rows x 13 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size = 0.14, random_state = 42)\n",
    "# x_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |  \n",
      " |      - `'none'`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |  \n",
      " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |            and 'saga' are faster for large ones;\n",
      " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      " |            'lbfgs' handle multinomial loss;\n",
      " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen:\n",
      " |         Supported penalties by solver:\n",
      " |  \n",
      " |         - 'newton-cg'   -   ['l2', 'none']\n",
      " |         - 'lbfgs'       -   ['l2', 'none']\n",
      " |         - 'liblinear'   -   ['l1', 'l2']\n",
      " |         - 'sag'         -   ['l2', 'none']\n",
      " |         - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      " |  \n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |         features with approximately the same scale. You can\n",
      " |         preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      " |  \n",
      " |      .. seealso::\n",
      " |         Refer to the User Guide for more information regarding\n",
      " |         :class:`LogisticRegression` and more specifically the\n",
      " |         `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      " |         summarazing solver/penalty supports.\n",
      " |         <!--\n",
      " |         # noqa: E501\n",
      " |         -->\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregmodel = LogisticRegression(random_state=0).fit(x_train,y_train)\n",
    "y_prediction = logregmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7014,   53],\n",
       "       [  89, 2294]], dtype=int64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9843435946260446"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (7013+2292)/(7013+2292+57+91)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871903153153153"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 7013/(7013+91)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923588509975945"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 7013/(7013+54)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
